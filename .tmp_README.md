  # CGM Data Processor

  ![Python](https://img.shields.io/badge/python-3.10%2B-blue) [![Project Status: Active](https://www.repostatus.org/badges/latest/active.svg)](https://github.com/Warren8824/cgm-data-processor) ![Release Status](https://img.shields.io/badge/status-pre--release-orange) ![Black](https://img.shields.io/badge/code%20style-black-4B8BBE.svg) ![isort](https://img.shields.io/badge/imports-isort-4B8BBE.svg) ![Pylint](https://img.shields.io/badge/code%20quality-pylint-4B8BBE.svg) [![licence: MIT](https://img.shields.io/badge/licence-MIT-yellow.svg)](https://opensource.org/licenses/MIT)


  > ⚠️ This project is a personal data analysis tool for processing CGM, insulin and meal export files. It is intended for analysis and research and is not a medical device.

  Built and maintained by a Type‑1 (T1D) user who processes their own device exports. The goal is to make device exports practical to analyse, compare and visualise with minimal manual cleanup.

  Summary
  -------
  A Python package that detects device export formats, parses CGM / insulin / carbs / notes data, standardises and validates fields, aligns multiple sources to a reference timeline, and produces clean CSV exports and processing metadata.

  Quick links
  -----------
  - Source: this repository
  - Documentation: https://warren8824.github.io/cgm-data-processor/ (API docs and guides)

  Why this exists
  ---------------
  Device export files are messy: different vendors use different column names, timestamp formats, preamble rows, and export layouts. This project automates the normalisation and produces analysis-ready CSVs with processing notes so personal / research workflows can start from consistent data.

  Features (what it does today)
  ---------------------------
  - Automatic format detection with a format registry and lightweight validators (CSV, SQLite, JSON, XML).
  - CSV reading with header-row hints and header auto-detection; header normalisation (strip + lowercasing) for robust matching.
  - Deterministic timestamp detection: numeric epoch detection, explicit format matching (including day-first variants), then a pandas-based fallback. The detector returns parse kwargs used to parse the entire column consistently and convert to UTC.
  - Per-type processors for CGM, insulin, carbs and notes: standardise columns, validate units, classify insulin doses, and produce processing notes.
  - Alignment onto a reference timeline and combined aligned CSV export.
  - Exporter writes per-run folders containing per-type CSVs and `processing_notes.json` and monthly splits scoped to the run.

  Installation
  ------------
  Recommended: use Poetry or pip. Examples for Windows PowerShell are shown below.

  Using Poetry
  ```powershell
  pip install poetry
  git clone https://github.com/Warren8824/cgm-data-processor.git
  cd cgm-data-processor
  poetry install
  ```

  Using pip
  ```powershell
  git clone https://github.com/Warren8824/cgm-data-processor.git
  cd cgm-data-processor
  pip install -r requirements.txt
  # For development dependencies
  pip install -r requirements-dev.txt
  pre-commit install
  ```

  Basic usage
  -----------
  Run the CLI on a local export file:

  ```powershell
  python -m src.cli path\to\export.file --debug
  ```

  Examples
  - LibreView CSV: `python -m src.cli data/sample.csv --debug`
  - xDrip+ SQLite: `python -m src.cli data/sample.sqlite --debug`

  CLI highlights
  - `--debug` : enable debug logging
  - `--output PATH` : change default export directory (defaults to `data/exports`)
  - `--interpolation-limit` : interpolation settings for CGM gaps
  - `--bolus-limit`, `--max-dose` : insulin classification thresholds

  Export layout (current)
  ------------------------
  Each run produces a self-contained directory under `data/exports` to avoid accidental overwrites:

  - `data/exports/{start}_to_{end}_complete_{run_id}/`
    - `cgm.csv`, `insulin.csv`, `carbs.csv`, `notes.csv` (when present)
    - `aligned_data.csv` (combined timeline)
    - `processing_notes.json` (metadata and processing summaries)
    - `monthly/` (monthly splits inside the run folder, e.g. `monthly/2025-10/cgm.csv`)

  This keeps exports reproducible and easy to share or archive per-run.

  Example `processing_notes.json` (excerpt)
  ```json
  {
    "export_date": "2025-10-04T19:10:59.503313",
    "date_range": "2023-06-03 to 2023-10-05",
    "record_count": 35666,
    "notes": [
      "CGM Processing Notes:",
      "  Processed 35666 total CGM readings"
    ]
  }
  ```

  Where to look in `src/`
  ----------------------
  - `src/cli.py` — command line entry and orchestration
  - `src/file_parser/format_detector.py` — format registry and validators
  - `src/readers/` — base reader (timestamp detection) and concrete CSV/SQLite readers
  - `src/processors/` — per-type processors and orchestrator
  - `src/core/aligner.py` — timeline alignment logic
  - `src/exporters/` — exporter logic and CSV exporter
  - `src/core/devices/` — device format definitions and `header_row` hints

  Limitations & caveats
  ---------------------
  - This is an analysis tool — not a medical device. Use outputs for analysis only.
  - Timestamp detection covers common patterns but can be conservative; mixed or malformed timestamps may require manual overrides.
  - `run_id` is a timestamp by default (second resolution); consider configuring or increasing resolution for automated workflows.

  Development notes
  -----------------
  - Code style: Black, isort. Linting with Pylint.
  - Tests: pytest is used where tests exist; coverage is still growing.
  - Documentation: MkDocs + mkdocstrings for API docs (see `mkdocs.yml`).

  Planned and suggested next steps
  -------------------------------------------
  1. Plotly visual exports — add optional interactive HTML/PNG plots per run (summary + monthly breakdowns).
  2. `data/exports/index.json` — a simple catalogue of runs for easier discovery.
  3. Unit tests for timestamp detection across epoch, day-first, month-first and ISO formats.
  4. Make `run_id` configurable and/or use millisecond timestamps for guaranteed uniqueness.

  Contributing
  ------------
  Contributions welcome. If you can help with device formats or sample files, that is especially useful. Typical contribution flow:

  1. Open an issue to discuss changes or new device support.
  2. Create a branch and submit a PR with tests where applicable.

  licence
  -------
  MIT — see `licence` in this repository.

  Contact / Sponsorship
  ---------------------
  Open an issue or contact via the repository profile. If you'd like to support further development (example: hosting, CI, or dataset curation), open an issue and we can discuss options.