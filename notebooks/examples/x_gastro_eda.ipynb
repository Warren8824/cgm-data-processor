{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf60efab",
   "metadata": {},
   "source": [
    "# xDrip+ Data Analysis for Gastroparesis Screening (XGastro-EDA)\n",
    "\n",
    "## Introduction\n",
    "This notebook demonstrates the processing and analysis of Continuous Glucose Monitoring (CGM) data from xDrip+ for potential gastroparesis screening applications. By carefully processing CGM data alongside meal and insulin records, we create a foundation for analyzing post-meal glucose response patterns.\n",
    "\n",
    "### Project Context\n",
    "Gastroparesis, a condition affecting gastric emptying, can be challenging to diagnose. Traditional screening methods often require specialized testing. This project explores the potential of using readily available CGM data to identify patterns that might indicate delayed gastric emptying.\n",
    "\n",
    "### Data Source\n",
    "- **xDrip+**: An open-source diabetes management application\n",
    "- **Time Period**: June 2023 - September 2024\n",
    "- **Key Components**:\n",
    " * CGM readings at 5-minute intervals\n",
    " * Meal records with carbohydrate quantities\n",
    " * Insulin dosing (basal and bolus)\n",
    "\n",
    "### Project Objectives\n",
    "1. **Data Integration**\n",
    "  - Align CGM, meal, and insulin data to common timeline\n",
    "  - Ensure data quality and completeness\n",
    "  - Create robust meal analysis framework\n",
    "\n",
    "2. **Quality Assurance**\n",
    "  - Identify and characterize data gaps\n",
    "  - Implement appropriate interpolation strategies\n",
    "  - Maintain data integrity for clinical analysis\n",
    "\n",
    "3. **Analysis Preparation**\n",
    "  - Focus on significant meals (>20g carbs)\n",
    "  - Create 4-hour post-meal analysis windows\n",
    "  - Develop quality metrics for meal data assessment\n",
    "\n",
    "### Notebook Structure\n",
    "1. Data extraction and cleaning\n",
    "2. Timeline alignment and gap analysis\n",
    "3. Meal period identification and classification\n",
    "4. Quality assessment and interpolation\n",
    "5. Statistical validation\n",
    "6. Preparation for further analysis\n",
    "\n",
    "This notebook focuses on data preparation and quality assessment, laying the groundwork for subsequent analysis of post-meal glucose patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a969fe54",
   "metadata": {},
   "source": [
    "# Required Libraries and Dependencies\n",
    "\n",
    "This notebook uses several Python libraries for data manipulation, analysis, and visualization:\n",
    "\n",
    "- **pandas**: Core data manipulation and analysis\n",
    "- **numpy**: Numerical operations and array handling (imported in respective module)\n",
    "- **matplotlib**: Data visualization and plotting\n",
    "- **sqlalchemy**: Database connectivity and operations\n",
    "- **ast**: Safe parsing of JSON-like strings (imported in respective module)\n",
    "- **enum, dataclass, typing**: Type hinting and structured data classes (imported in respective module)\n",
    "- **display**: Improved print options in Jupyter\n",
    "- **pprint**: Improved print layout of nested data structures\n",
    "\n",
    "Note: This project assumes you have these libraries installed. If needed, install via pip:\n",
    "```pip install pandas numpy matplotlib sqlalchemy```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861589b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from IPython.display import display\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee69978",
   "metadata": {},
   "source": [
    "# Import our created functions\n",
    "\n",
    "You can check all these out in the src directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56586112",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src.preprocessing.alignment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_184987/1182229842.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcleaning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclean_classify_insulin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_classify_carbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_glucose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malignment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0malign_diabetes_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMealQuality\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMealAnalysisConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manalysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgaps\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0manalyse_glucose_gaps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src.preprocessing.alignment'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "notebook_path = os.path.abspath('.')\n",
    "project_root = os.path.join(notebook_path, '../../')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "    \n",
    "from src.preprocessing.cleaning import clean_classify_insulin, clean_classify_carbs, clean_glucose\n",
    "from src.preprocessing.alignment import align_diabetes_data\n",
    "from src.analysis.config import MealQuality, MealAnalysisConfig\n",
    "from src.analysis.gaps import analyse_glucose_gaps\n",
    "from src.analysis.insulin import analyse_insulin_over_time\n",
    "from src.analysis.meals import analyse_meal_data\n",
    "from src.visualization.formatting import format_gaps_output, format_meal_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f26dde5",
   "metadata": {},
   "source": [
    "# Data Source: xDrip+ SQLite Database\n",
    "\n",
    "This notebook begins by connecting to an xDrip+ SQLite database export. xDrip+ is an open-source diabetes management application that stores various diabetes-related data including glucose readings, treatments, and device status information.\n",
    "\n",
    "## Database Connection\n",
    "We establish a connection to the SQLite database and inspect its structure. The database contains 29 tables storing different aspects of diabetes management data:\n",
    "\n",
    "Key tables for our analysis:\n",
    "- **BgReadings**: Continuous glucose monitoring (CGM) data\n",
    "- **Treatments**: Insulin doses and carbohydrate intake records\n",
    "- **Calibration**: Sensor calibration events\n",
    "- **Sensors**: CGM sensor information\n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bc008c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Path to your SQLite file\n",
    "db_path = 'data/export20240928-130349.sqlite'\n",
    "\n",
    "# Create an SQLAlchemy engine\n",
    "engine = create_engine(f'sqlite:///{db_path}')\n",
    "\n",
    "# Use SQLAlchemy's inspector to list all tables\n",
    "inspector = inspect(engine)\n",
    "tables = inspector.get_table_names()\n",
    "\n",
    "# Display firmatted table names\n",
    "#displayer.display_database_tables(db_path, tables)\n",
    "pprint(tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94cfcd9",
   "metadata": {},
   "source": [
    "# Loading and Initial Data Processing\n",
    "\n",
    "We extract three key datasets from the xDrip+ database and prepare them for analysis by:\n",
    "1. Converting Unix timestamps to pandas datetime format\n",
    "2. Setting timestamps as index for time-series analysis\n",
    "3. Creating separate dataframes for:\n",
    "  - Blood glucose readings\n",
    "  - Treatment records (insulin & carbohydrates)\n",
    "  - Heart rate measurements\n",
    "\n",
    "## Data Loading Process\n",
    "Each table requires the same preprocessing steps:\n",
    "- SQL to pandas DataFrame conversion\n",
    "- Timestamp conversion from milliseconds\n",
    "- Index creation from timestamp\n",
    "\n",
    "Let's examine the structure of our blood glucose data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111b2700",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load BgReadings table into a pandas DataFrame\n",
    "glucose_data = 'BgReadings'  # Table containing all BG Readings from XDrip+\n",
    "bg_df = pd.read_sql_table(glucose_data, con=engine)\n",
    "bg_df['timestamp'] = pd.to_datetime(bg_df['timestamp'], unit='ms')\n",
    "bg_df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Load Treatments table into a pandas DataFrame\n",
    "treatment_data = 'Treatments'  # Table containing all Treatments from XDrip+\n",
    "treatment_df = pd.read_sql_table(treatment_data, con=engine)\n",
    "treatment_df['timestamp'] = pd.to_datetime(treatment_df['timestamp'], unit='ms')\n",
    "treatment_df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Load HeartRate table into a pandas Dataframe (Not required for this analysis)\n",
    "heart_rate_data = 'HeartRate' # Table containing HeartRate data\n",
    "heart_rate_df = pd.read_sql_table(heart_rate_data, con=engine)\n",
    "heart_rate_df['timestamp'] = pd.to_datetime(heart_rate_df['timestamp'], unit='ms')\n",
    "heart_rate_df.set_index('timestamp', inplace=True)\n",
    "\n",
    "# Explore the first few rows of the blood glucose table\n",
    "bg_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee36fa97",
   "metadata": {},
   "source": [
    "# Treatment Data Preprocessing: Insulin Classification\n",
    "\n",
    "This function implements a sophisticated insulin classification algorithm for diabetes treatment data. The classification process handles both labeled and unlabeled insulin entries, using a combination of explicit insulin types and quantity-based rules.\n",
    "\n",
    "## Classification Rules\n",
    "1. **Labeled Insulin**\n",
    "  - Novorapid → Bolus insulin\n",
    "  - Levemir → Basal insulin\n",
    "\n",
    "2. **Unlabeled Insulin**\n",
    "  - ≤ 8 units → Classified as Bolus\n",
    "  - 8-15 units → Classified as Basal\n",
    "  - >15 units → Dropped (likely data entry errors)\n",
    "\n",
    "## Function Process\n",
    "- Creates separate columns for bolus and basal insulin\n",
    "- Processes JSON metadata for labeled entries\n",
    "- Applies rule-based classification for unlabeled entries\n",
    "- Maintains data integrity by dropping suspicious entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b7cc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_classify_insulin(df):\n",
    "    '''\n",
    "    Clean and separate basal and bolus insulin into separate columns based on insulin type if selected.\n",
    "    \"Novorapid\" = Bolus & \"Levemir\" = Basal, if undefined then all insulin <= 8 units are classified as bolus,\n",
    "    and <= 15 units are classified as Basal. All unclassified treatments over 15 units are dropped.\n",
    "    '''\n",
    "    \n",
    "    # Create a copy to avoid altering the original dataframe\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Keep only rows where insulin is > 0.0 units\n",
    "    df_clean = df_clean[df_clean['insulin'] > 0.0]\n",
    "    \n",
    "    # Initialize bolus and basal columns with 0\n",
    "    df_clean['bolus'] = 0.0\n",
    "    df_clean['basal'] = 0.0\n",
    "    \n",
    "    # Process labeled data first using insulinJSON column\n",
    "    def extract_insulin_type(row):\n",
    "        try:\n",
    "            if pd.isna(row['insulinJSON']):\n",
    "                return None\n",
    "            data = json.loads(row['insulinJSON'])\n",
    "            insulin_type = data.get('insulin', '').lower()\n",
    "            if 'novorapid' in insulin_type:\n",
    "                df_clean.at[row.name, 'bolus'] = row['insulin']\n",
    "            elif 'levemir' in insulin_type:\n",
    "                df_clean.at[row.name, 'basal'] = row['insulin']\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "    # Apply extract_insulin_types to each row\n",
    "    df_clean.apply(extract_insulin_type, axis=1)\n",
    "    \n",
    "    # Process unlabeled data\n",
    "    unlabeled_mask = (df_clean['bolus'] == 0) & (df_clean['basal'] == 0)\n",
    "    \n",
    "    # Drop all rows where insulin is unclassified AND > 15 units (Likely error)\n",
    "    df_clean = df_clean.drop(\n",
    "        df_clean[unlabeled_mask & (df_clean['insulin'] > 15)].index\n",
    "    )\n",
    "    \n",
    "    # Classify remaining unlabeled data\n",
    "    df_clean.loc[unlabeled_mask & (df_clean['insulin'] <= 8), 'bolus'] = df_clean['insulin']\n",
    "    df_clean.loc[unlabeled_mask & (df_clean['insulin'] > 8) & (df_clean['insulin'] <= 15), 'basal'] = df_clean['insulin']\n",
    "    \n",
    "    # Drop the original insulin column if desired\n",
    "    # df_clean = df_clean.drop('insulin', axis=1)\n",
    "    \n",
    "    return df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec783a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "insulin_df = clean_classify_insulin(treatment_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ebe143",
   "metadata": {},
   "source": [
    "# Analysing Insulin Usage Patterns Over Time\n",
    "\n",
    "This analysis examines the temporal patterns in insulin usage, providing insights into treatment consistency and data quality. The function performs both data analysis and visualization, helping identify trends and potential data collection issues.\n",
    "\n",
    "## Analysis Components\n",
    "1. **Temporal Aggregation**\n",
    "  - Groups data by month\n",
    "  - Processes JSON metadata for insulin types\n",
    "  - Calculates usage percentages for different insulin types\n",
    "\n",
    "2. **Insulin Classification**\n",
    "  - Novorapid (rapid-acting insulin)\n",
    "  - Levemir (long-acting insulin)\n",
    "  - Other insulin types\n",
    "  - Unspecified entries\n",
    "\n",
    "## Visualisation\n",
    "The function generates two complementary plots:\n",
    "1. **Stacked Area Chart**\n",
    "  - Shows relative distribution of insulin types\n",
    "  - Uses consistent color coding (Orange: Novorapid, Green: Levemir)\n",
    "  - Reveals changes in recording practices\n",
    "\n",
    "2. **Entry Count Timeline**\n",
    "  - Displays total monthly entries\n",
    "  - Helps identify data collection gaps or inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5259542",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_insulin_over_time(df):\n",
    "    # Create a copy of the dataframe with monthly periods\n",
    "    df_monthly = df.copy()\n",
    "    df_monthly['month'] = df_monthly.index.to_period('M')\n",
    "    \n",
    "    # Function to process insulin types from JSON\n",
    "    def get_insulin_types(entry):\n",
    "        if pd.isna(entry) or entry == \"[]\":\n",
    "            return [\"Not Stated\"]\n",
    "        insulin_data = ast.literal_eval(entry)\n",
    "        return [record['insulin'] for record in insulin_data]\n",
    "    \n",
    "    # Create monthly statistics\n",
    "    monthly_stats = []\n",
    "    \n",
    "    # Get unique months and sort them\n",
    "    unique_months = sorted(df_monthly['month'].unique())\n",
    "    \n",
    "    for month in unique_months:\n",
    "        month_data = df_monthly[df_monthly['month'] == month]\n",
    "        \n",
    "        # Count insulin types for this month\n",
    "        insulin_counts = {'Novorapid': 0, 'Levemir': 0, 'Other': 0, 'Not Stated': 0}\n",
    "        total_entries = len(month_data)\n",
    "        \n",
    "        for entry in month_data['insulinJSON']:\n",
    "            insulin_types = get_insulin_types(entry)\n",
    "            for insulin_type in insulin_types:\n",
    "                if insulin_type == \"Novorapid\":\n",
    "                    insulin_counts['Novorapid'] += 1\n",
    "                elif insulin_type == \"Levemir\":\n",
    "                    insulin_counts['Levemir'] += 1\n",
    "                elif insulin_type == \"Not Stated\":\n",
    "                    insulin_counts['Not Stated'] += 1\n",
    "                else:\n",
    "                    insulin_counts['Other'] += 1\n",
    "        \n",
    "        # Calculate percentages\n",
    "        monthly_stats.append({\n",
    "            'month': str(month),  # Convert period to string\n",
    "            'Novorapid': (insulin_counts['Novorapid'] / total_entries) * 100,\n",
    "            'Levemir': (insulin_counts['Levemir'] / total_entries) * 100,\n",
    "            'Other': (insulin_counts['Other'] / total_entries) * 100,\n",
    "            'Not Stated': (insulin_counts['Not Stated'] / total_entries) * 100,\n",
    "            'total_entries': total_entries\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    stats_df = pd.DataFrame(monthly_stats)\n",
    "    \n",
    "    # Create figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(25, 15))\n",
    "    \n",
    "    # Stacked area chart\n",
    "    ax1.stackplot(range(len(stats_df)), \n",
    "                 [stats_df['Novorapid'].values, \n",
    "                  stats_df['Levemir'].values,\n",
    "                  stats_df['Other'].values, \n",
    "                  stats_df['Not Stated'].values],\n",
    "                 labels=['Novorapid', 'Levemir', 'Other', 'Not Stated'],\n",
    "                 colors=['#F26F21', '#35b57d', '#808080', '#D3D3D3'])  # Add specific colors\n",
    "\n",
    "                 \n",
    "    \n",
    "    # Set x-axis ticks and labels\n",
    "    ax1.set_xticks(range(len(stats_df)))\n",
    "    ax1.set_xticklabels(stats_df['month'], rotation=45)\n",
    "    ax1.set_title('Distribution of Insulin Types Over Time')\n",
    "    ax1.set_xlabel('Month')\n",
    "    ax1.set_ylabel('Percentage')\n",
    "    ax1.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    # Line plot of total entries\n",
    "    ax2.plot(range(len(stats_df)), stats_df['total_entries'].values, marker='o')\n",
    "    ax2.set_xticks(range(len(stats_df)))\n",
    "    ax2.set_xticklabels(stats_df['month'], rotation=45)\n",
    "    ax2.set_title('Total Number of Entries per Month')\n",
    "    ax2.set_xlabel('Month')\n",
    "    ax2.set_ylabel('Number of Entries')\n",
    "    \n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    return plt, stats_df\n",
    "\n",
    "# Usage:\n",
    "plot, insulin_counts = analyse_insulin_over_time(insulin_df)\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6e819d",
   "metadata": {},
   "source": [
    "# Data Validation: Raw Insulin Distribution\n",
    "\n",
    "Before proceeding with our cleaned and classified insulin data, we visualize the raw insulin entries to:\n",
    "1. Verify the range of insulin values\n",
    "2. Identify potential outliers\n",
    "3. Check temporal distribution of entries\n",
    "4. Validate our classification thresholds (8u for bolus, 15u for basal)\n",
    "\n",
    "The scatter plot provides a quick visual validation that our insulin classification function's assumptions align with the actual data patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f99adf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot 25 x 12\n",
    "plt.figure(figsize=(25, 12))\n",
    "\n",
    "plt.plot(insulin_df['insulin'], label='Insulin', marker='o', linestyle = 'None', color='#F26F21')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Insulin')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de36e2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot 25 x 12\n",
    "plt.figure(figsize=(25, 12))\n",
    "\n",
    "# Create masks for NovoRapid (bolus) and Levemir (basal)\n",
    "bolus_mask = insulin_df['bolus'] >0.0\n",
    "basal_mask = insulin_df['basal']> 0.0\n",
    "\n",
    "# Plot bolus (NovoRapid) points\n",
    "plt.plot(\n",
    "    np.array(insulin_df[bolus_mask].index),  # Convert index to NumPy array\n",
    "    np.array(insulin_df[bolus_mask]['insulin']),  # Convert insulin values to NumPy array\n",
    "    label='NovoRapid (Bolus)', \n",
    "    marker='o', \n",
    "    linestyle='None',  # No line connecting points\n",
    "    color='#F26F21'  # NovoRapid orange\n",
    ")\n",
    "\n",
    "# Plot basal (Levemir) points\n",
    "plt.plot(\n",
    "    np.array(insulin_df[basal_mask].index),  # Convert index to NumPy array\n",
    "    np.array(insulin_df[basal_mask]['insulin']),  # Convert insulin values to NumPy array\n",
    "    label='Levemir (Basal)', \n",
    "    marker='o', \n",
    "    linestyle='None',  # No line connecting points\n",
    "    color='#35b57d'  # Levemir green\n",
    ")\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Insulin')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b63baec",
   "metadata": {},
   "source": [
    "# Refined Insulin Dataset Structure\n",
    "\n",
    "After cleaning and classification, we isolate our two key insulin variables (bolus and basal) and examine the resulting dataset structure. This inspection helps:\n",
    "\n",
    "1. **Verify Data Processing**\n",
    "  - Confirm successful separation of insulin types\n",
    "  - Check data types and memory usage\n",
    "  - Review null values if any\n",
    "\n",
    "2. **Statistical Overview**\n",
    "  - Distribution of both insulin types\n",
    "  - Typical dosing ranges\n",
    "  - Identify any remaining outliers\n",
    "\n",
    "The .info() shows our dataframe structure while .describe() provides key statistical measures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4809d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "insulin_df = insulin_df[['bolus', 'basal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f62d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "insulin_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43424d91",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "insulin_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cce444",
   "metadata": {},
   "source": [
    "# Carbohydrate Data Preprocessing\n",
    "\n",
    "A straightforward but essential cleaning step for carbohydrate data. This function:\n",
    "1. Creates a safe copy of the original data\n",
    "2. Removes trivial carbohydrate entries (<1g)\n",
    "3. Maintains original timestamp indexing\n",
    "\n",
    "## Rationale\n",
    "- Entries below 1g are likely recording errors or insignificant for meal analysis\n",
    "- Clean dataset focuses on meaningful carbohydrate intake events\n",
    "- Preserves original data structure for later meal impact analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5cdbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_classify_carbs(df):\n",
    "    \n",
    "    # Create a copy to avoid altering the original dataframe\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Keep only rows where carbs is >= 1.0 grams\n",
    "    df_clean = df_clean[df_clean['carbs'] >= 1.0]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "carb_df = clean_classify_carbs(treatment_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c63e03c",
   "metadata": {},
   "source": [
    "# Refined Carbohydrate Dataset Structure\n",
    "\n",
    "After cleaning our carbohydrates dataset, we simply wish to keep the index and carbs column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360f99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns except carbohydrate\n",
    "carb_df = carb_df[['carbs']]\n",
    "carb_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01bf7b5",
   "metadata": {},
   "source": [
    "# Visual Validation: Carbohydrate Distribution\n",
    "\n",
    "Following our carbohydrate data cleaning, we visualize the temporal distribution and magnitude of carbohydrate entries to:\n",
    "\n",
    "1. Confirm removal of sub-1g entries\n",
    "2. Examine the range of carbohydrate values\n",
    "3. Check recording frequency and patterns\n",
    "4. Identify any potential outliers requiring attention\n",
    "\n",
    "The scatter plot representation helps validate our minimal cleaning approach and provides insight into meal recording patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41059bf0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Make plot 25 x 12\n",
    "plt.figure(figsize=(25, 12))\n",
    "\n",
    "plt.plot(carb_df['carbs'], label='Carbohydrates', marker='o', linestyle = 'None')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Carbohydrates')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd76cb56",
   "metadata": {},
   "source": [
    "# Carbohydrate Dataset Summary\n",
    "\n",
    "After basic cleaning, we examine the carbohydrate dataset structure and statistics to:\n",
    "\n",
    "1. **Verify Dataset Properties**\n",
    "  - Confirm single carbohydrate column retention\n",
    "  - Check data type consistency\n",
    "  - Review number of recorded meals\n",
    "\n",
    "2. **Statistical Distribution**\n",
    "  - Typical meal carbohydrate content\n",
    "  - Range of carbohydrate values\n",
    "  - Help validate typical meal size assumptions\n",
    "\n",
    "The .info() provides structure verification while .describe() shows the statistical distribution of meal carbohydrates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f93302",
   "metadata": {},
   "outputs": [],
   "source": [
    "carb_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdd6bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "carb_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e897c3",
   "metadata": {},
   "source": [
    "# Blood Glucose Data Preprocessing\n",
    "\n",
    "This cleaning function prepares blood glucose data for analysis by implementing essential clinical and analytical standards:\n",
    "\n",
    "## Cleaning Steps\n",
    "1. **Unit Standardization**\n",
    "  - Renames 'calculated_value' to 'mg_dl' for clarity\n",
    "  - Creates 'mmol_l' column (standard international units)\n",
    "  - Conversion factor: mg/dL × 0.0555 = mmol/L\n",
    "\n",
    "2. **Clinical Range Enforcement**\n",
    "  - Lower bound: 39.64 mg/dL (2.2 mmol/L)\n",
    "  - Upper bound: 360.36 mg/dL (20.0 mmol/L)\n",
    "  - Values outside range clipped to these limits\n",
    "\n",
    "3. **Data Simplification**\n",
    "  - Retains only essential columns\n",
    "  - Preserves timestamp index\n",
    "  - Maintains dual unit representation\n",
    "\n",
    "## Rationale\n",
    "- Range limits reflect typical CGM sensor capabilities\n",
    "- Dual units support international analysis\n",
    "- Simplified structure focuses on key metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61045ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_glucose(df):\n",
    "    \n",
    "    # Create copy to avoid altering original dataframe\n",
    "    clean_df = df.copy()\n",
    "    \n",
    "    # Rename the 'calculated_value' column to 'mg_dl'\n",
    "    clean_df.rename(columns={'calculated_value': 'mg_dl'}, inplace=True)\n",
    "    \n",
    "    # Limit the 'mg_dl' column values to the range 39.64 to 360.36\n",
    "    clean_df['mg_dl'] = clean_df['mg_dl'].clip(lower=39.64, upper=360.36)\n",
    "\n",
    "    \n",
    "    # Create a new column 'mmol_l' by converting 'calculated_value' from mg/dL to mmol/L\n",
    "    clean_df['mmol_l'] = clean_df['mg_dl'] * 0.0555\n",
    "    \n",
    "    # Drop all rows except mg_dl and mmol_l\n",
    "    clean_df = clean_df[['mg_dl', 'mmol_l' ]]\n",
    "    \n",
    "    return clean_df\n",
    "\n",
    "glucose_df = clean_glucose(bg_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c798897",
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3338b620",
   "metadata": {},
   "source": [
    "# Blood Glucose Data Visualization\n",
    "\n",
    "We examine the glucose data through two complementary visualizations to validate our cleaning process and understand data patterns:\n",
    "\n",
    "## Full Dataset Scatter Plot\n",
    "- Shows complete temporal distribution\n",
    "- Validates clipping of extreme values\n",
    "- Reveals overall data density\n",
    "- Highlights any major gaps\n",
    "\n",
    "## Recent 90-Day Time Series\n",
    "- Focused view of latest 90 days (25,920 points at 5-min intervals)\n",
    "- Connected line plot shows glucose trends\n",
    "- More detailed examination of recent monitoring\n",
    "- Better visualization of daily patterns\n",
    "\n",
    "The contrast between these views helps validate both long-term data collection and recent monitoring quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6cf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make plot 25 x 12\n",
    "plt.figure(figsize=(25, 12))\n",
    "\n",
    "plt.plot(glucose_df['mmol_l'], label='mmol/L', marker='o', linestyle = 'None')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Blood Glucose (mmol/L)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2784b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display most recent 90 days of glucose readings in mmol/L\n",
    "last_90 = glucose_df[-25920:]\n",
    "\n",
    "# Make plot 25 x 12\n",
    "plt.figure(figsize=(25, 12))\n",
    "\n",
    "plt.plot(last_90['mmol_l'], label='mmol/L')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.title('Glucose readings in mmol/L - Most recent 90 Days')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Blood Glucose (mmol/L)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a908150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display most recent 48 Hours of glucose readings in mmol/L\n",
    "last_48h = glucose_df[-576:]\n",
    "\n",
    "# Make plot 25 x 12\n",
    "plt.figure(figsize=(25, 12))\n",
    "\n",
    "plt.plot(last_48h['mmol_l'], label='mmol/L')\n",
    "\n",
    "# Add labels, title, and legend\n",
    "plt.title('Glucose readings in mmol/L - Most recent 48 Hours')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Blood Glucose (mmol/L)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182a9c02",
   "metadata": {},
   "source": [
    "We can see that the last 48 hour graph shows messy lines especially around 3 am, this could be due to incosistent gap sizes causing the lines to dilate and contract depending on the size of the gap. We will visualise this again after aligning our datasets to regular 5-minute intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d565ddc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afadc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b89cd63",
   "metadata": {},
   "source": [
    "# Dataset Overview: Final Check\n",
    "\n",
    "Before proceeding with dataset alignment, we perform a final verification of our three core datasets. This check ensures:\n",
    "\n",
    "1. **Data Dimensions**\n",
    "  - Number of records in each dataset\n",
    "  - Column confirmations\n",
    "  - Memory usage\n",
    "\n",
    "2. **Time Range Verification**\n",
    "  - All datasets properly indexed by timestamp\n",
    "  - Presence of complete data columns\n",
    "  - No unexpected alterations from cleaning\n",
    "\n",
    "3. **Data Completeness**\n",
    "  - Blood glucose monitoring frequency\n",
    "  - Meal (carbohydrate) records\n",
    "  - Insulin treatment entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b8d0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ba3161",
   "metadata": {},
   "outputs": [],
   "source": [
    "carb_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2b6815",
   "metadata": {},
   "outputs": [],
   "source": [
    "insulin_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4a646d",
   "metadata": {},
   "source": [
    "# Data Alignment and Synthesis\n",
    "\n",
    "After cleaning individual datasets, we now combine them into a unified timeframe with consistent 5-minute intervals. This critical step creates our foundation for meal impact analysis.\n",
    "\n",
    "## Alignment Process\n",
    "1. **Timestamp Standardization**\n",
    "  - Rounds all timestamps to nearest 5-minute mark\n",
    "  - Creates uniform timeline across all data types\n",
    "  - Spans from earliest to latest record\n",
    "\n",
    "2. **Data Integration**\n",
    "  - Blood Glucose: Averaged within intervals\n",
    "  - Carbohydrates: Summed within intervals\n",
    "  - Insulin: Bolus and basal summed separately\n",
    "\n",
    "3. **Quality Assurance**\n",
    "  - Shape verification\n",
    "  - Column completeness\n",
    "  - Statistical distribution check\n",
    "\n",
    "The resulting dataset provides:\n",
    "- Complete timeline with no gaps\n",
    "- Aligned treatment and response data\n",
    "- Missing value handling appropriate to data type\n",
    " * Glucose: NaN preserved\n",
    " * Treatments: 0 for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae15493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_diabetes_data(\n",
    "    bg_df: pd.DataFrame,\n",
    "    carb_df: pd.DataFrame,\n",
    "    insulin_df: pd.DataFrame\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aligns diabetes data (blood glucose, carbohydrates, insulin) to regular 5-minute intervals.\n",
    "    \n",
    "    This function takes three dataframes with timestamped diabetes data and produces a single\n",
    "    dataframe with regular 5-minute intervals. Blood glucose readings are averaged within\n",
    "    each interval, while carbohydrates and insulin entries are summed.\n",
    "    \n",
    "    Args:\n",
    "        bg_df: DataFrame with timestamp index and columns ['mg_dl', 'mmol_l']\n",
    "              Blood glucose readings at irregular intervals\n",
    "        carb_df: DataFrame with timestamp index and column ['carbs']\n",
    "                Sporadic carbohydrate entries\n",
    "        insulin_df: DataFrame with timestamp index and columns ['bolus', 'basal']\n",
    "                   Sporadic insulin entries\n",
    "    \n",
    "    Processing steps:\n",
    "    1. Round all timestamps to nearest 5-minute interval\n",
    "    2. Create complete timeline at 5-minute intervals from earliest to latest data point\n",
    "    3. Resample BG data - average any readings within each 5-min window\n",
    "    4. Resample treatment data - sum any entries within each 5-min window\n",
    "    5. Combine all data and ensure all intervals exist\n",
    "    6. Fill missing treatment values with 0 (keep BG as NaN)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with:\n",
    "        - Regular 5-minute interval index\n",
    "        - Columns: ['mg_dl', 'mmol_l', 'carbs', 'bolus', 'basal']\n",
    "        - BG values averaged within intervals, NaN where missing\n",
    "        - Treatment values summed within intervals, 0 where missing\n",
    "    \"\"\"\n",
    "    # First round timestamps in all dataframes to 5-min intervals\n",
    "    bg_df = bg_df.copy()\n",
    "    carb_df = carb_df.copy()\n",
    "    insulin_df = insulin_df.copy()\n",
    "    \n",
    "    bg_df.index = bg_df.index.round('5min')\n",
    "    carb_df.index = carb_df.index.round('5min')\n",
    "    insulin_df.index = insulin_df.index.round('5min')\n",
    "    \n",
    "    \n",
    "    # Create complete timeline of 5-min intervals\n",
    "    full_range = pd.date_range(\n",
    "        start=min(bg_df.index.min(), carb_df.index.min(), insulin_df.index.min()),\n",
    "        end=max(bg_df.index.max(), carb_df.index.max(), insulin_df.index.max()),\n",
    "        freq='5min'\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # Resample blood glucose - average within windows\n",
    "    bg_resampled = bg_df.resample('5min').agg({\n",
    "        'mg_dl': 'mean',\n",
    "        'mmol_l': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Resample carbs - sum within windows\n",
    "    carb_resampled = carb_df.resample('5min').agg({\n",
    "        'carbs': 'sum'\n",
    "    })\n",
    "    \n",
    "    # Resample insulin - sum within windows\n",
    "    insulin_resampled = insulin_df.resample('5min').agg({\n",
    "        'bolus': 'sum',\n",
    "        'basal': 'sum'\n",
    "    })\n",
    "    \n",
    "    # Combine all data\n",
    "    aligned_df = pd.concat([bg_resampled, carb_resampled, insulin_resampled], axis=1)\n",
    "    \n",
    "    # Ensure all 5-minute intervals exist\n",
    "    aligned_df = aligned_df.reindex(full_range)\n",
    "    \n",
    "    # Fill missing treatment values with 0\n",
    "    aligned_df['carbs'] = aligned_df['carbs'].fillna(0)\n",
    "    aligned_df['bolus'] = aligned_df['bolus'].fillna(0)\n",
    "    aligned_df['basal'] = aligned_df['basal'].fillna(0)\n",
    "\n",
    "    \n",
    "    return aligned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f27769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = align_diabetes_data(glucose_df, carb_df, insulin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7397317",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627b53a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e707c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd86ea94",
   "metadata": {},
   "source": [
    "# Glucose Data Gap Analysis\n",
    "\n",
    "After alignment, we analyze the continuity of glucose monitoring data to identify and characterize monitoring gaps. This analysis is crucial for:\n",
    "\n",
    "## Gap Detection Process\n",
    "1. **Identification**\n",
    "  - Finds sequences of missing glucose values\n",
    "  - Records start and end times\n",
    "  - Calculates gap durations\n",
    "\n",
    "2. **Classification**\n",
    "  - Stores gap information in structured format\n",
    "  - Identifies longest gaps\n",
    "  - Creates human-readable duration format\n",
    "\n",
    "3. **Meal Impact Analysis**\n",
    "  - Checks for meals preceding gaps\n",
    "  - Flags gaps that might affect meal analysis\n",
    "  - 4-hour pre-gap meal window check\n",
    "\n",
    "## Key Outputs\n",
    "- Top N largest gaps with detailed timing\n",
    "- Total number of gaps\n",
    "- Complete gap inventory\n",
    "- Meal proximity warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d254feb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_glucose_gaps(aligned_df: pd.DataFrame, show_top_n: int = 10) -> dict:\n",
    "    \"\"\"\n",
    "    Analyzes gaps in glucose readings with detailed information about largest gaps.\n",
    "    \n",
    "    Args:\n",
    "        aligned_df: DataFrame with 5-min interval index and columns including\n",
    "                   'mg_dl' and 'mmol_l'\n",
    "        show_top_n: Number of largest gaps to show details for\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing gap analysis including detailed info about largest gaps\n",
    "    \"\"\"\n",
    "    missing_mask = aligned_df['mg_dl'].isna()\n",
    "    \n",
    "    gap_starts = []\n",
    "    gap_lengths = []\n",
    "    gap_ends = []  # Added to track end times\n",
    "    current_start = None\n",
    "    \n",
    "    for idx, (time, is_missing) in enumerate(missing_mask.items()):\n",
    "        if is_missing and current_start is None:\n",
    "            current_start = time\n",
    "        elif not is_missing and current_start is not None:\n",
    "            gap_starts.append(current_start)\n",
    "            gap_ends.append(time)  # Store end time\n",
    "            gap_lengths.append((time - current_start).total_seconds() / 60)\n",
    "            current_start = None\n",
    "    \n",
    "    # Handle case where dataset ends with a gap\n",
    "    if current_start is not None:\n",
    "        gap_starts.append(current_start)\n",
    "        gap_ends.append(missing_mask.index[-1])\n",
    "        gap_lengths.append((missing_mask.index[-1] - current_start).total_seconds() / 60)\n",
    "    \n",
    "    # Create DataFrame with gap details\n",
    "    gaps_df = pd.DataFrame({\n",
    "        'start_time': gap_starts,\n",
    "        'end_time': gap_ends,\n",
    "        'length_minutes': gap_lengths\n",
    "    })\n",
    "    \n",
    "    # Sort by gap length and get details of largest gaps\n",
    "    largest_gaps = gaps_df.sort_values('length_minutes', ascending=False).head(show_top_n)\n",
    "    \n",
    "    # Add human-readable duration\n",
    "    largest_gaps['duration'] = largest_gaps.apply(\n",
    "        lambda x: f\"{int(x['length_minutes'] // 60)}h {int(x['length_minutes'] % 60)}m\", \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'total_gaps': len(gaps_df),\n",
    "        'largest_gaps': largest_gaps,\n",
    "        'gaps_df': gaps_df\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298ab73c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def format_gaps_output(gaps_data):\n",
    "    \"\"\"\n",
    "    Pretty-prints the gaps data in a Jupyter Notebook.\n",
    "    \n",
    "    Args:\n",
    "        gaps_data (dict): Dictionary containing 'total_gaps', 'largest_gaps', and 'gaps_df'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    from IPython.display import display  # For better display in Jupyter Notebook\n",
    "    \n",
    "    # Extract data from the dictionary\n",
    "    total_gaps = gaps_data.get('total_gaps', 0)\n",
    "    largest_gaps = gaps_data.get('largest_gaps', None)\n",
    "    gaps_df = gaps_data.get('gaps_df', None)\n",
    "    \n",
    "    # Print total gaps\n",
    "    print(\"\\033[1mTotal Number of Gaps:\\033[0m\", total_gaps)  # Bold title\n",
    "    \n",
    "    # Display largest gaps if available\n",
    "    if largest_gaps is not None:\n",
    "        print(\"\\n\\033[1mTop 10 Largest Gaps:\\033[0m\")\n",
    "        display(largest_gaps.style.set_table_styles(\n",
    "            [{'selector': 'th', 'props': [('font-weight', 'bold')]}]  # Bold column headers\n",
    "        ).highlight_max(subset=['length_minutes'], color='orange'))  # Highlight largest gap\n",
    "    \n",
    "    # Display a preview of all gaps if available\n",
    "    if gaps_df is not None:\n",
    "        print(\"\\n\\033[1mAll Gaps (Summary - Top 10 Rows):\\033[0m\")\n",
    "        display(gaps_df.head(10).style.set_table_styles(\n",
    "            [{'selector': 'th', 'props': [('font-weight', 'bold')]}]\n",
    "        ).highlight_max(subset=['length_minutes'], color='orange'))\n",
    "\n",
    "    # Return the original dictionary for further use if needed\n",
    "    return gaps_data\n",
    "\n",
    "gap_analysis = analyse_glucose_gaps(combined_df)\n",
    "display_gaps = format_gaps_output(gap_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e92b81",
   "metadata": {},
   "source": [
    "# Glucose Monitoring Gap Analysis Results\n",
    "\n",
    "## Overview of Data Gaps\n",
    "Our analysis identified 5,743 total gaps in glucose monitoring, with durations ranging from 5 minutes to over 40 hours. This comprehensive gap analysis reveals several key patterns:\n",
    "\n",
    "## Gap Distribution\n",
    "1. **Major Gaps** (>4 hours):\n",
    "  - One exceptional gap of 40h 15m (Dec 20-22, 2023)\n",
    "  - One significant gap of 10h 55m (July 29-30, 2023)\n",
    "  - One moderate gap of 4h 10m (July 27, 2024)\n",
    "\n",
    "2. **Medium Gaps** (1-4 hours):\n",
    "  - Only two gaps in this range\n",
    "  - Maximum of 1h 55m (May 25, 2024)\n",
    "  - Minimum of 1h (Aug 23, 2023)\n",
    "\n",
    "3. **Minor Gaps** (<1 hour):\n",
    "  - Most common duration: 5 minutes\n",
    "  - Several gaps between 40-50 minutes\n",
    "  - Generally well-distributed across the monitoring period\n",
    "\n",
    "## Critical Observations\n",
    "1. **Meal Impact**\n",
    "  - 5 of the top 10 gaps have meals in the preceding 4 hours\n",
    "  - Most concerning for the 4h+ gaps affecting post-meal analysis\n",
    "\n",
    "2. **Data Quality**\n",
    "  - Majority of gaps (>5,000) are brief 5-minute interruptions\n",
    "  - Only 3 gaps exceed typical meal analysis window (4 hours)\n",
    "  - Overall excellent data continuity with few significant interruptions\n",
    "\n",
    "3. **Monitoring Patterns**\n",
    "  - No seasonal pattern in major gaps\n",
    "  - Even distribution of minor gaps suggests routine sensor changes\n",
    "  - Good recovery after interruptions\n",
    "\n",
    "This analysis supports our decision to proceed with meal impact analysis, as the vast majority of gaps are brief and manageable with our chosen interpolation approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adacbc15",
   "metadata": {},
   "source": [
    "# Meal Analysis Configuration and Quality Assessment\n",
    "\n",
    "## Data Quality Framework\n",
    "We implement a structured approach for assessing meal data quality and handling glucose data gaps, with clear classification criteria and configurable parameters.\n",
    "\n",
    "## Quality Categories\n",
    "1. **Clean Meals**\n",
    "   - Perfect data quality\n",
    "   - No gaps in glucose readings\n",
    "   - Ideal for analysis\n",
    "\n",
    "2. **Usable Meals**\n",
    "   - Small gaps (≤15 minutes)\n",
    "   - Missing data ≤10%\n",
    "   - Suitable for analysis after interpolation\n",
    "\n",
    "3. **Borderline Meals**\n",
    "   - Medium gaps (≤25 minutes)\n",
    "   - Missing data ≤20%\n",
    "   - Use with caution\n",
    "\n",
    "4. **Unusable Meals**\n",
    "   - Large gaps (>25 minutes)\n",
    "   - Missing data >20%\n",
    "   - Exclude from analysis\n",
    "\n",
    "## Configuration Parameters\n",
    "- Minimum meal size: 20g carbohydrates\n",
    "- Post-meal analysis window: 4 hours\n",
    "- Maximum interpolation: 5 consecutive readings (25 minutes)\n",
    "- Clear thresholds for gap classification\n",
    "- Configurable quality criteria\n",
    "\n",
    "## Processing Workflow\n",
    "1. Gap Analysis\n",
    "   - Identifies missing data patterns\n",
    "   - Calculates gap durations\n",
    "   - Determines data completeness\n",
    "\n",
    "2. Quality Assessment\n",
    "   - Evaluates each meal period\n",
    "   - Assigns quality categories\n",
    "   - Marks meals for inclusion/exclusion\n",
    "\n",
    "3. Data Enhancement\n",
    "   - Interpolates acceptable gaps\n",
    "   - Marks interpolated values\n",
    "   - Preserves data integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5986a103",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MealQuality(Enum):\n",
    "    \"\"\"Enum for meal analysis quality categories\"\"\"\n",
    "    CLEAN = \"Clean\"          # No gaps\n",
    "    USABLE = \"Usable\"        # Small gaps only, ≤10% missing\n",
    "    BORDERLINE = \"Borderline\"  # Medium gaps, ≤20% missing\n",
    "    UNUSABLE = \"Unusable\"    # Large gaps or >20% missing\n",
    "\n",
    "@dataclass\n",
    "class MealAnalysisConfig:\n",
    "    \"\"\"Configuration parameters for meal analysis and interpolation\"\"\"\n",
    "    post_meal_hours: int = 4\n",
    "    usable_gap_mins: int = 15\n",
    "    max_gap_mins: int = 25\n",
    "    usable_missing_pct: float = 0.10\n",
    "    max_missing_pct: float = 0.20\n",
    "    min_carbs: float = 20.0  # Minimum carbs to consider as meal\n",
    "    interpolation_limit: int = 5  # Maximum points to interpolate (25 mins at 5-min intervals)\n",
    "\n",
    "def analyse_meal_data(aligned_df: pd.DataFrame, config: MealAnalysisConfig = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyses diabetes data for meal analysis suitability and handles missing data interpolation, using \n",
    "    MealAnalysisConfig variables if not overwritten and interpolates wheres suitable.\n",
    "    \n",
    "    Processes the data in these steps:\n",
    "    1. Analyses each meal period (4h post-meal by default)\n",
    "    2. Categorises meal quality based on missing data patterns\n",
    "    3. Interpolates missing glucose values within acceptable gaps\n",
    "    4. Marks quality and interpolation status\n",
    "    \n",
    "    Args:\n",
    "        aligned_df: DataFrame with 5-min aligned diabetes data\n",
    "                   Must have columns: mg_dl, mmol_l, carbs\n",
    "        config: Optional MealAnalysisConfig object with analysis parameters\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with:\n",
    "        - Interpolated glucose values where appropriate\n",
    "        - Additional columns:\n",
    "            - has_missing_data: Boolean, True if any missing BG in post-meal period\n",
    "            - gap_duration_mins: Maximum gap duration in post-meal period\n",
    "            - missing_pct: Percentage of missing readings in post-meal period\n",
    "            - meal_quality: Category from MealQuality enum\n",
    "            - skip_meal: Boolean, True if meal should be excluded from analysis\n",
    "            - interpolated: Boolean, True if value was interpolated\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = MealAnalysisConfig()\n",
    "    \n",
    "    # Create copy to avoid modifying original\n",
    "    df = aligned_df.copy()\n",
    "    \n",
    "    # Initialize new columns\n",
    "    df['has_missing_data'] = False\n",
    "    df['gap_duration_mins'] = 0.0\n",
    "    df['missing_pct'] = 0.0\n",
    "    df['meal_quality'] = None\n",
    "    df['skip_meal'] = False\n",
    "    df['interpolated'] = False\n",
    "    \n",
    "    # Number of 5-min intervals in post-meal period\n",
    "    intervals = config.post_meal_hours * 12\n",
    "    \n",
    "    # First pass: Analyze gaps and mark meal quality\n",
    "    meal_rows = df[df['carbs'] > config.min_carbs].index\n",
    "    \n",
    "    for meal_time in meal_rows:\n",
    "        # Get post-meal period\n",
    "        end_time = meal_time + pd.Timedelta(hours=config.post_meal_hours)\n",
    "        post_meal = df[meal_time:end_time]\n",
    "        \n",
    "        # Skip if we don't have enough future data\n",
    "        if len(post_meal) < intervals:\n",
    "            df.loc[meal_time, 'skip_meal'] = True\n",
    "            df.loc[meal_time, 'meal_quality'] = MealQuality.UNUSABLE.value\n",
    "            continue\n",
    "        \n",
    "        # Analyze missing data\n",
    "        missing_mask = post_meal['mg_dl'].isna()\n",
    "        df.loc[meal_time, 'has_missing_data'] = missing_mask.any()\n",
    "        \n",
    "        # Calculate missing percentage\n",
    "        missing_pct = missing_mask.mean()\n",
    "        df.loc[meal_time, 'missing_pct'] = missing_pct * 100\n",
    "        \n",
    "        # Find longest gap\n",
    "        gap_duration = 0\n",
    "        current_gap = 0\n",
    "        \n",
    "        for is_missing in missing_mask:\n",
    "            if is_missing:\n",
    "                current_gap += 5  # 5-minute intervals\n",
    "                gap_duration = max(gap_duration, current_gap)\n",
    "            else:\n",
    "                current_gap = 0\n",
    "        \n",
    "        df.loc[meal_time, 'gap_duration_mins'] = gap_duration\n",
    "        \n",
    "        # Determine meal quality\n",
    "        if not missing_mask.any():\n",
    "            quality = MealQuality.CLEAN\n",
    "            skip = False\n",
    "        elif gap_duration <= config.usable_gap_mins and missing_pct <= config.usable_missing_pct:\n",
    "            quality = MealQuality.USABLE\n",
    "            skip = False\n",
    "        elif gap_duration <= config.max_gap_mins and missing_pct <= config.max_missing_pct:\n",
    "            quality = MealQuality.BORDERLINE\n",
    "            skip = False\n",
    "        else:\n",
    "            quality = MealQuality.UNUSABLE\n",
    "            skip = True\n",
    "        \n",
    "        df.loc[meal_time, 'meal_quality'] = quality.value\n",
    "        df.loc[meal_time, 'skip_meal'] = skip\n",
    "    \n",
    "    # Second pass: Interpolate missing data where appropriate\n",
    "    # Only interpolate for non-skipped meals\n",
    "    usable_meals = df[\n",
    "        (df['carbs'] > config.min_carbs) & \n",
    "        (~df['skip_meal'])\n",
    "    ].index\n",
    "    \n",
    "    for meal_time in usable_meals:\n",
    "        end_time = meal_time + pd.Timedelta(hours=config.post_meal_hours)\n",
    "        post_meal_idx = df[meal_time:end_time].index\n",
    "        \n",
    "        # Interpolate glucose values\n",
    "        original_mg_dl = df.loc[post_meal_idx, 'mg_dl'].copy()\n",
    "        original_mmol_l = df.loc[post_meal_idx, 'mmol_l'].copy()\n",
    "        \n",
    "        # Use time-based interpolation with limit\n",
    "        df.loc[post_meal_idx, 'mg_dl'] = df.loc[post_meal_idx, 'mg_dl'].interpolate(\n",
    "            method='time', \n",
    "            limit=config.interpolation_limit\n",
    "        )\n",
    "        df.loc[post_meal_idx, 'mmol_l'] = df.loc[post_meal_idx, 'mmol_l'].interpolate(\n",
    "            method='time', \n",
    "            limit=config.interpolation_limit\n",
    "        )\n",
    "        \n",
    "        # Mark interpolated values\n",
    "        df.loc[post_meal_idx, 'interpolated'] = (\n",
    "            df.loc[post_meal_idx, 'mg_dl'].notna() & \n",
    "            original_mg_dl.isna()\n",
    "        )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ecdf2",
   "metadata": {},
   "source": [
    "# Meal Quality Statistics Generation\n",
    "\n",
    "This function computes comprehensive statistics about our meal dataset quality, providing key metrics for assessing data reliability and processing effectiveness.\n",
    "\n",
    "## Generated Statistics\n",
    "The function calculates essential metrics across several dimensions:\n",
    "\n",
    "1. **Volume Metrics**\n",
    "  - Total number of meals (>20g carbs)\n",
    "  - Counts per quality category\n",
    "  - Percentage distribution across categories\n",
    "\n",
    "2. **Quality Indicators**\n",
    "  - Average percentage of missing data\n",
    "  - Average gap duration\n",
    "  - Proportion of usable meals\n",
    "  - Number and percentage of interpolated points\n",
    "\n",
    "## Statistical Output Format\n",
    "Returns a dictionary containing:\n",
    "- Raw counts and percentages\n",
    "- Data completeness metrics\n",
    "- Interpolation impact measures\n",
    "- All metrics aligned with MealAnalysisConfig parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82433cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_meal_statistics(df: pd.DataFrame, config: MealAnalysisConfig = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Calculates statistics about meal quality and data completeness.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with meal analysis columns\n",
    "        config: MealAnalysisConfig object to ensure consistent carb threshold\n",
    "               If None, uses default config\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary containing:\n",
    "        - Total meals analyzed\n",
    "        - Counts and percentages for each meal quality category\n",
    "        - Average missing data percentage\n",
    "        - Distribution of gap durations\n",
    "        - Interpolation statistics\n",
    "    \"\"\"\n",
    "    if config is None:\n",
    "        config = MealAnalysisConfig()\n",
    "        \n",
    "    meals = df[df['carbs'] > config.min_carbs]\n",
    "    \n",
    "    stats = {\n",
    "        'total_meals': len(meals),\n",
    "        'quality_counts': meals['meal_quality'].value_counts().to_dict(),\n",
    "        'quality_percentages': meals['meal_quality'].value_counts(normalize=True).multiply(100).to_dict(),\n",
    "        'avg_missing_pct': meals['missing_pct'].mean(),\n",
    "        'avg_gap_duration': meals['gap_duration_mins'].mean(),\n",
    "        'usable_meals_pct': meals[~meals['skip_meal']].shape[0] / len(meals) * 100,\n",
    "        'interpolated_points': df['interpolated'].sum(),\n",
    "        'interpolated_pct': (df['interpolated'].sum() / len(df)) * 100\n",
    "    }\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc57b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_meal_statistics(stats: Dict):\n",
    "    \"\"\"\n",
    "    Pretty-prints meal statistics in a Jupyter Notebook.\n",
    "    \n",
    "    Args:\n",
    "        stats: Dictionary returned by the get_meal_statistics function.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Create bold headers for clarity\n",
    "    print(\"\\033[1mMeal Analysis Statistics:\\033[0m\\n\")\n",
    "    \n",
    "    # Total meals and averages\n",
    "    print(f\"\\033[1mTotal Meals Analyzed:\\033[0m {stats['total_meals']}\")\n",
    "    print(f\"\\033[1mAverage Missing Data Percentage:\\033[0m {stats['avg_missing_pct']:.2f}%\")\n",
    "    print(f\"\\033[1mAverage Gap Duration:\\033[0m {stats['avg_gap_duration']:.2f} minutes\")\n",
    "    print(f\"\\033[1mUsable Meals Percentage:\\033[0m {stats['usable_meals_pct']:.2f}%\")\n",
    "    \n",
    "    # Quality counts and percentages\n",
    "    print(\"\\n\\033[1mMeal Quality Distribution:\\033[0m\")\n",
    "    quality_df = pd.DataFrame({\n",
    "        'Quality': stats['quality_counts'].keys(),\n",
    "        'Count': stats['quality_counts'].values(),\n",
    "        'Percentage (%)': stats['quality_percentages'].values()\n",
    "    })\n",
    "    quality_df = quality_df.sort_values('Count', ascending=False)\n",
    "    \n",
    "    # Apply color to the Count column by using a custom color function\n",
    "    def color_count(val):\n",
    "        if val < 5:\n",
    "            color = 'lightpink'\n",
    "        elif val < 10:\n",
    "            color = 'lightblue'\n",
    "        else:\n",
    "            color = 'lightgreen'\n",
    "        return f'background-color: {color}'\n",
    "\n",
    "    # Apply the color function to the 'Count' column\n",
    "    quality_df_styled = quality_df.style.map(color_count, subset=['Percentage (%)'])\n",
    "    \n",
    "    # Format Percentage column with 2 decimal places\n",
    "    quality_df_styled = quality_df_styled.format({'Percentage (%)': '{:.2f}%'})\n",
    "    \n",
    "    # Display the styled DataFrame\n",
    "    display(quality_df_styled.set_table_styles(\n",
    "        [{'selector': 'th', 'props': [('font-weight', 'bold')]}]\n",
    "    ))\n",
    "\n",
    "    # Interpolation statistics\n",
    "    print(\"\\n\\033[1mInterpolation Statistics:\\033[0m\")\n",
    "    interpolation_df = pd.DataFrame({\n",
    "        'Metric': ['Interpolated Points', 'Interpolated Percentage'],\n",
    "        'Value': [stats['interpolated_points'], f\"{stats['interpolated_pct']:.2f}%\"]\n",
    "    })\n",
    "    display(interpolation_df.style.set_table_styles(\n",
    "        [{'selector': 'th', 'props': [('font-weight', 'bold')]}]\n",
    "    ).hide(axis=\"index\"))\n",
    "\n",
    "    print(\"\\n\\033[1mAll statistics displayed.\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca12d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysed_df = analyse_meal_data(combined_df) # Run meal quality analysis\n",
    "stats = get_meal_statistics(analyzed_df)     # Create analysis statistics\n",
    "display_quality = format_meal_statistics(stats) # Format and print statistics\n",
    "display_quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb4ec41",
   "metadata": {},
   "source": [
    "# Meal Analysis Results Summary\n",
    "\n",
    "## Dataset Overview\n",
    "Analysis of 1,628 meals (>20g carbs) reveals excellent data quality with high usability for gastroparesis screening:\n",
    "\n",
    "## Quality Distribution\n",
    "- **Clean Meals**: 170 (10.44%)\n",
    " * Perfect data quality\n",
    " * No interpolation needed\n",
    "- **Usable Meals**: 1,309 (80.41%)\n",
    " * Primary analysis dataset\n",
    " * Minor gaps successfully interpolated\n",
    "- **Borderline Meals**: 137 (8.42%)\n",
    " * Usable with caution\n",
    " * Higher but acceptable gap rates\n",
    "- **Unusable Meals**: 12 (0.74%)\n",
    " * Excluded from analysis\n",
    " * Excessive missing data\n",
    "\n",
    "## Data Quality Metrics\n",
    "1. **Gap Characteristics**\n",
    "  - Average gap duration: 4.90 minutes\n",
    "  - Average missing data: 4.55%\n",
    "  - Both well within acceptable limits\n",
    "\n",
    "2. **Interpolation Impact**\n",
    "  - 1,808 points interpolated\n",
    "  - Only 1.30% of total readings\n",
    "  - Minimal data manipulation required\n",
    "\n",
    "3. **Overall Usability**\n",
    "  - 99.26% of meals usable for analysis\n",
    "  - Strong foundation for gastroparesis screening\n",
    "  - High confidence in data integrity\n",
    "\n",
    "These results indicate exceptional data quality with minimal need for interpolation, providing a robust dataset for subsequent analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f90bfa5",
   "metadata": {},
   "source": [
    "# Dataset Structure and Statistical Summary\n",
    "\n",
    "## Dataset Properties Analysis\n",
    "After meal quality analysis and interpolation, let's examine our complete dataset structure and statistical distribution.\n",
    "\n",
    "## Data Structure\n",
    "Let's examine:\n",
    "1. Total records and memory usage\n",
    "2. Column data types\n",
    "3. Non-null counts\n",
    "4. Index structure and timeline coverage\n",
    "\n",
    "## Statistical Distribution\n",
    "The describe() output provides key statistics for each column:\n",
    "1. Central tendency (mean, std)\n",
    "2. Range and quartiles\n",
    "3. Treatment frequency and size\n",
    "4. Glucose value distribution\n",
    "\n",
    "This verification ensures our processing maintained data integrity while improving usability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67578661",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysed_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b7464",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysed_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c48139",
   "metadata": {},
   "source": [
    "# Diabetes Data Processing and Meal Analysis Summary\n",
    "\n",
    "## Initial Data Processing\n",
    "We started with three separate dataframes extracted from xDrip+ CGM management software:\n",
    "- Blood glucose readings (mg/dl and mmol/l) at approximately 5-minute intervals\n",
    "- Carbohydrate intake records\n",
    "- Insulin (bolus and basal) records processed from treatment logs\n",
    "\n",
    "## Data Alignment and Quality Analysis\n",
    "1. Aligned all data to regular 5-minute intervals\n",
    "2. Created a meal analysis framework with configurable parameters:\n",
    "  - 4-hour post-meal analysis windows\n",
    "  - Minimum 20g carbs to consider as significant meal\n",
    "  - Maximum gap tolerance of 25 minutes\n",
    "  - Maximum 20% missing data allowed\n",
    "  - Interpolation limit of 25 minutes (5 readings)\n",
    "\n",
    "## Quality Metrics and Results\n",
    "Our analysis of 1,628 significant meals (>20g carbs) shows:\n",
    "- 170 Clean meals (10.44%): Perfect data, no gaps\n",
    "- 1,309 Usable meals (80.41%): Small gaps only, ≤10% missing\n",
    "- 137 Borderline meals (8.42%): Medium gaps, ≤20% missing\n",
    "- Only 12 Unusable meals (0.74%): Large gaps or excessive missing data\n",
    "\n",
    "The data quality is excellent:\n",
    "- 99.26% of meals are usable for analysis\n",
    "- Average missing data is only 4.55%\n",
    "- Average gap duration is 4.90 minutes\n",
    "- Only 1.30% of points needed interpolation (1,808 points)\n",
    "\n",
    "## New Dataset Features\n",
    "The processed dataframe includes additional columns crucial for gastroparesis screening:\n",
    "- has_missing_data: Flags any missing BG in 4h post-meal, crucial for response curve integrity\n",
    "- gap_duration_mins: Longest gap in post-meal period, important for meal absorption analysis\n",
    "- missing_pct: Percentage of missing readings, validates data quality\n",
    "- meal_quality: Clean/Usable/Borderline/Unusable, enables filtered analysis\n",
    "- skip_meal: Boolean for excluding from analysis, maintains data quality standards\n",
    "- interpolated: Marks interpolated values, ensures transparency in analysis\n",
    "\n",
    "## Important Notes\n",
    "1. All original data is preserved - no rows have been removed\n",
    "2. Small gaps have been interpolated where appropriate\n",
    "3. Quality markers allow easy filtering for analysis\n",
    "4. 20g carbohydrate threshold ensures significant meal impacts\n",
    "5. Data structure supports comprehensive meal response analysis\n",
    "\n",
    "## Next Steps\n",
    "While the data is now well-organized and quality-assessed, specific gastroparesis analysis may require:\n",
    "- Meal size stratification analysis (varying carbohydrate loads)\n",
    "- Time-of-day impact assessment on gastric emptying\n",
    "- Insulin timing relationship with meal absorption\n",
    "- Sensor type and calibration effects on readings\n",
    "- Additional meal characteristic analysis\n",
    "- Specific time window selections for different meal types\n",
    "- Additional derived features for absorption patterns\n",
    "- Custom filtering based on research requirements\n",
    "\n",
    "This processed dataset provides a robust foundation for investigating gastric motility patterns through post-meal glucose responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8eb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframe to CSV file for future use\n",
    "analysed_df.to_csv('data/processed_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a90dbc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
