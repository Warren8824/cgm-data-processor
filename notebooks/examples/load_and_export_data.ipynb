{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd97948c-84b0-49f5-a1f1-a633da5dab84",
   "metadata": {},
   "source": [
    "# CGM Data Quality Analysis Tutorial\n",
    "## Using cgm-data-processor with XDrip+ Backups\n",
    "\n",
    "This notebook demonstrates a practical workflow for processing Continuous Glucose Monitoring (CGM) data using the cgm-data-processor tool. We'll walk through loading data from an XDrip+ SQLite backup, performing quality assessments, and exporting the processed data in a standardized format suitable for further analysis.\n",
    "\n",
    "### Overview\n",
    "\n",
    "The cgm-data-processor tool simplifies the process of working with CGM data by handling common preprocessing tasks and standardizing the output format. This example focuses on three key aspects:\n",
    "\n",
    "1. Data Loading: Extracting CGM measurements, carbohydrate records, and insulin data from an XDrip+ SQLite backup\n",
    "2. Quality Assessment: Evaluating data completeness, identifying gaps, and assessing measurement reliability\n",
    "3. Standardized Export: Saving the processed data in a consistent CSV format that facilitates further analysis\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "- The cgm-data-processor package installed\n",
    "- An XDrip+ SQLite backup file\n",
    "- Basic familiarity with Python and pandas\n",
    "\n",
    "### Expected Output\n",
    "\n",
    "The processed dataset will include:\n",
    "- Glucose measurements aligned to 5-minute intervals\n",
    "- Values in both mg/dL and mmol/L units\n",
    "- Validated carbohydrate and insulin records\n",
    "- Quality metrics for each time period\n",
    "- Clearly marked data gaps and interpolated values\n",
    "\n",
    "### Data Quality Considerations\n",
    "\n",
    "Throughout this tutorial, we'll examine several key quality metrics:\n",
    "- Measurement frequency and consistency\n",
    "- Gap duration and distribution\n",
    "- Sensor reliability indicators\n",
    "- Record completeness for insulin and carbohydrate data\n",
    "\n",
    "This quality assessment helps ensure that subsequent analyses are based on reliable data and that any limitations are well understood.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "After completing this tutorial, you'll have a standardized dataset ready for various analyses such as:\n",
    "- Glucose variability assessment\n",
    "- Meal response patterns\n",
    "- Insulin sensitivity calculations\n",
    "- Time-in-range analysis\n",
    "\n",
    "Let's begin by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa8e8cf4-d96c-4743-8f1a-bd93e3f3ebec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML, Image\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\")) # Make Jupyter cells wider for better visuals\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36caf1d7-33b7-4a37-88fc-acfe3e54b69d",
   "metadata": {},
   "source": [
    "## Project Setup and Module Imports\n",
    "\n",
    "This notebook relies on a modular codebase organized into three main components:\n",
    "\n",
    "1. Preprocessing: Handles data loading from XDrip+ backups, cleaning operations, and timeline alignment\n",
    "2. Analysis: Provides tools for assessing data quality, gap detection, and statistical analysis\n",
    "3. Visualization: Creates informative dashboards and plots for quality assessment\n",
    "\n",
    "The following code adds the project root to the Python path and imports the necessary functionality from each module. Each import is organized by its primary function to maintain clarity and facilitate future extensions of the codebase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f1737cc-8ef3-4b86-b1d0-813115c7512a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path modification used to allow Notebook access to src directory\n",
    "import os\n",
    "import sys\n",
    "notebook_path = os.path.abspath('.')\n",
    "project_root = os.path.join(notebook_path, '../../')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Preprocessing Module - Load, clean and align data\n",
    "from src.preprocessing.loading import XDrip\n",
    "from src.preprocessing.cleaning import clean_classify_insulin, clean_classify_carbs, clean_glucose\n",
    "from src.preprocessing.alignment import align_diabetes_data\n",
    "\n",
    "# Analysis Module - Check and display data quality\n",
    "from src.analysis.gaps import analyse_glucose_gaps\n",
    "from src.analysis.insulin import analyse_insulin_over_time\n",
    "from src.analysis.metrics import display_quality_metrics\n",
    "\n",
    "# Visualisation Module - Format data for visual appeal in Jupyter\n",
    "from src.visualisation.quality_dashboard import create_quality_dashboard\n",
    "from src.visualisation.meal_statistics_dashboard import create_meal_statistics_dashboard\n",
    "from src.visualisation.gap_dashboard import create_gap_dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1297d20d-1b46-4f21-a4b3-f33d04a860ae",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Here we initialize our data processing pipeline by loading an XDrip+ SQLite backup file. The `XDrip` class provides a clean interface for accessing and processing the raw CGM data.\n",
    "\n",
    "The SQLite backup file contains the complete dataset including glucose readings, insulin records, and carbohydrate entries. The `XDrip` class handles the low-level database interactions and provides structured access to this data.\n",
    "\n",
    "Note: When using your own data, replace the `db_path` with the path to your XDrip+ SQLite backup file. XDrip+ backups can be generated from within the XDrip+ application under Settings > Data Source > Export Database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17120955-aa27-4a85-8b05-d571809b058c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your SQLite file\n",
    "db_path = '../../data/export20240928-130349.sqlite'\n",
    "data = XDrip(db_path) # Load db path into XDrip class - Class found in src directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758a6d79-a22f-4955-b9ee-3a9a107d221c",
   "metadata": {},
   "source": [
    "## Initial Data Extraction\n",
    "\n",
    "In this step, we extract two primary datasets from the XDrip+ backup:\n",
    "\n",
    "1. Glucose Measurements (`bg_df`): This dataset contains continuous glucose monitoring readings, typically recorded at 5-minute intervals. Each reading includes the glucose value and associated metadata such as the timestamp and reading quality indicators.\n",
    "\n",
    "2. Treatment Records (`treatment_df`): This dataset encompasses both insulin administration and carbohydrate intake records. The timestamps in this dataset may be irregular as they correspond to specific events rather than continuous monitoring.\n",
    "\n",
    "Both dataframes are automatically configured with their timestamps as indices, and dropping any rows with duplicate timestamps, facilitating temporal analysis and alignment in subsequent processing steps. The `XDrip` class handles the SQL queries and initial data structuring, ensuring consistent data types and timestamp handling across the extracted datasets.\n",
    "\n",
    "Note: These raw dataframes will undergo further processing and quality assessment before being combined into our standardized format. This two-stage loading approach allows us to validate and clean each data type independently before integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbad67f1-f754-44b4-add7-2d0f6791d819",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_df = data.load_glucose_df() # Load glucose data into a pandas dataframe - Function found in src directory\n",
    "treatment_df = data.load_treatment_df() # Load treatment data into a pandas dataframe - Function found in src directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbdcd42-fa7c-449e-88b0-71425a160753",
   "metadata": {},
   "source": [
    "## Insulin Data Processing\n",
    "\n",
    "The `clean_classify_insulin()` function processes the raw treatment records to create a structured insulin dataset. This critical step separates insulin records from other treatments and applies standardization rules specific to insulin data. \n",
    "\n",
    "There are two optional parameters than can be supplied to this function:\n",
    "- bolus_limit - Number of units where insulin doses above this should be classified as basal, default = 8\n",
    "- max_limit - Number of units where user would suggest it must be an error to be discarded, default = 15\n",
    "\n",
    "The function handles several key aspects of insulin data processing:\n",
    "- Extracts insulin-specific records from the treatment dataset\n",
    "- Classifies insulin entries into basal and bolus categories - through meta-data or doseage\n",
    "- Validates dosage values and units\n",
    "- Standardizes timestamp formats\n",
    "- Removes any duplicate or invalid entries\n",
    "- Sets index to timestamp column\n",
    "\n",
    "The resulting `insulin_df` provides a clean, validated dataset of insulin records, split by basal vs bolus and with a flag to see if the data was labeled by the user, that is ready to be integrated into our final standardized format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f16f664e-7392-49d4-a3c5-883d1a90f2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "insulin_df = clean_classify_insulin(treatment_df) # Function in source directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50ef05d-050b-4fd1-8ed1-42216a67a8ff",
   "metadata": {},
   "source": [
    "## Carbohydrate Data Processing\n",
    "\n",
    "After processing insulin records, we now clean and standardize the carbohydrate data using the `clean_classify_carbs()` function. This function implements specific validation rules to ensure data quality and consistency:\n",
    "\n",
    "1. The function filters for meaningful carbohydrate entries by keeping only records with 1.0 grams or more, eliminating negligible or potentially erroneous entries.\n",
    "\n",
    "2. It handles duplicate timestamps by keeping only the first entry for any given time, which prevents double-counting of meals while preserving the earliest recorded entry.\n",
    "\n",
    "3. The resulting `carb_df` contains a simplified structure with just the essential carbohydrate quantities indexed by timestamp, making it ready for integration with our other standardized data streams.\n",
    "\n",
    "This cleaned carbohydrate dataset will be crucial for analyzing meal-related glucose responses and understanding overall patterns in carbohydrate intake alongside glucose measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1714dde-0693-403b-8e27-5c87fee2bfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "carb_df = clean_classify_carbs(treatment_df) # Function in source directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d55d887-0e07-4027-823b-14d261b358af",
   "metadata": {},
   "source": [
    "## Glucose Data Processing and Standardization\n",
    "\n",
    "The `clean_glucose()` function performs comprehensive processing of raw CGM data to create a standardized, analysis-ready glucose dataset with consistent time intervals and validated measurements.\n",
    "\n",
    "### Temporal Standardization\n",
    "The function first standardizes the temporal aspects of the data by rounding timestamps to 5-minute intervals, which is the standard measurement frequency for most CGM systems. It then creates a complete timeline by generating a continuous 5-minute interval index spanning the entire monitoring period. This ensures we have a consistent temporal structure, even when raw data points are missing or irregularly spaced.\n",
    "\n",
    "### Gap Handling and Data Quality\n",
    "A key feature of the processing is its sophisticated handling of data gaps. The function identifies and flags missing data points, creating a 'missing' indicator that allows downstream analyses to distinguish between measured and interpolated values. For gaps up to 20 minutes (four 5-minute intervals), the function applies linear interpolation to estimate glucose values. This approach balances the need for continuous data with the importance of maintaining data integrity.\n",
    "\n",
    "### Measurement Standardization\n",
    "The function implements several measurement quality controls:\n",
    "- Glucose values are constrained to a physiologically reasonable range of 39.64 to 360.36 mg/dL (2.2 to 20.0 mmol/L)\n",
    "- Measurements are provided in both mg/dL and mmol/L units using the standard conversion factor of 0.0555\n",
    "- Where multiple readings exist for a single 5-minute interval, they are averaged to provide a single representative value\n",
    "\n",
    "The resulting dataset includes three essential columns:\n",
    "1. Glucose measurements in mg/dL\n",
    "2. Parallel measurements in mmol/L\n",
    "3. Missing data flags to indicate interpolated values\n",
    "\n",
    "This processed glucose dataset forms the backbone of our standardized CGM data structure, providing a reliable foundation for subsequent analysis while maintaining transparency about data quality and completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ceac8102-c519-4915-be32-0cc973d8437b",
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose_df = clean_glucose(bg_df) # Function in source directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec5fea6-69d2-4e38-9b47-060b4dd09655",
   "metadata": {},
   "source": [
    "## Data Alignment and Integration\n",
    "\n",
    "The `align_diabetes_data()` function performs a critical step in our processing pipeline: combining our pre-processed glucose measurements (including interpolated values), carbohydrate records, and insulin data into a single, temporally aligned dataset with consistent 5-minute intervals.\n",
    "\n",
    "### Input Data Characteristics\n",
    "The function expects three pre-processed dataframes:\n",
    "- Glucose data (`bg_df`) with measurements in both units (mg/dL and mmol/L) and a 'missing' flag indicating where values have been interpolated over gaps up to 20 minutes\n",
    "- Carbohydrate records (`carb_df`) with validated gram quantities\n",
    "- Insulin data (`insulin_df`) with classified basal and bolus doses\n",
    "\n",
    "### Alignment Process\n",
    "The function takes these independently processed dataframes and performs several key integration steps:\n",
    "\n",
    "1. **Temporal Alignment**: All timestamps are rounded to 5-minute intervals to ensure consistent temporal binning across all data types, maintaining the integrity of our interpolated glucose values.\n",
    "\n",
    "2. **Treatment Data Aggregation**:\n",
    "  - Multiple insulin doses within the same 5-minute window are summed\n",
    "  - Multiple carbohydrate entries within the same window are summed\n",
    "  - Insulin entries maintain their classification (bolus/basal) and labeling\n",
    "\n",
    "3. **Unified Timeline**: The function uses the glucose measurements' timeline (including interpolated values) as the master index, ensuring:\n",
    "  - Complete coverage of the monitoring period\n",
    "  - Consistent 5-minute intervals throughout\n",
    "  - Preservation of gap indicators for transparency\n",
    "\n",
    "### Output Structure\n",
    "The resulting `aligned_df` provides a comprehensive view of diabetes management data with:\n",
    "- Regular 5-minute interval timestamps\n",
    "- Glucose values in both mg/dL and mmol/L units\n",
    "- Missing data indicators showing where glucose values were interpolated\n",
    "- Summed carbohydrate quantities\n",
    "- Aggregated insulin doses (both basal and bolus)\n",
    "- Boolean flags for labeled insulin entries\n",
    "\n",
    "Missing treatment values (carbohydrates and insulin) are filled with zeros, while the glucose values and their interpolation status are preserved from the pre-processed glucose dataset. This maintains transparency about data quality while providing a complete timeline for analysis.\n",
    "\n",
    "This aligned dataset serves as our standardized format, ready for various analyses including meal response patterns, insulin sensitivity calculations, and machine learning applications where understanding data completeness and interpolation is crucial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f285f85a-5fc1-448e-9ca0-02328712d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_df = align_diabetes_data(glucose_df, carb_df, insulin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cac15c2-626f-4668-abd7-33399e4ab5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89b28e0d-91d9-42e5-a6dc-13f8a803651b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_df.to_csv('../../data/complete.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a53aed1-bf8a-47f8-9b17-6a1873e5c898",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
