{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"CGM Data Processor <p>A robust Python framework for processing and analysing diabetes device data</p> <p> </p>"},{"location":"#process-your-diabetes-data","title":"\ud83d\udcc8 Process Your Diabetes Data","text":"<p>Analyse data from multiple diabetes management systems including XDrip+ and Freestyle Libre. Handle CGM readings, insulin doses, carbs, and treatment notes with confidence.</p>"},{"location":"#cgm-analysis","title":"\ud83e\ude78 CGM Analysis","text":"<ul> <li>Gap detection</li> <li>Configurable Interpolation</li> <li>Quality metrics</li> </ul>"},{"location":"#treatment-data","title":"\ud83d\udc89 Treatment Data","text":"<ul> <li>Insulin doses</li> <li>Carb intake</li> <li>Event notes</li> </ul>"},{"location":"#advanced-features","title":"\ud83e\uddd1\u200d\ud83d\udd2c Advanced Features","text":"<ul> <li>Automated format detection</li> <li>Data alignment</li> <li>Flexible export options</li> <li>Complete metadata carried through to output format</li> </ul>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Install CGM Data Processor - Installation Guide</p> <p>The simplest way to use the CGM Data Processor is to run <code>python -m src.cli path/to/data/export.file</code> from the root directory. The following arguments can be supllied:</p> <pre><code>python -m src.cli data.sqlite \\\n    --debug                  # Display verbose output\n    --interpolation-limit 6  # Max CGM gaps to fill (6 = 30 mins)\n    --bolus-limit 10.0       # Max bolus insulin units\n    --max-dose 20.0          # Max valid insulin dose\n    --output ./my_analysis   # Output location\n</code></pre> <p></p> <p>The cli script, performs multiple processing steps and outputs standardised CSV data. The library can be used in many different configurations depending on your use case. For individual use cases check out our API Reference section.</p> <p>Example of simple use case: <pre><code>from src.core.exceptions import DataProcessingError, ReaderError\nfrom src.core.format_registry import FormatRegistry\nfrom src.file_parser.format_detector import FormatDetector\nfrom src.processors import DataProcessor\nfrom src.readers import BaseReader\n\n# Initialise format detection\nregistry = FormatRegistry()\ndetector = FormatDetector(registry)\nprocessor = DataProcessor()\nfile_path = \"example_data.sqlite\"\n\n# Process file\ndetected_format, _, _ = detector.detect_format(file_path)\nreader = BaseReader.get_reader_for_format(detected_format, file_path)\n    with reader:\n        table_data = reader.read_all_tables()\n        if not table_data:\n            raise ReaderError(\"No valid data found in file\")\n        try:\n            processed_data = processor.process_tables(\n                table_data=table_data,\n                detected_format=detected_format,\n            )\n            if not processed_data:\n                raise DataProcessingError(\"No data could be processed\")\n\n        except DataProcessingError as e:\n            raise DataProcessingError(f\"Failed to process data: {str(e)}\") from e\n</code></pre></p>"},{"location":"#key-features","title":"\ud83d\udca1 Key Features","text":"<ul> <li>Automated format detection for multiple data sources</li> <li>Robust data validation and cleaning</li> <li>Gap detection and interpolation for CGM data</li> <li>Treatment classification and verification</li> <li>Flexible data export options</li> </ul>"},{"location":"#example-output-structure","title":"\ud83d\udcca Example Output Structure","text":"![output](assets/output_structure.png)"},{"location":"#responsible-use","title":"\ud83d\udee1\ufe0f Responsible Use","text":"This tool is designed for data analysis only. Not intended for real-time monitoring or medical decision making. Always consult healthcare providers for medical advice."},{"location":"CONTRIBUTING_FORMATS/","title":"CONTRIBUTING FORMATS","text":"<p>Contributing device formats \u2014 detailed guide</p> <p>This document explains exactly how to add or update device format definitions for cgm-data-processor. It covers how formats are discovered, validated, and used at runtime and by the format detector. Follow this carefully when adding CSV or SQLite formats so they pass validation and work with the readers.</p> <p>Contents - Format discovery (FormatRegistry) - Format keys and naming - CSV format specifics (TableStructure, timestamp_column, header_row) - SQLite format specifics (table names, quoted identifiers) - Timestamp handling and detection - Examples (LibreView CSV, XDrip SQLite) \u2014 copy/paste-ready - Tests and PR checklist</p> <p>Format discovery (FormatRegistry)</p> <ul> <li>The registry is implemented in <code>src/core/format_registry.py</code>.</li> <li>At startup the registry recursively imports every <code>.py</code> under <code>src/core/devices/</code>.</li> <li>Files named <code>__init__.py</code> are ignored.</li> <li>Each imported module is inspected for <code>DeviceFormat</code> instances. Any     instance found is validated and registered.</li> <li>Registered formats are stored under a key: <code>format_name + '_' + file_type.value</code>.</li> <li>Example: <code>xdrip_sqlite_sqlite</code> for the <code>xdrip_sqlite</code> format whose first file     has <code>file_type=FileType.SQLITE</code>.</li> <li>Keep device modules declarative (data-only). Import-time side-effects will run   during registry loading and may break CI or CLI tools.</li> </ul> <p>Format key and API</p> <ul> <li>Use <code>FormatRegistry()</code> to access formats programmatically.</li> <li><code>registry.formats</code> returns all registered <code>DeviceFormat</code> objects.</li> <li><code>registry.get_format(name)</code> returns the format registered under <code>name</code>.</li> <li><code>registry.get_formats_for_file(path)</code> filters formats by the file extension     (maps extension to <code>FileType</code>).</li> <li><code>registry.get_formats_with_data_type(DataType.CGM)</code> returns formats that     declare that data type.</li> </ul> <p>Format detection (FormatDetector)</p> <ul> <li>Implemented in <code>src/file_parser/format_detector.py</code>.</li> <li>Detection steps:</li> <li>Ensure file exists.</li> <li>Get potential formats for the file extension via <code>FormatRegistry.get_formats_for_file()</code>.</li> <li>For each potential <code>DeviceFormat</code>, call a validator based on <code>FileType</code>.</li> <li>Validator checks the presence of required tables/columns (case-insensitive      normalisation for CSV/JSON/XML) and returns a <code>ValidationResult</code>.</li> <li>First format that passes validation is chosen.</li> <li>If no header_row is provided for a CSV format, the detector reads the first   4 rows and treats the first row that contains all required columns as the   header.</li> </ul> <p>CSV formats (concrete rules)</p> <ul> <li><code>FileConfig</code> for CSV must have exactly one <code>TableStructure</code>.</li> <li>The table's <code>name</code> MUST be an empty string (<code>\"\"</code>). This is enforced by   <code>FileConfig.__post_init__</code>.</li> <li><code>TableStructure.timestamp_column</code> is REQUIRED. The CSV reader will try to   convert this column to UTC datetimes and set it as the DataFrame index.</li> <li><code>TableStructure.header_row</code> is optional and is a 0-based index. If provided,   the CSV reader will read the file and skip rows up to <code>header_row</code>, treating   that row as the header. If not provided, detection tries to find a header row   within the first 4 rows.</li> <li>Column names in <code>ColumnMapping.source_name</code> must match CSV headers after   normalization (strip whitespace). The detector and reader compare headers in   lower-case when auto-detecting, but the reader uses the exact header strings   when selecting columns after reading (it strips whitespace but keeps case).</li> <li>Use <code>ColumnRequirement</code> to control required fields:</li> <li><code>CONFIRMATION_ONLY</code>: used to assert presence but not loaded into DataFrame</li> <li><code>REQUIRED_WITH_DATA</code>: must exist and contain data</li> <li><code>REQUIRED_NULLABLE</code>: must exist but may be all-null</li> <li><code>OPTIONAL</code>: not required</li> </ul> <p>CSV example (LibreView \u2014 real example in repo)</p> <p>See <code>src/core/devices/libreview/csv.py</code> in the repository. Key points: - <code>header_row=1</code> means skip row 0 and treat row 1 as header. - Timestamp column is <code>Device Timestamp</code>. - Uses <code>ColumnRequirement.CONFIRMATION_ONLY</code> for device ID columns.</p> <p>SQLite formats (concrete rules)</p> <ul> <li>Each <code>TableStructure.name</code> is used as the exact (case-sensitive) table name in   the SQLite file. The SQLite reader validates the identifier using   <code>BaseReader._validate_identifier</code> (alphanumeric, underscore, dot allowed).</li> <li><code>TableStructure.timestamp_column</code> is REQUIRED.</li> <li>The reader constructs a SQL query using quoted identifiers (double quotes) to   read only the required columns and orders by the timestamp column.</li> <li>Column names are matched exactly (case-sensitive) against <code>inspector.get_columns(table_name)</code> during detection.</li> </ul> <p>Timestamp handling</p> <ul> <li>The BaseReader implements a deterministic timestamp detector (<code>detect_timestamp_format</code>) that:</li> <li>samples up to 50 non-null values</li> <li>detects numeric epochs (seconds vs milliseconds)</li> <li>looks for ISO-like strings (T, Z, +/- offsets)</li> <li>tries a short list of explicit formats</li> <li>falls back to pandas inference if needed</li> <li>If detection returns UNKNOWN the reader raises <code>TimestampProcessingError</code>.</li> <li>Epochs are handled (unit 's' for seconds, 'ms' for milliseconds) and parsed   to timezone-aware UTC datetimes. If parsed datetimes are naive they are   localized to UTC.</li> </ul> <p>Example format definitions (copy/paste)</p> <ul> <li>XDrip SQLite (already in repo): <code>src/core/devices/xdrip/sqlite.py</code></li> </ul> <pre><code>from src.core.data_types import (\n    ColumnMapping,\n    ColumnRequirement,\n    DataType,\n    DeviceFormat,\n    FileConfig,\n    FileType,\n    TableStructure,\n    Unit,\n)\n\nXDRIP_SQLITE_FORMAT = DeviceFormat(\n    name=\"xdrip_sqlite\",\n    files=[\n        FileConfig(\n            name_pattern=\"*.sqlite\",\n            file_type=FileType.SQLITE,\n            tables=[\n                TableStructure(\n                    name=\"BgReadings\",\n                    timestamp_column=\"timestamp\",\n                    columns=[\n                        ColumnMapping(source_name=\"calculated_value\", data_type=DataType.CGM, unit=Unit.MGDL),\n                        ColumnMapping(source_name=\"raw_data\", data_type=DataType.CGM, is_primary=False),\n                    ],\n                ),\n                TableStructure(\n                    name=\"Treatments\",\n                    timestamp_column=\"timestamp\",\n                    columns=[\n                        ColumnMapping(source_name=\"insulin\", data_type=DataType.INSULIN, unit=Unit.UNITS),\n                        ColumnMapping(source_name=\"insulinJSON\", data_type=DataType.INSULIN_META, requirement=ColumnRequirement.REQUIRED_NULLABLE),\n                        ColumnMapping(source_name=\"carbs\", data_type=DataType.CARBS, unit=Unit.GRAMS),\n                        ColumnMapping(source_name=\"notes\", data_type=DataType.NOTES, requirement=ColumnRequirement.REQUIRED_NULLABLE),\n                    ],\n                ),\n            ],\n        )\n    ],\n)\n</code></pre> <ul> <li>LibreView CSV (already in repo): <code>src/core/devices/libreview/csv.py</code></li> </ul> <pre><code>from src.core.data_types import (\n    ColumnMapping,\n    ColumnRequirement,\n    DataType,\n    DeviceFormat,\n    FileConfig,\n    FileType,\n    TableStructure,\n    Unit,\n)\n\nLIBREVIEW_CSV_FORMAT = DeviceFormat(\n    name=\"libreview_csv\",\n    files=[\n        FileConfig(\n            name_pattern=\"*.csv\",\n            file_type=FileType.CSV,\n            tables=[\n                TableStructure(\n                    name=\"\",\n                    timestamp_column=\"Device Timestamp\",\n                    header_row=1,\n                    columns=[\n                        ColumnMapping(source_name=\"Device\", requirement=ColumnRequirement.CONFIRMATION_ONLY),\n                        ColumnMapping(source_name=\"Serial Number\", requirement=ColumnRequirement.CONFIRMATION_ONLY),\n                        ColumnMapping(source_name=\"Historic Glucose mmol/L\", data_type=DataType.CGM, unit=Unit.MMOL),\n                        ColumnMapping(source_name=\"Scan Glucose mmol/L\", data_type=DataType.CGM, unit=Unit.MMOL, is_primary=False),\n                        ColumnMapping(source_name=\"Strip Glucose mmol/L\", data_type=DataType.BGM, unit=Unit.MMOL, requirement=ColumnRequirement.REQUIRED_NULLABLE),\n                        ColumnMapping(source_name=\"Rapid-Acting Insulin (units)\", data_type=DataType.INSULIN, unit=Unit.UNITS, requirement=ColumnRequirement.REQUIRED_NULLABLE),\n                        ColumnMapping(source_name=\"Carbohydrates (grams)\", data_type=DataType.CARBS, unit=Unit.GRAMS, requirement=ColumnRequirement.REQUIRED_NULLABLE),\n                        ColumnMapping(source_name=\"Notes\", data_type=DataType.NOTES, requirement=ColumnRequirement.REQUIRED_NULLABLE),\n                    ],\n                ),\n            ],\n        )\n    ],\n)\n</code></pre> <p>Testing &amp; PR checklist</p> <p>Before opening a PR:</p> <ul> <li>Ensure your device module has no runtime side-effects \u2014 the registry imports it.</li> <li>Include at least one sample file under <code>tests/data/sample_formats/&lt;vendor&gt;/</code> that matches your <code>name_pattern</code>.</li> <li>Add a unit test in <code>tests/test_file_parser/</code> that uses the FormatDetector to assert your sample file is detected and validated.</li> <li>Run tests locally with Poetry (<code>poetry install --with dev &amp;&amp; poetry run pytest -q</code>).</li> <li>In the PR description include:</li> <li><code>name_pattern</code> used</li> <li>example file path(s)</li> <li><code>timestamp_column</code> used and whether epoch/ISO</li> <li>whether <code>header_row</code> is non-default and its 0-based index</li> <li>note if a new reader was required (CSV/SQLite/JSON/XML)</li> </ul> <p>If you want, give me a vendor name and a small CSV header + 3 rows and I will scaffold: - <code>src/core/devices/&lt;vendor&gt;/csv.py</code> (DeviceFormat) - <code>tests/data/sample_formats/&lt;vendor&gt;/sample.csv</code> - <code>tests/test_file_parser/test_&lt;vendor&gt;_detection.py</code></p> <p>I will then run the test suite and report results.</p>"},{"location":"api/exporters/","title":"Exporters API","text":""},{"location":"api/exporters/#src.exporters","title":"<code>src.exporters</code>","text":"<p>initialise processors package and register all processors.</p>"},{"location":"api/exporters/#src.exporters.__all__","title":"<code>__all__ = ['BaseExporter', 'CSVExporter']</code>  <code>module-attribute</code>","text":""},{"location":"api/exporters/#src.exporters.BaseExporter","title":"<code>BaseExporter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for data exporters.</p> Source code in <code>src/exporters/base.py</code> <pre><code>class BaseExporter(ABC):\n    \"\"\"Abstract base class for data exporters.\"\"\"\n\n    def __init__(self, config: ExportConfig):\n        self.config = config\n        self.monthly_notes_cache = {}\n        # Ensure attribute exists so linters don't complain when it's set later\n        # This will be set per-run inside export_data()\n        self._current_monthly_base = None\n\n    def _generate_type_stats(\n        self, data: pd.DataFrame, data_type: Optional[DataType] = None\n    ) -&gt; List[str]:\n        \"\"\"Generate statistics for a specific data type.\"\"\"\n        stats = []\n\n        if data_type == DataType.CGM or \"cgm_primary\" in data.columns:\n            missing_count = data.get(\"missing_cgm\", data.get(\"missing\", 0)).sum()\n            total_readings = len(data)\n            total_na = data[\"cgm_primary\"].isna().sum()\n            initial_completeness = (\n                (total_readings - missing_count) / total_readings\n            ) * 100\n            remaining_completeness = (\n                (total_readings - total_na) / total_readings\n            ) * 100\n\n            stats.extend(\n                [\n                    \"CGM Processing Notes:\",\n                    f\"  Processed {total_readings} total CGM readings\",\n                    f\"  Found {missing_count} missing or interpolated values\",\n                    f\"  Initial CGM completeness: {initial_completeness:.2f}%\",\n                    f\"  CGM completeness after interpolation: {remaining_completeness:.2f}%\",\n                ]\n            )\n\n        if data_type == DataType.BGM or any(\n            col.startswith(\"bgm_\") for col in data.columns\n        ):\n            bgm_cols = [\n                col\n                for col in data.columns\n                if col.startswith(\"bgm_\") and not col.endswith((\"_clipped\", \"_mmol\"))\n            ]\n            for bgm_col in bgm_cols:\n                clipped_col = f\"{bgm_col}_clipped\"\n                total_readings = data[bgm_col].notna().sum()\n\n                if clipped_col in data.columns:\n                    clipped_readings = data[clipped_col].sum()\n                    clipped_percent = (\n                        (clipped_readings / total_readings * 100)\n                        if total_readings &gt; 0\n                        else 0\n                    )\n\n                    stats.extend(\n                        [\n                            f\"BGM Processing Notes ({bgm_col}):\",\n                            f\"  Processed {total_readings} total BGM readings\",\n                            f\"  Found {clipped_readings} clipped values ({clipped_percent:.1f}%)\",\n                        ]\n                    )\n                else:\n                    stats.extend(\n                        [\n                            f\"BGM Processing Notes ({bgm_col}):\",\n                            f\"  Processed {total_readings} total BGM readings\",\n                        ]\n                    )\n\n        if data_type == DataType.INSULIN or any(\n            col in data.columns for col in [\"dose\", \"basal_dose\", \"bolus_dose\"]\n        ):\n            if data_type == DataType.INSULIN:\n                basal_count = (\n                    data[\"is_basal\"].sum() if \"is_basal\" in data.columns else 0\n                )\n                bolus_count = (\n                    data[\"is_bolus\"].sum() if \"is_bolus\" in data.columns else 0\n                )\n                total_count = len(data)\n                stats.extend(\n                    [\n                        \"INSULIN Processing Notes:\",\n                        f\"  Found {total_count} total doses\",\n                        f\"  {basal_count} basal doses\",\n                        f\"  {bolus_count} bolus doses\",\n                    ]\n                )\n            else:\n                basal_count = (data[\"basal_dose\"] &gt; 0).sum()\n                bolus_count = (data[\"bolus_dose\"] &gt; 0).sum()\n                stats.extend(\n                    [\n                        \"INSULIN Processing Notes:\",\n                        f\"  Found {basal_count + bolus_count} total doses\",\n                        f\"  {basal_count} basal doses\",\n                        f\"  {bolus_count} bolus doses\",\n                    ]\n                )\n\n        if data_type == DataType.CARBS or \"carbs_primary\" in data.columns:\n            carb_entries = (\n                (data[\"carbs_primary\"] &gt; 0).sum()\n                if \"carbs_primary\" in data.columns\n                else (data &gt; 0).sum()\n            )\n            stats.extend(\n                [\"CARBS Processing Notes:\", f\"  Found {carb_entries} carb entries\"]\n            )\n\n        if data_type == DataType.NOTES or \"notes_primary\" in data.columns:\n            note_count = (\n                data[\"notes_primary\"].notna().sum()\n                if \"notes_primary\" in data.columns\n                else data.notna().sum()\n            )\n            stats.extend(\n                [\"NOTES Processing Notes:\", f\"  Found {note_count} notes entries\"]\n            )\n\n        return stats\n\n    def _handle_monthly_exports(\n        self, data: ProcessedTypeData, data_type: DataType\n    ) -&gt; None:\n        \"\"\"Handle monthly data splits and exports.\"\"\"\n        # Use per-run monthly base if set (to avoid collisions between runs)\n        monthly_base = getattr(self, \"_current_monthly_base\", None)\n        if monthly_base is None:\n            monthly_base = self.config.output_dir / \"monthly\"\n\n        for timestamp, group in data.dataframe.groupby(pd.Grouper(freq=\"ME\")):\n            if not group.empty:\n                month_str = pd.Timestamp(timestamp).strftime(\"%Y-%m\")\n                month_dir = monthly_base / month_str\n                month_dir.mkdir(parents=True, exist_ok=True)\n\n                # Generate fresh stats just for this month's data\n                monthly_stats = [\n                    f\"Period: {month_str}\",\n                    f\"Records: {len(group)}\",\n                    *self._generate_type_stats(group, data_type),\n                ]\n\n                # Create monthly data with new stats, but keep original source_units\n                monthly_data = ProcessedTypeData(\n                    dataframe=group,\n                    source_units=data.source_units,\n                    processing_notes=monthly_stats,  # Only using the fresh monthly stats\n                )\n\n                if self.config.include_processing_notes:\n                    self.export_processing_notes(monthly_data, month_dir)\n                self.export_monthly_split(monthly_data, data_type, month_dir)\n\n    def _handle_monthly_aligned_exports(self, data: AlignmentResult) -&gt; None:\n        \"\"\"Handle monthly splits for aligned data.\"\"\"\n        monthly_base = getattr(self, \"_current_monthly_base\", None)\n        if monthly_base is None:\n            monthly_base = self.config.output_dir / \"monthly\"\n\n        for timestamp, group in data.dataframe.groupby(pd.Grouper(freq=\"ME\")):\n            if not group.empty:\n                month_str = pd.Timestamp(timestamp).strftime(\"%Y-%m\")\n                month_dir = monthly_base / month_str\n                month_dir.mkdir(parents=True, exist_ok=True)\n\n                monthly_notes = [\n                    f\"Period: {month_str}\",\n                    f\"Records: {len(group)}\",\n                    *self._generate_type_stats(group),\n                ]\n\n                monthly_aligned = AlignmentResult(\n                    dataframe=group,\n                    start_time=group.index.min(),\n                    end_time=group.index.max(),\n                    frequency=data.frequency,\n                    processing_notes=monthly_notes,\n                    source_units=data.source_units,\n                )\n\n                self.export_aligned_monthly_split(monthly_aligned, month_dir)\n                if self.config.include_processing_notes:\n                    self.export_processing_notes(monthly_aligned, month_dir)\n\n    def _accumulate_monthly_stats(\n        self, month_str: str, group: pd.DataFrame, data_type: DataType\n    ) -&gt; None:\n        \"\"\"Accumulate statistics for a month across all data types.\"\"\"\n        if month_str not in self.monthly_notes_cache:\n            self.monthly_notes_cache[month_str] = {\n                \"period\": month_str,\n                \"record_count\": len(group),\n                \"stats\": [],\n            }\n\n        type_stats = self._generate_type_stats(group, data_type)\n        if type_stats:\n            self.monthly_notes_cache[month_str][\"stats\"].extend(type_stats)\n\n    @abstractmethod\n    def export_complete_dataset(\n        self, data: ProcessedTypeData, data_type: DataType, output_dir: Path\n    ) -&gt; None:\n        \"\"\"Export complete dataset for a specific data type.\"\"\"\n\n    @abstractmethod\n    def export_monthly_split(\n        self, data: ProcessedTypeData, data_type: DataType, month_dir: Path\n    ) -&gt; None:\n        \"\"\"Export monthly split for a specific data type.\"\"\"\n\n    @abstractmethod\n    def export_processing_notes(\n        self, data: Union[ProcessedTypeData, AlignmentResult], output_path: Path\n    ) -&gt; None:\n        \"\"\"Export processing notes.\"\"\"\n\n    @abstractmethod\n    def export_aligned_complete_dataset(\n        self, data: AlignmentResult, output_dir: Path\n    ) -&gt; None:\n        \"\"\"Export complete aligned dataset.\"\"\"\n\n    @abstractmethod\n    def export_aligned_monthly_split(\n        self, data: AlignmentResult, month_dir: Path\n    ) -&gt; None:\n        \"\"\"Export monthly split for aligned data.\"\"\"\n\n    def export_data(\n        self,\n        processed_data: Dict[DataType, ProcessedTypeData],\n        aligned_data: Optional[AlignmentResult] = None,\n    ) -&gt; None:\n        \"\"\"Export all processed data and aligned data if available.\"\"\"\n        if not processed_data and not aligned_data:\n            return\n\n        # Reset monthly notes cache\n        self.monthly_notes_cache = {}\n        # Get date range from either source\n        date_range = self.get_date_range(\n            next(iter(processed_data.values())) if processed_data else aligned_data\n        )\n\n        # Use a unique complete directory per run to avoid clobbering previous exports\n        run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n        complete_dir = self.config.output_dir / f\"{date_range}_complete_{run_id}\"\n        complete_dir.mkdir(parents=True, exist_ok=True)\n\n        # Place monthly exports inside the run's complete directory to keep exports self-contained\n        self._current_monthly_base = complete_dir / \"monthly\"\n        self._current_monthly_base.mkdir(parents=True, exist_ok=True)\n\n        # Export individual datasets\n        for data_type, type_data in processed_data.items():\n            self.export_complete_dataset(type_data, data_type, complete_dir)\n            if self.config.include_processing_notes:\n                self.export_processing_notes(type_data, complete_dir)\n            if self.config.split_by_month:\n                self._handle_monthly_exports(type_data, data_type)\n\n        # Export aligned data if available\n        if aligned_data:\n            self.export_aligned_complete_dataset(aligned_data, complete_dir)\n            if self.config.include_processing_notes:\n                self.export_processing_notes(aligned_data, complete_dir)\n            if self.config.split_by_month:\n                self._handle_monthly_aligned_exports(aligned_data)\n\n    @staticmethod\n    def get_date_range(data: Union[ProcessedTypeData, AlignmentResult]) -&gt; str:\n        \"\"\"Get date range string from data.\"\"\"\n        df = data.dataframe\n        if not isinstance(df.index, pd.DatetimeIndex):\n            raise ValueError(\"DataFrame must have DatetimeIndex\")\n        start = df.index.min().strftime(\"%Y-%m-%d\")\n        end = df.index.max().strftime(\"%Y-%m-%d\")\n        return f\"{start}_to_{end}\"\n</code></pre>"},{"location":"api/exporters/#src.exporters.BaseExporter.config","title":"<code>config = config</code>  <code>instance-attribute</code>","text":""},{"location":"api/exporters/#src.exporters.BaseExporter.monthly_notes_cache","title":"<code>monthly_notes_cache = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/exporters/#src.exporters.BaseExporter.__init__","title":"<code>__init__(config: ExportConfig)</code>","text":"Source code in <code>src/exporters/base.py</code> <pre><code>def __init__(self, config: ExportConfig):\n    self.config = config\n    self.monthly_notes_cache = {}\n    # Ensure attribute exists so linters don't complain when it's set later\n    # This will be set per-run inside export_data()\n    self._current_monthly_base = None\n</code></pre>"},{"location":"api/exporters/#src.exporters.BaseExporter.export_complete_dataset","title":"<code>export_complete_dataset(data: ProcessedTypeData, data_type: DataType, output_dir: Path) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Export complete dataset for a specific data type.</p> Source code in <code>src/exporters/base.py</code> <pre><code>@abstractmethod\ndef export_complete_dataset(\n    self, data: ProcessedTypeData, data_type: DataType, output_dir: Path\n) -&gt; None:\n    \"\"\"Export complete dataset for a specific data type.\"\"\"\n</code></pre>"},{"location":"api/exporters/#src.exporters.BaseExporter.export_monthly_split","title":"<code>export_monthly_split(data: ProcessedTypeData, data_type: DataType, month_dir: Path) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Export monthly split for a specific data type.</p> Source code in <code>src/exporters/base.py</code> <pre><code>@abstractmethod\ndef export_monthly_split(\n    self, data: ProcessedTypeData, data_type: DataType, month_dir: Path\n) -&gt; None:\n    \"\"\"Export monthly split for a specific data type.\"\"\"\n</code></pre>"},{"location":"api/exporters/#src.exporters.BaseExporter.export_processing_notes","title":"<code>export_processing_notes(data: Union[ProcessedTypeData, AlignmentResult], output_path: Path) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Export processing notes.</p> Source code in <code>src/exporters/base.py</code> <pre><code>@abstractmethod\ndef export_processing_notes(\n    self, data: Union[ProcessedTypeData, AlignmentResult], output_path: Path\n) -&gt; None:\n    \"\"\"Export processing notes.\"\"\"\n</code></pre>"},{"location":"api/exporters/#src.exporters.BaseExporter.export_aligned_complete_dataset","title":"<code>export_aligned_complete_dataset(data: AlignmentResult, output_dir: Path) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Export complete aligned dataset.</p> Source code in <code>src/exporters/base.py</code> <pre><code>@abstractmethod\ndef export_aligned_complete_dataset(\n    self, data: AlignmentResult, output_dir: Path\n) -&gt; None:\n    \"\"\"Export complete aligned dataset.\"\"\"\n</code></pre>"},{"location":"api/exporters/#src.exporters.BaseExporter.export_aligned_monthly_split","title":"<code>export_aligned_monthly_split(data: AlignmentResult, month_dir: Path) -&gt; None</code>  <code>abstractmethod</code>","text":"<p>Export monthly split for aligned data.</p> Source code in <code>src/exporters/base.py</code> <pre><code>@abstractmethod\ndef export_aligned_monthly_split(\n    self, data: AlignmentResult, month_dir: Path\n) -&gt; None:\n    \"\"\"Export monthly split for aligned data.\"\"\"\n</code></pre>"},{"location":"api/exporters/#src.exporters.BaseExporter.export_data","title":"<code>export_data(processed_data: Dict[DataType, ProcessedTypeData], aligned_data: Optional[AlignmentResult] = None) -&gt; None</code>","text":"<p>Export all processed data and aligned data if available.</p> Source code in <code>src/exporters/base.py</code> <pre><code>def export_data(\n    self,\n    processed_data: Dict[DataType, ProcessedTypeData],\n    aligned_data: Optional[AlignmentResult] = None,\n) -&gt; None:\n    \"\"\"Export all processed data and aligned data if available.\"\"\"\n    if not processed_data and not aligned_data:\n        return\n\n    # Reset monthly notes cache\n    self.monthly_notes_cache = {}\n    # Get date range from either source\n    date_range = self.get_date_range(\n        next(iter(processed_data.values())) if processed_data else aligned_data\n    )\n\n    # Use a unique complete directory per run to avoid clobbering previous exports\n    run_id = datetime.now().strftime(\"%Y%m%dT%H%M%S\")\n    complete_dir = self.config.output_dir / f\"{date_range}_complete_{run_id}\"\n    complete_dir.mkdir(parents=True, exist_ok=True)\n\n    # Place monthly exports inside the run's complete directory to keep exports self-contained\n    self._current_monthly_base = complete_dir / \"monthly\"\n    self._current_monthly_base.mkdir(parents=True, exist_ok=True)\n\n    # Export individual datasets\n    for data_type, type_data in processed_data.items():\n        self.export_complete_dataset(type_data, data_type, complete_dir)\n        if self.config.include_processing_notes:\n            self.export_processing_notes(type_data, complete_dir)\n        if self.config.split_by_month:\n            self._handle_monthly_exports(type_data, data_type)\n\n    # Export aligned data if available\n    if aligned_data:\n        self.export_aligned_complete_dataset(aligned_data, complete_dir)\n        if self.config.include_processing_notes:\n            self.export_processing_notes(aligned_data, complete_dir)\n        if self.config.split_by_month:\n            self._handle_monthly_aligned_exports(aligned_data)\n</code></pre>"},{"location":"api/exporters/#src.exporters.BaseExporter.get_date_range","title":"<code>get_date_range(data: Union[ProcessedTypeData, AlignmentResult]) -&gt; str</code>  <code>staticmethod</code>","text":"<p>Get date range string from data.</p> Source code in <code>src/exporters/base.py</code> <pre><code>@staticmethod\ndef get_date_range(data: Union[ProcessedTypeData, AlignmentResult]) -&gt; str:\n    \"\"\"Get date range string from data.\"\"\"\n    df = data.dataframe\n    if not isinstance(df.index, pd.DatetimeIndex):\n        raise ValueError(\"DataFrame must have DatetimeIndex\")\n    start = df.index.min().strftime(\"%Y-%m-%d\")\n    end = df.index.max().strftime(\"%Y-%m-%d\")\n    return f\"{start}_to_{end}\"\n</code></pre>"},{"location":"api/exporters/#src.exporters.CSVExporter","title":"<code>CSVExporter</code>","text":"<p>               Bases: <code>BaseExporter</code></p> <p>CSV implementation of data exporter.</p> Source code in <code>src/exporters/csv.py</code> <pre><code>class CSVExporter(BaseExporter):\n    \"\"\"CSV implementation of data exporter.\"\"\"\n\n    def export_complete_dataset(\n        self, data: ProcessedTypeData, data_type: DataType, output_dir: Path\n    ) -&gt; None:\n        \"\"\"Export complete dataset as CSV.\"\"\"\n        filename = f\"{data_type.name.lower()}.csv\"\n        data.dataframe.to_csv(output_dir / filename)\n\n        if self.config.include_processing_notes:\n            self.export_processing_notes(data, output_dir)\n\n    def export_monthly_split(\n        self, data: ProcessedTypeData, data_type: DataType, month_dir: Path\n    ) -&gt; None:\n        \"\"\"Export monthly split as CSV.\"\"\"\n        filename = f\"{data_type.name.lower()}.csv\"\n        data.dataframe.to_csv(month_dir / filename)\n\n    def export_aligned_complete_dataset(\n        self, data: AlignmentResult, output_dir: Path\n    ) -&gt; None:\n        \"\"\"Export complete aligned dataset as CSV.\"\"\"\n        data.dataframe.to_csv(output_dir / \"aligned_data.csv\")\n\n    def export_aligned_monthly_split(\n        self, data: AlignmentResult, month_dir: Path\n    ) -&gt; None:\n        \"\"\"Export monthly split of aligned data as CSV.\"\"\"\n        data.dataframe.to_csv(month_dir / \"aligned_data.csv\")\n        if self.config.include_processing_notes:\n            self.export_processing_notes(data, month_dir)\n\n    def export_processing_notes(\n        self, data: Union[ProcessedTypeData, AlignmentResult], output_path: Path\n    ) -&gt; None:\n        \"\"\"Export processing notes as JSON.\"\"\"\n        common_data = {\n            \"export_date\": pd.Timestamp.now().isoformat(),\n            \"date_range\": f\"{data.dataframe.index.min().strftime('%Y-%m-%d')} to {data.dataframe.index.max().strftime('%Y-%m-%d')}\",\n            \"record_count\": len(data.dataframe),\n            \"columns_present\": list(data.dataframe.columns),\n            \"notes\": data.processing_notes,\n        }\n\n        if isinstance(data, ProcessedTypeData):\n            common_data[\"source_units\"] = {\n                k: v.value for k, v in data.source_units.items()\n            }\n        else:  # AlignmentResult\n            common_data[\"frequency\"] = data.frequency\n            common_data[\"completeness\"] = {\n                col: f\"{(data.dataframe[col].notna().mean() * 100):.2f}%\"\n                for col in data.dataframe.columns\n            }\n            common_data[\"source_units\"] = {\n                k: v.value for k, v in data.source_units.items()\n            }\n\n        with open(output_path / \"processing_notes.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(common_data, f, indent=2, ensure_ascii=False)\n</code></pre>"},{"location":"api/exporters/#src.exporters.CSVExporter.export_complete_dataset","title":"<code>export_complete_dataset(data: ProcessedTypeData, data_type: DataType, output_dir: Path) -&gt; None</code>","text":"<p>Export complete dataset as CSV.</p> Source code in <code>src/exporters/csv.py</code> <pre><code>def export_complete_dataset(\n    self, data: ProcessedTypeData, data_type: DataType, output_dir: Path\n) -&gt; None:\n    \"\"\"Export complete dataset as CSV.\"\"\"\n    filename = f\"{data_type.name.lower()}.csv\"\n    data.dataframe.to_csv(output_dir / filename)\n\n    if self.config.include_processing_notes:\n        self.export_processing_notes(data, output_dir)\n</code></pre>"},{"location":"api/exporters/#src.exporters.CSVExporter.export_monthly_split","title":"<code>export_monthly_split(data: ProcessedTypeData, data_type: DataType, month_dir: Path) -&gt; None</code>","text":"<p>Export monthly split as CSV.</p> Source code in <code>src/exporters/csv.py</code> <pre><code>def export_monthly_split(\n    self, data: ProcessedTypeData, data_type: DataType, month_dir: Path\n) -&gt; None:\n    \"\"\"Export monthly split as CSV.\"\"\"\n    filename = f\"{data_type.name.lower()}.csv\"\n    data.dataframe.to_csv(month_dir / filename)\n</code></pre>"},{"location":"api/exporters/#src.exporters.CSVExporter.export_aligned_complete_dataset","title":"<code>export_aligned_complete_dataset(data: AlignmentResult, output_dir: Path) -&gt; None</code>","text":"<p>Export complete aligned dataset as CSV.</p> Source code in <code>src/exporters/csv.py</code> <pre><code>def export_aligned_complete_dataset(\n    self, data: AlignmentResult, output_dir: Path\n) -&gt; None:\n    \"\"\"Export complete aligned dataset as CSV.\"\"\"\n    data.dataframe.to_csv(output_dir / \"aligned_data.csv\")\n</code></pre>"},{"location":"api/exporters/#src.exporters.CSVExporter.export_aligned_monthly_split","title":"<code>export_aligned_monthly_split(data: AlignmentResult, month_dir: Path) -&gt; None</code>","text":"<p>Export monthly split of aligned data as CSV.</p> Source code in <code>src/exporters/csv.py</code> <pre><code>def export_aligned_monthly_split(\n    self, data: AlignmentResult, month_dir: Path\n) -&gt; None:\n    \"\"\"Export monthly split of aligned data as CSV.\"\"\"\n    data.dataframe.to_csv(month_dir / \"aligned_data.csv\")\n    if self.config.include_processing_notes:\n        self.export_processing_notes(data, month_dir)\n</code></pre>"},{"location":"api/exporters/#src.exporters.CSVExporter.export_processing_notes","title":"<code>export_processing_notes(data: Union[ProcessedTypeData, AlignmentResult], output_path: Path) -&gt; None</code>","text":"<p>Export processing notes as JSON.</p> Source code in <code>src/exporters/csv.py</code> <pre><code>def export_processing_notes(\n    self, data: Union[ProcessedTypeData, AlignmentResult], output_path: Path\n) -&gt; None:\n    \"\"\"Export processing notes as JSON.\"\"\"\n    common_data = {\n        \"export_date\": pd.Timestamp.now().isoformat(),\n        \"date_range\": f\"{data.dataframe.index.min().strftime('%Y-%m-%d')} to {data.dataframe.index.max().strftime('%Y-%m-%d')}\",\n        \"record_count\": len(data.dataframe),\n        \"columns_present\": list(data.dataframe.columns),\n        \"notes\": data.processing_notes,\n    }\n\n    if isinstance(data, ProcessedTypeData):\n        common_data[\"source_units\"] = {\n            k: v.value for k, v in data.source_units.items()\n        }\n    else:  # AlignmentResult\n        common_data[\"frequency\"] = data.frequency\n        common_data[\"completeness\"] = {\n            col: f\"{(data.dataframe[col].notna().mean() * 100):.2f}%\"\n            for col in data.dataframe.columns\n        }\n        common_data[\"source_units\"] = {\n            k: v.value for k, v in data.source_units.items()\n        }\n\n    with open(output_path / \"processing_notes.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(common_data, f, indent=2, ensure_ascii=False)\n</code></pre>"},{"location":"api/file_parser/","title":"Format Detection API","text":""},{"location":"api/file_parser/#src.file_parser.format_detector","title":"<code>src.file_parser.format_detector</code>","text":"<p>Format detection for diabetes device data files.</p> <p>This module provides functionality to detect device formats by examining file structure. It validates only the presence of required tables and columns, without checking data content.</p>"},{"location":"api/file_parser/#src.file_parser.format_detector.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/file_parser/#src.file_parser.format_detector.parser","title":"<code>parser = argparse.ArgumentParser(description='Diabetes Data Format Detection Tool')</code>  <code>module-attribute</code>","text":""},{"location":"api/file_parser/#src.file_parser.format_detector.args","title":"<code>args = parser.parse_args()</code>  <code>module-attribute</code>","text":""},{"location":"api/file_parser/#src.file_parser.format_detector.file_path","title":"<code>file_path = Path(args.file_path)</code>  <code>module-attribute</code>","text":""},{"location":"api/file_parser/#src.file_parser.format_detector.registry","title":"<code>registry = FormatRegistry()</code>  <code>module-attribute</code>","text":""},{"location":"api/file_parser/#src.file_parser.format_detector.detector","title":"<code>detector = FormatDetector(registry)</code>  <code>module-attribute</code>","text":""},{"location":"api/file_parser/#src.file_parser.format_detector.ValidationResult","title":"<code>ValidationResult</code>","text":"<p>Container for structure validation results.</p> Source code in <code>src/file_parser/format_detector.py</code> <pre><code>class ValidationResult:\n    \"\"\"Container for structure validation results.\"\"\"\n\n    def __init__(self):\n        self.missing_tables: List[str] = []\n        self.missing_columns: Dict[str, List[str]] = {}  # table: [columns]\n\n    def has_errors(self) -&gt; bool:\n        \"\"\"Check if any validation errors exist.\"\"\"\n        return bool(self.missing_tables or self.missing_columns)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Format validation errors as string.\"\"\"\n        errors = []\n        if self.missing_tables:\n            errors.append(f\"Missing tables: {', '.join(self.missing_tables)}\")\n        if self.missing_columns:\n            for current_table, columns in self.missing_columns.items():\n                errors.append(\n                    f\"Missing required columns in {current_table}: {', '.join(columns)}\"\n                )\n        return \"\\n\".join(errors)\n</code></pre>"},{"location":"api/file_parser/#src.file_parser.format_detector.ValidationResult.missing_tables","title":"<code>missing_tables: List[str] = []</code>  <code>instance-attribute</code>","text":""},{"location":"api/file_parser/#src.file_parser.format_detector.ValidationResult.missing_columns","title":"<code>missing_columns: Dict[str, List[str]] = {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/file_parser/#src.file_parser.format_detector.ValidationResult.__init__","title":"<code>__init__()</code>","text":"Source code in <code>src/file_parser/format_detector.py</code> <pre><code>def __init__(self):\n    self.missing_tables: List[str] = []\n    self.missing_columns: Dict[str, List[str]] = {}  # table: [columns]\n</code></pre>"},{"location":"api/file_parser/#src.file_parser.format_detector.ValidationResult.has_errors","title":"<code>has_errors() -&gt; bool</code>","text":"<p>Check if any validation errors exist.</p> Source code in <code>src/file_parser/format_detector.py</code> <pre><code>def has_errors(self) -&gt; bool:\n    \"\"\"Check if any validation errors exist.\"\"\"\n    return bool(self.missing_tables or self.missing_columns)\n</code></pre>"},{"location":"api/file_parser/#src.file_parser.format_detector.ValidationResult.__str__","title":"<code>__str__() -&gt; str</code>","text":"<p>Format validation errors as string.</p> Source code in <code>src/file_parser/format_detector.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Format validation errors as string.\"\"\"\n    errors = []\n    if self.missing_tables:\n        errors.append(f\"Missing tables: {', '.join(self.missing_tables)}\")\n    if self.missing_columns:\n        for current_table, columns in self.missing_columns.items():\n            errors.append(\n                f\"Missing required columns in {current_table}: {', '.join(columns)}\"\n            )\n    return \"\\n\".join(errors)\n</code></pre>"},{"location":"api/file_parser/#src.file_parser.format_detector.FormatDetector","title":"<code>FormatDetector</code>","text":"<p>Detects device formats by examining file structure.</p> Source code in <code>src/file_parser/format_detector.py</code> <pre><code>class FormatDetector:\n    \"\"\"Detects device formats by examining file structure.\"\"\"\n\n    def __init__(self, format_registry: FormatRegistry):\n        \"\"\"initialise detector with format registry.\"\"\"\n        self._registry = format_registry\n\n    def detect_format(\n        self, path: Path\n    ) -&gt; Tuple[Optional[DeviceFormat], Optional[str], Dict[str, ValidationResult]]:\n        \"\"\"Detect format of provided file.\n\n        Args:\n            path: Path to the file to check\n\n        Returns:\n            Tuple containing:\n                - Matched format (or None)\n                - Error message (or None)\n                - Dictionary of validation results per format tried\n\n        Example:\n            &gt;&gt;&gt; detector = FormatDetector(registry)\n            &gt;&gt;&gt; fmt, error, results = detector.detect_format(Path(\"data.sqlite\"))\n            &gt;&gt;&gt; if fmt:\n            ...     print(f\"Matched format: {fmt.name}\")\n            ... else:\n            ...     print(f\"No match: {error}\")\n        \"\"\"\n        logger.debug(\"Starting format detection for: %s\", path)\n        val_results = {}\n\n        try:\n            # Validate file exists and is readable\n            if not self._validate_file_exists(path):\n                return None, f\"File not found or not accessible: {path}\", {}\n\n            # Get potential formats based on file extension\n            potential_formats = self._registry.get_formats_for_file(path)\n            if not potential_formats:\n                return None, f\"No formats available for {path.suffix} files\", {}\n\n            # Try each format\n            for fmt in potential_formats:\n                try:\n                    val_test_result = ValidationResult()\n                    if self._validate_format(path, fmt, val_test_result):\n                        logger.debug(\"Successfully matched format: %s\", fmt.name)\n                        return fmt, None, val_results\n                    val_results[fmt.name] = val_test_result\n                except FormatValidationError as e:\n                    logger.debug(\"Error validating format %s: %s\", fmt.name, str(e))\n                    continue\n\n            return None, \"No matching format found\", val_results\n\n        except FileAccessError as e:\n            logger.error(\"Unexpected error during format detection: %s\", str(e))\n            return None, f\"Detection error: {str(e)}\", {}\n\n    def _validate_file_exists(self, path: Path) -&gt; bool:\n        \"\"\"Validate file exists and is accessible.\"\"\"\n        try:\n            return path.exists() and path.is_file()\n        except Exception as e:\n            raise FileAccessError(f\"Error occurred: {str(e)}\") from e\n\n    def _validate_format(\n        self, path: Path, fmt: DeviceFormat, validation_result: ValidationResult\n    ) -&gt; bool:\n        \"\"\"Validate if file matches format definition.\"\"\"\n        for config in fmt.files:\n            validator = self._get_validator(config.file_type)\n            if validator is None:\n                logger.warning(\"No validator available for %s\", config.file_type.value)\n                return False\n            try:\n                if not validator(path, config, validation_result):\n                    return False\n            except FormatValidationError as e:\n                logger.debug(\"Validation failed: %s\", {str(e)})\n                return False\n        return True\n\n    def _get_validator(self, file_type: FileType):\n        \"\"\"Get appropriate validation function for file type.\"\"\"\n        validators = {\n            FileType.SQLITE: self._validate_sqlite,\n            FileType.CSV: self._validate_csv,\n            FileType.JSON: self._validate_json,\n            FileType.XML: self._validate_xml,\n        }\n        return validators.get(file_type)\n\n    def _validate_sqlite(\n        self, path: Path, config, val_result: ValidationResult\n    ) -&gt; bool:\n        \"\"\"Validate SQLite file structure.\"\"\"\n        try:\n            engine = create_engine(f\"sqlite:///{path}\")\n            inspector = inspect(engine)\n\n            # Get all tables (case sensitive)\n            actual_tables = {name: name for name in inspector.get_table_names()}\n\n            # Check each required table\n            for required_table in config.tables:\n                table_name = required_table.name\n                if table_name not in actual_tables:\n                    val_result.missing_tables.append(required_table.name)\n                    continue\n\n                # Check columns\n                columns = inspector.get_columns(table_name)\n                column_names = {col[\"name\"] for col in columns}\n\n                # Check required columns exist in file\n                required_columns = {\n                    col.source_name\n                    for col in required_table.columns\n                    if col.requirement != ColumnRequirement.OPTIONAL\n                }\n                missing = required_columns - column_names\n                if missing:\n                    val_result.missing_columns[required_table.name] = [\n                        col.source_name\n                        for col in required_table.columns\n                        if col.requirement != ColumnRequirement.OPTIONAL\n                        and col.source_name in missing\n                    ]\n\n            return not val_result.has_errors()\n\n        except FormatValidationError as e:\n            logger.debug(\"SQLite validation error: %s\", str(e))\n            return False\n\n    def _validate_csv(self, path: Path, config, val_result: ValidationResult) -&gt; bool:\n        \"\"\"Validate CSV file structure.\"\"\"\n        try:\n            # CSV should have exactly one table\n            csv_table = config.tables[0]\n\n            # If the format specifies a header_row use it directly\n            header_row = getattr(csv_table, \"header_row\", None)\n            if header_row is not None:\n                header_found = False\n                with open(path, encoding=\"utf-8\", newline=\"\") as fh:\n                    reader = csv.reader(fh)\n                    for idx, row in enumerate(reader):\n                        if idx == header_row:\n                            # normalize column names: strip whitespace and lowercase\n                            row_columns = {\n                                str(col).strip().lower()\n                                for col in row\n                                if col is not None\n                            }\n                            header_found = True\n                            break\n\n                if not header_found:\n                    # requested header row does not exist in file\n                    val_result.missing_columns[\"\"] = [\n                        col.source_name\n                        for col in csv_table.columns\n                        if col.requirement != ColumnRequirement.OPTIONAL\n                    ]\n                    return False\n\n                required_columns = {\n                    col.source_name.strip().lower()\n                    for col in csv_table.columns\n                    if col.requirement != ColumnRequirement.OPTIONAL\n                }\n\n                missing = required_columns - row_columns\n                if missing:\n                    val_result.missing_columns[csv_table.name] = [\n                        col.source_name\n                        for col in csv_table.columns\n                        if col.requirement != ColumnRequirement.OPTIONAL\n                        and col.source_name.strip().lower() in missing\n                    ]\n                    return False\n\n                return True\n\n            # No header_row specified: keep scanning first 4 rows for a header\n            df = pd.read_csv(path, nrows=4, header=None)\n\n            # Check each of the first 4 rows to see if it contains valid column headers\n            found_header_row = None\n            for row_idx in range(min(4, len(df))):\n                # normalise string values: strip whitespace and lowercase\n                row_columns = {str(col).strip().lower() for col in df.iloc[row_idx]}\n                logger.debug(\"CSV header row %d: %s\", row_idx, row_columns)\n\n                required_columns = {\n                    col.source_name.strip().lower()\n                    for col in csv_table.columns\n                    if col.requirement != ColumnRequirement.OPTIONAL\n                }\n\n                missing = required_columns - row_columns\n\n                # If this row has all required columns, treat it as the header\n                if not missing:\n                    found_header_row = row_idx\n                    break\n\n            if found_header_row is None:\n                # No valid header found in first 4 rows\n                val_result.missing_columns[\"\"] = [\n                    col.source_name\n                    for col in csv_table.columns\n                    if col.requirement != ColumnRequirement.OPTIONAL\n                ]\n                return False\n\n            # Successfully found header row\n            return True\n\n        except FormatValidationError as e:\n            logger.debug(\"CSV validation error: %s\", str(e))\n            return False\n\n    def _validate_json(self, path: Path, config, val_result: ValidationResult) -&gt; bool:\n        \"\"\"Validate JSON file structure.\"\"\"\n        try:\n            with open(path, encoding=\"utf-8\") as f:\n                data = json.load(f)\n\n            for json_table in config.tables:\n                if isinstance(data, list):\n                    if not data:\n                        val_result.missing_tables.append(json_table.name)\n                        continue\n                    record = data[0]\n                else:\n                    if json_table.name not in data:\n                        val_result.missing_tables.append(json_table.name)\n                        continue\n                    record = (\n                        data[json_table.name][0]\n                        if isinstance(data[json_table.name], list)\n                        else data[json_table.name]\n                    )\n\n                # Check required fields\n                fields = {k.lower() for k in record.keys()}\n                required_fields = {\n                    col.source_name.lower()\n                    for col in json_table.columns\n                    if col.requirement != ColumnRequirement.OPTIONAL\n                }\n                missing = required_fields - fields\n                if missing:\n                    val_result.missing_columns[json_table.name] = [\n                        col\n                        for col in json_table.columns\n                        if col.requirement != ColumnRequirement.OPTIONAL\n                        and col.source_name.lower() in missing\n                    ]\n\n            return not val_result.has_errors()\n\n        except FormatValidationError as e:\n            logger.debug(\"JSON validation error: %s\", str(e))\n            return False\n\n    def _validate_xml(self, path: Path, config, val_result: ValidationResult) -&gt; bool:\n        \"\"\"Validate XML file structure.\"\"\"\n        try:\n            tree = ET.parse(path)\n            root = tree.getroot()\n\n            for xml_table in config.tables:\n                elements = root.findall(f\".//{xml_table.name}\")\n                if not elements:\n                    val_result.missing_tables.append(xml_table.name)\n                    continue\n\n                # Check first element\n                element = elements[0]\n                fields = set()\n                fields.update(element.attrib.keys())\n                fields.update(child.tag for child in element)\n                fields = {f.lower() for f in fields}\n\n                required_fields = {\n                    col.source_name.lower()\n                    for col in xml_table.columns\n                    if col.requirement != ColumnRequirement.OPTIONAL\n                }\n                missing = required_fields - fields\n                if missing:\n                    val_result.missing_columns[xml_table.name] = [\n                        col\n                        for col in xml_table.columns\n                        if col.requirement != ColumnRequirement.OPTIONAL\n                        and col.source_name.lower() in missing\n                    ]\n\n            return not val_result.has_errors()\n\n        except FormatValidationError as e:\n            logger.debug(\"XML validation error: %s\", str(e))\n            return False\n</code></pre>"},{"location":"api/file_parser/#src.file_parser.format_detector.FormatDetector.__init__","title":"<code>__init__(format_registry: FormatRegistry)</code>","text":"<p>initialise detector with format registry.</p> Source code in <code>src/file_parser/format_detector.py</code> <pre><code>def __init__(self, format_registry: FormatRegistry):\n    \"\"\"initialise detector with format registry.\"\"\"\n    self._registry = format_registry\n</code></pre>"},{"location":"api/file_parser/#src.file_parser.format_detector.FormatDetector.detect_format","title":"<code>detect_format(path: Path) -&gt; Tuple[Optional[DeviceFormat], Optional[str], Dict[str, ValidationResult]]</code>","text":"<p>Detect format of provided file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the file to check</p> required <p>Returns:</p> Type Description <code>Tuple[Optional[DeviceFormat], Optional[str], Dict[str, ValidationResult]]</code> <p>Tuple containing: - Matched format (or None) - Error message (or None) - Dictionary of validation results per format tried</p> Example <p>detector = FormatDetector(registry) fmt, error, results = detector.detect_format(Path(\"data.sqlite\")) if fmt: ...     print(f\"Matched format: {fmt.name}\") ... else: ...     print(f\"No match: {error}\")</p> Source code in <code>src/file_parser/format_detector.py</code> <pre><code>def detect_format(\n    self, path: Path\n) -&gt; Tuple[Optional[DeviceFormat], Optional[str], Dict[str, ValidationResult]]:\n    \"\"\"Detect format of provided file.\n\n    Args:\n        path: Path to the file to check\n\n    Returns:\n        Tuple containing:\n            - Matched format (or None)\n            - Error message (or None)\n            - Dictionary of validation results per format tried\n\n    Example:\n        &gt;&gt;&gt; detector = FormatDetector(registry)\n        &gt;&gt;&gt; fmt, error, results = detector.detect_format(Path(\"data.sqlite\"))\n        &gt;&gt;&gt; if fmt:\n        ...     print(f\"Matched format: {fmt.name}\")\n        ... else:\n        ...     print(f\"No match: {error}\")\n    \"\"\"\n    logger.debug(\"Starting format detection for: %s\", path)\n    val_results = {}\n\n    try:\n        # Validate file exists and is readable\n        if not self._validate_file_exists(path):\n            return None, f\"File not found or not accessible: {path}\", {}\n\n        # Get potential formats based on file extension\n        potential_formats = self._registry.get_formats_for_file(path)\n        if not potential_formats:\n            return None, f\"No formats available for {path.suffix} files\", {}\n\n        # Try each format\n        for fmt in potential_formats:\n            try:\n                val_test_result = ValidationResult()\n                if self._validate_format(path, fmt, val_test_result):\n                    logger.debug(\"Successfully matched format: %s\", fmt.name)\n                    return fmt, None, val_results\n                val_results[fmt.name] = val_test_result\n            except FormatValidationError as e:\n                logger.debug(\"Error validating format %s: %s\", fmt.name, str(e))\n                continue\n\n        return None, \"No matching format found\", val_results\n\n    except FileAccessError as e:\n        logger.error(\"Unexpected error during format detection: %s\", str(e))\n        return None, f\"Detection error: {str(e)}\", {}\n</code></pre>"},{"location":"api/processors/","title":"Processors API","text":""},{"location":"api/processors/#src.processors","title":"<code>src.processors</code>","text":"<p>initialise processors package and register all processors.</p>"},{"location":"api/processors/#src.processors.__all__","title":"<code>__all__ = ['DataProcessor', 'CGMProcessor', 'BGMProcessor', 'CarbsProcessor', 'InsulinProcessor', 'NotesProcessor']</code>  <code>module-attribute</code>","text":""},{"location":"api/processors/#src.processors.DataProcessor","title":"<code>DataProcessor</code>","text":"<p>Main processor class that handles processing of all data types.</p> Source code in <code>src/processors/base.py</code> <pre><code>class DataProcessor:\n    \"\"\"Main processor class that handles processing of all data types.\"\"\"\n\n    _type_processors: Dict[DataType, Type[BaseTypeProcessor]] = {}\n\n    # Define defaults for insulin classification centrally\n    DEFAULT_INSULIN_BOLUS_LIMIT: float = 8.0\n    DEFAULT_INSULIN_MAX_DOSE: float = 15.0\n\n    @staticmethod\n    def create_table_configs(\n        detected_format: DeviceFormat,\n    ) -&gt; Dict[str, TableStructure]:\n        \"\"\"\n        Creates a table configuration dictionary from detected format.\n\n        Args:\n            detected_format: Format object containing files and their table configurations\n\n        Returns:\n            Dict[str, TableStructure]: Dictionary mapping table names to their structures\n        \"\"\"\n        try:\n            return {\n                table.name: table\n                for file_config in detected_format.files\n                for table in file_config.tables\n            }\n        except Exception as e:\n            logger.error(\"Failed to create table configurations: %s\", str(e))\n            raise ProcessingError(\"Failed to create table configurations\") from e\n\n    def process_tables(\n        self,\n        table_data: Dict[str, TableData],\n        detected_format: DeviceFormat,\n        interpolation_limit: Optional[\n            Any\n        ] = None,  # Optional parameter for CGM processor\n        bolus_limit: Optional[\n            float\n        ] = None,  # Optional parameters for insulin processor\n        max_dose: Optional[float] = None,\n    ) -&gt; Dict[DataType, ProcessedTypeData]:\n        \"\"\"\n        Process all tables according to their configuration.\n\n        Args:\n            table_data: Dictionary mapping table names to their data\n            detected_format: Format object containing table configurations\n            interpolation_limit: Max length of gaps to interpolate\n            bolus_limit: Maximum insulin dose to be classified as bolus(default = 8)\n            max_dose: Maximum insulin dose - all over will be discarded\n\n        Returns:\n            Dict[DataType, ProcessedTypeData]: Processed data organised by type\n        \"\"\"\n        table_configs = self.create_table_configs(detected_format)\n\n        # Rest of your existing process_tables implementation\n        type_data: Dict[DataType, List[ColumnData]] = {}\n\n        for table_name, data in table_data.items():\n            config = table_configs[table_name]\n\n            # Group columns by data type\n            for column in config.columns:\n                if column.data_type:\n                    # Include insulin meta data with insulin data\n                    target_type = (\n                        DataType.INSULIN\n                        if column.data_type == DataType.INSULIN_META\n                        else column.data_type\n                    )\n\n                    df_subset = data.dataframe[[column.source_name]].copy()\n                    df_subset.columns = [\"value\"]\n\n                    column_data = ColumnData(\n                        dataframe=df_subset,\n                        unit=column.unit,\n                        config=column,\n                        is_primary=column.is_primary,\n                    )\n\n                    if target_type not in type_data:\n                        type_data[target_type] = []\n\n                    type_data[target_type].append(column_data)\n\n        # Process each data type\n        results = {}\n        for data_type, columns in type_data.items():\n            try:\n                processor = self.get_processor_for_type(data_type)\n\n                # Inject optional parameters based on processor type\n                if data_type == DataType.CGM and interpolation_limit is not None:\n                    result = processor.process_type(columns, interpolation_limit)\n                elif data_type == DataType.INSULIN:\n                    # Use provided values or defaults, but never None\n                    final_bolus_limit = (\n                        bolus_limit\n                        if bolus_limit is not None\n                        else self.DEFAULT_INSULIN_BOLUS_LIMIT\n                    )\n                    final_max_dose = (\n                        max_dose\n                        if max_dose is not None\n                        else self.DEFAULT_INSULIN_MAX_DOSE\n                    )\n                    result = processor.process_type(\n                        columns, final_bolus_limit, final_max_dose\n                    )\n                else:\n                    result = processor.process_type(columns)\n\n                if not result.dataframe.empty:\n                    results[data_type] = result\n\n                    col_count = len(columns)\n                    primary_count = sum(1 for c in columns if c.is_primary)\n                    logger.info(\n                        \"    \\u2713 Processed %s: %d primary and %d secondary columns\",\n                        data_type.name,\n                        primary_count,\n                        col_count - primary_count,\n                    )\n\n            except ProcessingError as e:\n                logger.error(\"Error processing %s: %s\", data_type, str(e))\n                continue\n\n        return results\n\n    @classmethod\n    def register_processor(cls, data_type: DataType):\n        \"\"\"Register a processor class for a specific data type.\"\"\"\n\n        def wrapper(processor_cls: Type[BaseTypeProcessor]):\n            cls._type_processors[data_type] = processor_cls\n            return processor_cls\n\n        return wrapper\n\n    def get_processor_for_type(self, data_type: DataType) -&gt; BaseTypeProcessor:\n        \"\"\"Get appropriate processor instance for the data type.\"\"\"\n        processor_cls = self._type_processors.get(data_type)\n        if processor_cls is None:\n            raise ProcessingError(\n                f\"No processor registered for data type: {data_type.value}\"\n            )\n        return processor_cls()\n</code></pre>"},{"location":"api/processors/#src.processors.DataProcessor.DEFAULT_INSULIN_BOLUS_LIMIT","title":"<code>DEFAULT_INSULIN_BOLUS_LIMIT: float = 8.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/processors/#src.processors.DataProcessor.DEFAULT_INSULIN_MAX_DOSE","title":"<code>DEFAULT_INSULIN_MAX_DOSE: float = 15.0</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/processors/#src.processors.DataProcessor.create_table_configs","title":"<code>create_table_configs(detected_format: DeviceFormat) -&gt; Dict[str, TableStructure]</code>  <code>staticmethod</code>","text":"<p>Creates a table configuration dictionary from detected format.</p> <p>Parameters:</p> Name Type Description Default <code>detected_format</code> <code>DeviceFormat</code> <p>Format object containing files and their table configurations</p> required <p>Returns:</p> Type Description <code>Dict[str, TableStructure]</code> <p>Dict[str, TableStructure]: Dictionary mapping table names to their structures</p> Source code in <code>src/processors/base.py</code> <pre><code>@staticmethod\ndef create_table_configs(\n    detected_format: DeviceFormat,\n) -&gt; Dict[str, TableStructure]:\n    \"\"\"\n    Creates a table configuration dictionary from detected format.\n\n    Args:\n        detected_format: Format object containing files and their table configurations\n\n    Returns:\n        Dict[str, TableStructure]: Dictionary mapping table names to their structures\n    \"\"\"\n    try:\n        return {\n            table.name: table\n            for file_config in detected_format.files\n            for table in file_config.tables\n        }\n    except Exception as e:\n        logger.error(\"Failed to create table configurations: %s\", str(e))\n        raise ProcessingError(\"Failed to create table configurations\") from e\n</code></pre>"},{"location":"api/processors/#src.processors.DataProcessor.process_tables","title":"<code>process_tables(table_data: Dict[str, TableData], detected_format: DeviceFormat, interpolation_limit: Optional[Any] = None, bolus_limit: Optional[float] = None, max_dose: Optional[float] = None) -&gt; Dict[DataType, ProcessedTypeData]</code>","text":"<p>Process all tables according to their configuration.</p> <p>Parameters:</p> Name Type Description Default <code>table_data</code> <code>Dict[str, TableData]</code> <p>Dictionary mapping table names to their data</p> required <code>detected_format</code> <code>DeviceFormat</code> <p>Format object containing table configurations</p> required <code>interpolation_limit</code> <code>Optional[Any]</code> <p>Max length of gaps to interpolate</p> <code>None</code> <code>bolus_limit</code> <code>Optional[float]</code> <p>Maximum insulin dose to be classified as bolus(default = 8)</p> <code>None</code> <code>max_dose</code> <code>Optional[float]</code> <p>Maximum insulin dose - all over will be discarded</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[DataType, ProcessedTypeData]</code> <p>Dict[DataType, ProcessedTypeData]: Processed data organised by type</p> Source code in <code>src/processors/base.py</code> <pre><code>def process_tables(\n    self,\n    table_data: Dict[str, TableData],\n    detected_format: DeviceFormat,\n    interpolation_limit: Optional[\n        Any\n    ] = None,  # Optional parameter for CGM processor\n    bolus_limit: Optional[\n        float\n    ] = None,  # Optional parameters for insulin processor\n    max_dose: Optional[float] = None,\n) -&gt; Dict[DataType, ProcessedTypeData]:\n    \"\"\"\n    Process all tables according to their configuration.\n\n    Args:\n        table_data: Dictionary mapping table names to their data\n        detected_format: Format object containing table configurations\n        interpolation_limit: Max length of gaps to interpolate\n        bolus_limit: Maximum insulin dose to be classified as bolus(default = 8)\n        max_dose: Maximum insulin dose - all over will be discarded\n\n    Returns:\n        Dict[DataType, ProcessedTypeData]: Processed data organised by type\n    \"\"\"\n    table_configs = self.create_table_configs(detected_format)\n\n    # Rest of your existing process_tables implementation\n    type_data: Dict[DataType, List[ColumnData]] = {}\n\n    for table_name, data in table_data.items():\n        config = table_configs[table_name]\n\n        # Group columns by data type\n        for column in config.columns:\n            if column.data_type:\n                # Include insulin meta data with insulin data\n                target_type = (\n                    DataType.INSULIN\n                    if column.data_type == DataType.INSULIN_META\n                    else column.data_type\n                )\n\n                df_subset = data.dataframe[[column.source_name]].copy()\n                df_subset.columns = [\"value\"]\n\n                column_data = ColumnData(\n                    dataframe=df_subset,\n                    unit=column.unit,\n                    config=column,\n                    is_primary=column.is_primary,\n                )\n\n                if target_type not in type_data:\n                    type_data[target_type] = []\n\n                type_data[target_type].append(column_data)\n\n    # Process each data type\n    results = {}\n    for data_type, columns in type_data.items():\n        try:\n            processor = self.get_processor_for_type(data_type)\n\n            # Inject optional parameters based on processor type\n            if data_type == DataType.CGM and interpolation_limit is not None:\n                result = processor.process_type(columns, interpolation_limit)\n            elif data_type == DataType.INSULIN:\n                # Use provided values or defaults, but never None\n                final_bolus_limit = (\n                    bolus_limit\n                    if bolus_limit is not None\n                    else self.DEFAULT_INSULIN_BOLUS_LIMIT\n                )\n                final_max_dose = (\n                    max_dose\n                    if max_dose is not None\n                    else self.DEFAULT_INSULIN_MAX_DOSE\n                )\n                result = processor.process_type(\n                    columns, final_bolus_limit, final_max_dose\n                )\n            else:\n                result = processor.process_type(columns)\n\n            if not result.dataframe.empty:\n                results[data_type] = result\n\n                col_count = len(columns)\n                primary_count = sum(1 for c in columns if c.is_primary)\n                logger.info(\n                    \"    \\u2713 Processed %s: %d primary and %d secondary columns\",\n                    data_type.name,\n                    primary_count,\n                    col_count - primary_count,\n                )\n\n        except ProcessingError as e:\n            logger.error(\"Error processing %s: %s\", data_type, str(e))\n            continue\n\n    return results\n</code></pre>"},{"location":"api/processors/#src.processors.DataProcessor.register_processor","title":"<code>register_processor(data_type: DataType)</code>  <code>classmethod</code>","text":"<p>Register a processor class for a specific data type.</p> Source code in <code>src/processors/base.py</code> <pre><code>@classmethod\ndef register_processor(cls, data_type: DataType):\n    \"\"\"Register a processor class for a specific data type.\"\"\"\n\n    def wrapper(processor_cls: Type[BaseTypeProcessor]):\n        cls._type_processors[data_type] = processor_cls\n        return processor_cls\n\n    return wrapper\n</code></pre>"},{"location":"api/processors/#src.processors.DataProcessor.get_processor_for_type","title":"<code>get_processor_for_type(data_type: DataType) -&gt; BaseTypeProcessor</code>","text":"<p>Get appropriate processor instance for the data type.</p> Source code in <code>src/processors/base.py</code> <pre><code>def get_processor_for_type(self, data_type: DataType) -&gt; BaseTypeProcessor:\n    \"\"\"Get appropriate processor instance for the data type.\"\"\"\n    processor_cls = self._type_processors.get(data_type)\n    if processor_cls is None:\n        raise ProcessingError(\n            f\"No processor registered for data type: {data_type.value}\"\n        )\n    return processor_cls()\n</code></pre>"},{"location":"api/processors/#src.processors.BGMProcessor","title":"<code>BGMProcessor</code>","text":"<p>               Bases: <code>BaseTypeProcessor</code></p> <p>Processes BGM data with validation and cleaning.</p> Source code in <code>src/processors/bgm.py</code> <pre><code>@DataProcessor.register_processor(DataType.BGM)\nclass BGMProcessor(BaseTypeProcessor):\n    \"\"\"Processes BGM data with validation and cleaning.\"\"\"\n\n    def process_type(\n        self,\n        columns: List[ColumnData],\n    ) -&gt; ProcessedTypeData:\n        \"\"\"Process all BGM data from various sources.\n\n        Args:\n            columns: List of ColumnData containing all BGM data columns\n\n        Returns:\n            ProcessedTypeData containing combined and cleaned BGM data\n        \"\"\"\n        processing_notes = []\n\n        try:\n            # Validate we have at least one primary column\n            if not any(col.is_primary for col in columns):\n                raise ProcessingError(\"No primary BGM column found\")\n\n            # Create initial dataframe\n            combined_df = pd.DataFrame(index=pd.DatetimeIndex([]))\n            column_units = {}\n\n            # First pass - combine all data and convert units\n            for idx, col_data in enumerate(\n                sorted(columns, key=lambda x: (not x.is_primary))\n            ):\n                # Convert to mg/dL if needed\n                df = col_data.dataframe.copy()\n\n                # Filter out invalid/zero values before unit conversion\n                valid_mask = df[\"value\"] &gt; 0.0\n                invalid_count = (~valid_mask).sum()\n                if invalid_count &gt; 0:\n                    processing_notes.append(\n                        f\"Removed {invalid_count} invalid/zero values from column {idx + 1}\"\n                    )\n                df = df[valid_mask]\n\n                if col_data.unit == Unit.MMOL:\n                    df[\"value\"] = df[\"value\"] * 18.0182\n                    processing_notes.append(\n                        f\"Converted BGM column {idx + 1} from {Unit.MMOL.value} to {Unit.MGDL.value}\"\n                    )\n\n                # Generate column name\n                new_name = self._generate_column_name(\n                    DataType.BGM, col_data.is_primary, idx\n                )\n                df.columns = [new_name]\n\n                # Add clipped flag column before any clipping occurs\n                clipped_name = f\"{new_name}_clipped\"\n                df[clipped_name] = False\n\n                # Identify values outside valid range\n                below_range = df[new_name] &lt; 39.64\n                above_range = df[new_name] &gt; 360.36\n\n                # Update clipped flags\n                df.loc[below_range | above_range, clipped_name] = True\n\n                # Clip values to valid range\n                df[new_name] = df[new_name].clip(lower=39.64, upper=360.36)\n\n                # Log clipping statistics\n                clipped_count = (below_range | above_range).sum()\n                if clipped_count &gt; 0:\n                    processing_notes.append(\n                        f\"Clipped {clipped_count} values in column {new_name} \"\n                        f\"({below_range.sum()} below range, {above_range.sum()} above range)\"\n                    )\n\n                # Merge with existing data\n                if combined_df.empty:\n                    combined_df = df\n                else:\n                    combined_df = combined_df.join(df, how=\"outer\")\n\n                column_units[new_name] = Unit.MGDL\n\n            # Handle duplicate timestamps by keeping the last value\n            duplicates = combined_df.index.duplicated(keep=\"last\")\n            if duplicates.any():\n                dup_count = duplicates.sum()\n                processing_notes.append(\n                    f\"Found {dup_count} duplicate timestamps - keeping last values\"\n                )\n                combined_df = combined_df[~duplicates]\n\n            # Create mmol/L columns for each mg/dL column (excluding clipped flag columns)\n            value_columns = [\n                col for col in combined_df.columns if not col.endswith(\"_clipped\")\n            ]\n            for col in value_columns:\n                mmol_col = f\"{col}_mmol\"\n                combined_df[mmol_col] = combined_df[col] * 0.0555\n                column_units[mmol_col] = Unit.MMOL\n\n            # Track stats about the processed data\n            total_readings = len(combined_df)\n            readings_per_day = (\n                total_readings\n                / (\n                    (combined_df.index.max() - combined_df.index.min()).total_seconds()\n                    / 86400\n                )\n                if total_readings &gt; 0\n                else 0\n            )\n\n            processing_notes.extend(\n                [\n                    f\"Processed {total_readings} total BGM readings\",\n                    f\"Average of {readings_per_day:.1f} readings per day\",\n                    f\"Date range: {combined_df.index.min()} to {combined_df.index.max()}\",\n                ]\n            )\n\n            return ProcessedTypeData(\n                dataframe=combined_df,\n                source_units=column_units,\n                processing_notes=processing_notes,\n            )\n\n        except Exception as e:\n            raise ProcessingError(f\"Error processing BGM data: {str(e)}\") from e\n</code></pre>"},{"location":"api/processors/#src.processors.BGMProcessor.process_type","title":"<code>process_type(columns: List[ColumnData]) -&gt; ProcessedTypeData</code>","text":"<p>Process all BGM data from various sources.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>List[ColumnData]</code> <p>List of ColumnData containing all BGM data columns</p> required <p>Returns:</p> Type Description <code>ProcessedTypeData</code> <p>ProcessedTypeData containing combined and cleaned BGM data</p> Source code in <code>src/processors/bgm.py</code> <pre><code>def process_type(\n    self,\n    columns: List[ColumnData],\n) -&gt; ProcessedTypeData:\n    \"\"\"Process all BGM data from various sources.\n\n    Args:\n        columns: List of ColumnData containing all BGM data columns\n\n    Returns:\n        ProcessedTypeData containing combined and cleaned BGM data\n    \"\"\"\n    processing_notes = []\n\n    try:\n        # Validate we have at least one primary column\n        if not any(col.is_primary for col in columns):\n            raise ProcessingError(\"No primary BGM column found\")\n\n        # Create initial dataframe\n        combined_df = pd.DataFrame(index=pd.DatetimeIndex([]))\n        column_units = {}\n\n        # First pass - combine all data and convert units\n        for idx, col_data in enumerate(\n            sorted(columns, key=lambda x: (not x.is_primary))\n        ):\n            # Convert to mg/dL if needed\n            df = col_data.dataframe.copy()\n\n            # Filter out invalid/zero values before unit conversion\n            valid_mask = df[\"value\"] &gt; 0.0\n            invalid_count = (~valid_mask).sum()\n            if invalid_count &gt; 0:\n                processing_notes.append(\n                    f\"Removed {invalid_count} invalid/zero values from column {idx + 1}\"\n                )\n            df = df[valid_mask]\n\n            if col_data.unit == Unit.MMOL:\n                df[\"value\"] = df[\"value\"] * 18.0182\n                processing_notes.append(\n                    f\"Converted BGM column {idx + 1} from {Unit.MMOL.value} to {Unit.MGDL.value}\"\n                )\n\n            # Generate column name\n            new_name = self._generate_column_name(\n                DataType.BGM, col_data.is_primary, idx\n            )\n            df.columns = [new_name]\n\n            # Add clipped flag column before any clipping occurs\n            clipped_name = f\"{new_name}_clipped\"\n            df[clipped_name] = False\n\n            # Identify values outside valid range\n            below_range = df[new_name] &lt; 39.64\n            above_range = df[new_name] &gt; 360.36\n\n            # Update clipped flags\n            df.loc[below_range | above_range, clipped_name] = True\n\n            # Clip values to valid range\n            df[new_name] = df[new_name].clip(lower=39.64, upper=360.36)\n\n            # Log clipping statistics\n            clipped_count = (below_range | above_range).sum()\n            if clipped_count &gt; 0:\n                processing_notes.append(\n                    f\"Clipped {clipped_count} values in column {new_name} \"\n                    f\"({below_range.sum()} below range, {above_range.sum()} above range)\"\n                )\n\n            # Merge with existing data\n            if combined_df.empty:\n                combined_df = df\n            else:\n                combined_df = combined_df.join(df, how=\"outer\")\n\n            column_units[new_name] = Unit.MGDL\n\n        # Handle duplicate timestamps by keeping the last value\n        duplicates = combined_df.index.duplicated(keep=\"last\")\n        if duplicates.any():\n            dup_count = duplicates.sum()\n            processing_notes.append(\n                f\"Found {dup_count} duplicate timestamps - keeping last values\"\n            )\n            combined_df = combined_df[~duplicates]\n\n        # Create mmol/L columns for each mg/dL column (excluding clipped flag columns)\n        value_columns = [\n            col for col in combined_df.columns if not col.endswith(\"_clipped\")\n        ]\n        for col in value_columns:\n            mmol_col = f\"{col}_mmol\"\n            combined_df[mmol_col] = combined_df[col] * 0.0555\n            column_units[mmol_col] = Unit.MMOL\n\n        # Track stats about the processed data\n        total_readings = len(combined_df)\n        readings_per_day = (\n            total_readings\n            / (\n                (combined_df.index.max() - combined_df.index.min()).total_seconds()\n                / 86400\n            )\n            if total_readings &gt; 0\n            else 0\n        )\n\n        processing_notes.extend(\n            [\n                f\"Processed {total_readings} total BGM readings\",\n                f\"Average of {readings_per_day:.1f} readings per day\",\n                f\"Date range: {combined_df.index.min()} to {combined_df.index.max()}\",\n            ]\n        )\n\n        return ProcessedTypeData(\n            dataframe=combined_df,\n            source_units=column_units,\n            processing_notes=processing_notes,\n        )\n\n    except Exception as e:\n        raise ProcessingError(f\"Error processing BGM data: {str(e)}\") from e\n</code></pre>"},{"location":"api/processors/#src.processors.CarbsProcessor","title":"<code>CarbsProcessor</code>","text":"<p>               Bases: <code>BaseTypeProcessor</code></p> <p>Processes carbohydrate intake data with validation and cleaning.</p> Source code in <code>src/processors/carbs.py</code> <pre><code>@DataProcessor.register_processor(DataType.CARBS)\nclass CarbsProcessor(BaseTypeProcessor):\n    \"\"\"Processes carbohydrate intake data with validation and cleaning.\"\"\"\n\n    def process_type(\n        self,\n        columns: List[ColumnData],\n    ) -&gt; ProcessedTypeData:\n        \"\"\"Process all carbohydrate data from various sources.\n\n        Args:\n            columns: List of ColumnData containing all carb data columns\n\n        Returns:\n            ProcessedTypeData containing combined and cleaned carb data\n        \"\"\"\n        processing_notes = []\n\n        try:\n            # Validate we have at least one primary column\n            if not any(col.is_primary for col in columns):\n                raise ProcessingError(\"No primary carbohydrate column found\")\n\n            # Sort columns to ensure primary is first\n            sorted_columns = sorted(columns, key=lambda x: (not x.is_primary))\n\n            # Combine all columns with standardized names\n            combined_df, column_units = self._combine_and_rename_columns(\n                sorted_columns, DataType.CARBS\n            )\n\n            if combined_df.empty:\n                raise ProcessingError(\"No carbohydrate data to process\")\n\n            # Log what we're processing\n            processing_notes.append(\n                f\"Processing {len(combined_df.columns)} carb columns: {', '.join(combined_df.columns)}\"\n            )\n\n            # Track original row count\n            original_count = len(combined_df)\n\n            # Process each carb column\n            for col in combined_df.columns:\n                # Keep only rows where carbs is &gt;= 1.0 grams\n                mask = combined_df[col] &gt;= 1.0\n                combined_df.loc[~mask, col] = None\n\n                filtered_count = mask.sum()\n                processing_notes.append(\n                    f\"Column {col}: Kept {filtered_count} entries \u22651g \"\n                    f\"({filtered_count / original_count * 100:.1f}%)\"\n                )\n\n            # Drop rows where all values are null (no significant carbs in any column)\n            combined_df = combined_df.dropna(how=\"all\")\n\n            # Handle duplicate timestamps by keeping the first occurrence\n            duplicates = combined_df.index.duplicated()\n            if duplicates.any():\n                dup_count = duplicates.sum()\n                processing_notes.append(f\"Removed {dup_count} duplicate timestamps\")\n                combined_df = combined_df[~duplicates]\n\n            # Final stats\n            processing_notes.append(\n                f\"Final dataset contains {len(combined_df)} entries \"\n                f\"from {original_count} original records\"\n            )\n\n            return ProcessedTypeData(\n                dataframe=combined_df,\n                source_units=column_units,\n                processing_notes=processing_notes,\n            )\n\n        except Exception as e:\n            raise ProcessingError(\n                f\"Error processing carbohydrate data: {str(e)}\"\n            ) from e\n</code></pre>"},{"location":"api/processors/#src.processors.CarbsProcessor.process_type","title":"<code>process_type(columns: List[ColumnData]) -&gt; ProcessedTypeData</code>","text":"<p>Process all carbohydrate data from various sources.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>List[ColumnData]</code> <p>List of ColumnData containing all carb data columns</p> required <p>Returns:</p> Type Description <code>ProcessedTypeData</code> <p>ProcessedTypeData containing combined and cleaned carb data</p> Source code in <code>src/processors/carbs.py</code> <pre><code>def process_type(\n    self,\n    columns: List[ColumnData],\n) -&gt; ProcessedTypeData:\n    \"\"\"Process all carbohydrate data from various sources.\n\n    Args:\n        columns: List of ColumnData containing all carb data columns\n\n    Returns:\n        ProcessedTypeData containing combined and cleaned carb data\n    \"\"\"\n    processing_notes = []\n\n    try:\n        # Validate we have at least one primary column\n        if not any(col.is_primary for col in columns):\n            raise ProcessingError(\"No primary carbohydrate column found\")\n\n        # Sort columns to ensure primary is first\n        sorted_columns = sorted(columns, key=lambda x: (not x.is_primary))\n\n        # Combine all columns with standardized names\n        combined_df, column_units = self._combine_and_rename_columns(\n            sorted_columns, DataType.CARBS\n        )\n\n        if combined_df.empty:\n            raise ProcessingError(\"No carbohydrate data to process\")\n\n        # Log what we're processing\n        processing_notes.append(\n            f\"Processing {len(combined_df.columns)} carb columns: {', '.join(combined_df.columns)}\"\n        )\n\n        # Track original row count\n        original_count = len(combined_df)\n\n        # Process each carb column\n        for col in combined_df.columns:\n            # Keep only rows where carbs is &gt;= 1.0 grams\n            mask = combined_df[col] &gt;= 1.0\n            combined_df.loc[~mask, col] = None\n\n            filtered_count = mask.sum()\n            processing_notes.append(\n                f\"Column {col}: Kept {filtered_count} entries \u22651g \"\n                f\"({filtered_count / original_count * 100:.1f}%)\"\n            )\n\n        # Drop rows where all values are null (no significant carbs in any column)\n        combined_df = combined_df.dropna(how=\"all\")\n\n        # Handle duplicate timestamps by keeping the first occurrence\n        duplicates = combined_df.index.duplicated()\n        if duplicates.any():\n            dup_count = duplicates.sum()\n            processing_notes.append(f\"Removed {dup_count} duplicate timestamps\")\n            combined_df = combined_df[~duplicates]\n\n        # Final stats\n        processing_notes.append(\n            f\"Final dataset contains {len(combined_df)} entries \"\n            f\"from {original_count} original records\"\n        )\n\n        return ProcessedTypeData(\n            dataframe=combined_df,\n            source_units=column_units,\n            processing_notes=processing_notes,\n        )\n\n    except Exception as e:\n        raise ProcessingError(\n            f\"Error processing carbohydrate data: {str(e)}\"\n        ) from e\n</code></pre>"},{"location":"api/processors/#src.processors.CGMProcessor","title":"<code>CGMProcessor</code>","text":"<p>               Bases: <code>BaseTypeProcessor</code></p> <p>Processes CGM data with validation and cleaning.</p> Source code in <code>src/processors/cgm.py</code> <pre><code>@DataProcessor.register_processor(DataType.CGM)\nclass CGMProcessor(BaseTypeProcessor):\n    \"\"\"Processes CGM data with validation and cleaning.\"\"\"\n\n    def process_type(\n        self,\n        columns: List[ColumnData],\n        interpolation_limit: int = 4,\n    ) -&gt; ProcessedTypeData:\n        \"\"\"Process all CGM data from various sources.\n\n        Args:\n            columns: List of ColumnData containing all CGM data columns\n            interpolation_limit: Maximum number of consecutive missing values to interpolate\n                               (default: 4, equivalent to 20 minutes at 5-min intervals)\n\n        Returns:\n            ProcessedTypeData containing combined and cleaned CGM data\n        \"\"\"\n        processing_notes = []\n\n        try:\n            # Validate we have at least one primary column\n            if not any(col.is_primary for col in columns):\n                raise ProcessingError(\"No primary CGM column found\")\n\n            # Sort columns to ensure primary is first\n            sorted_columns = sorted(columns, key=lambda x: (not x.is_primary))\n\n            # Create initial dataframe\n            combined_df = pd.DataFrame(index=pd.DatetimeIndex([]))\n            column_units = {}\n\n            # First pass - combine all data and convert units\n            for idx, col_data in enumerate(sorted_columns):\n                # Convert to mg/dL if needed\n                df = col_data.dataframe.copy()\n                if col_data.unit == Unit.MMOL:\n                    df[\"value\"] = df[\"value\"] * 18.0182\n                    processing_notes.append(\n                        f\"Converted CGM column {idx + 1} from {Unit.MMOL.value} to {Unit.MGDL.value}\"\n                    )\n\n                # Generate column name\n                new_name = self._generate_column_name(\n                    DataType.CGM, col_data.is_primary, idx\n                )\n                df.columns = [new_name]\n\n                # Merge with existing data\n                if combined_df.empty:\n                    combined_df = df\n                else:\n                    combined_df = combined_df.join(df, how=\"outer\")\n\n                column_units[new_name] = col_data.unit\n\n            # Round all timestamps to nearest 5 minute interval\n            combined_df.index = combined_df.index.round(\"5min\")\n\n            # Handle duplicate times by taking mean\n            combined_df = combined_df.groupby(level=0).mean()\n\n            # Create complete 5-minute interval index\n            full_index = pd.date_range(\n                start=combined_df.index.min(), end=combined_df.index.max(), freq=\"5min\"\n            )\n\n            # Reindex to include all intervals\n            combined_df = combined_df.reindex(full_index)\n\n            # Get primary column name\n            primary_col = next(col for col in combined_df.columns if \"primary\" in col)\n\n            # Create missing flags for each column\n            missing_flags = pd.DataFrame(index=combined_df.index)\n            missing_flags[\"missing\"] = combined_df[primary_col].isna()\n\n            # Handle interpolation for each column\n            for col in combined_df.columns:\n                # Save original missing mask BEFORE interpolation so we can\n                # reliably revert fills that span larger-than-allowed gaps.\n                orig_missing = combined_df[col].isna()\n\n                # Create groups of consecutive non-missing values (group id per block)\n                gap_groups = (~orig_missing).cumsum()\n\n                # Count missing entries per group (these are the gap sizes)\n                # Use sum on boolean series which counts True as 1\n                gap_size = orig_missing.groupby(gap_groups).sum()\n\n                # Identify gap groups that are larger than interpolation_limit\n                large_gaps = gap_size[gap_size &gt; interpolation_limit].index\n\n                # Interpolate all gaps initially (honours pandas 'limit')\n                combined_df[col] = combined_df[col].interpolate(\n                    method=\"linear\",\n                    limit=interpolation_limit,\n                    limit_direction=\"forward\",\n                )\n\n                # Revert interpolated values back to NaN for the large gaps we\n                # identified using the original missing mask. We use the saved\n                # orig_missing mask here because after interpolation those\n                # positions are no longer NaN.\n                if len(large_gaps) &gt; 0:\n                    reset_mask = orig_missing &amp; gap_groups.isin(large_gaps)\n                    combined_df.loc[reset_mask, col] = np.nan\n\n                # Clip values to valid range\n                combined_df[col] = combined_df[col].clip(lower=39.64, upper=360.36)\n\n            # Create mmol/L columns for each mg/dL column\n            for col in combined_df.columns.copy():\n                mmol_col = f\"{col}_mmol\"\n                combined_df[mmol_col] = combined_df[col] * 0.0555\n                column_units[mmol_col] = Unit.MMOL\n                column_units[col] = Unit.MGDL\n\n            # Add the missing flags\n            combined_df[\"missing\"] = missing_flags[\"missing\"]\n\n            # Track stats about the processed data\n            total_readings = len(combined_df)\n            missing_primary = combined_df[\"missing\"].sum()\n            total_na = combined_df[\"cgm_primary\"].isna().sum()\n            initial_completeness_percent = (\n                (total_readings - missing_primary) / total_readings\n            ) * 100\n            remaining_completeness_percent = (\n                (total_readings - total_na) / total_readings\n            ) * 100\n            processing_notes.extend(\n                [\n                    f\"Processed {total_readings} total CGM readings\",\n                    f\"Found {missing_primary} missing or interpolated values in primary data\",\n                    f\"Found {total_na} missing values after interpolation\",\n                    f\"Initial CGM dataset completeness: {initial_completeness_percent:.2f}%\",\n                    f\"CGM completeness after interpolation: {remaining_completeness_percent:.2f}%\",\n                ]\n            )\n\n            return ProcessedTypeData(\n                dataframe=combined_df,\n                source_units=column_units,\n                processing_notes=processing_notes,\n            )\n\n        except Exception as e:\n            raise ProcessingError(f\"Error processing CGM data: {str(e)}\") from e\n</code></pre>"},{"location":"api/processors/#src.processors.CGMProcessor.process_type","title":"<code>process_type(columns: List[ColumnData], interpolation_limit: int = 4) -&gt; ProcessedTypeData</code>","text":"<p>Process all CGM data from various sources.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>List[ColumnData]</code> <p>List of ColumnData containing all CGM data columns</p> required <code>interpolation_limit</code> <code>int</code> <p>Maximum number of consecutive missing values to interpolate                (default: 4, equivalent to 20 minutes at 5-min intervals)</p> <code>4</code> <p>Returns:</p> Type Description <code>ProcessedTypeData</code> <p>ProcessedTypeData containing combined and cleaned CGM data</p> Source code in <code>src/processors/cgm.py</code> <pre><code>def process_type(\n    self,\n    columns: List[ColumnData],\n    interpolation_limit: int = 4,\n) -&gt; ProcessedTypeData:\n    \"\"\"Process all CGM data from various sources.\n\n    Args:\n        columns: List of ColumnData containing all CGM data columns\n        interpolation_limit: Maximum number of consecutive missing values to interpolate\n                           (default: 4, equivalent to 20 minutes at 5-min intervals)\n\n    Returns:\n        ProcessedTypeData containing combined and cleaned CGM data\n    \"\"\"\n    processing_notes = []\n\n    try:\n        # Validate we have at least one primary column\n        if not any(col.is_primary for col in columns):\n            raise ProcessingError(\"No primary CGM column found\")\n\n        # Sort columns to ensure primary is first\n        sorted_columns = sorted(columns, key=lambda x: (not x.is_primary))\n\n        # Create initial dataframe\n        combined_df = pd.DataFrame(index=pd.DatetimeIndex([]))\n        column_units = {}\n\n        # First pass - combine all data and convert units\n        for idx, col_data in enumerate(sorted_columns):\n            # Convert to mg/dL if needed\n            df = col_data.dataframe.copy()\n            if col_data.unit == Unit.MMOL:\n                df[\"value\"] = df[\"value\"] * 18.0182\n                processing_notes.append(\n                    f\"Converted CGM column {idx + 1} from {Unit.MMOL.value} to {Unit.MGDL.value}\"\n                )\n\n            # Generate column name\n            new_name = self._generate_column_name(\n                DataType.CGM, col_data.is_primary, idx\n            )\n            df.columns = [new_name]\n\n            # Merge with existing data\n            if combined_df.empty:\n                combined_df = df\n            else:\n                combined_df = combined_df.join(df, how=\"outer\")\n\n            column_units[new_name] = col_data.unit\n\n        # Round all timestamps to nearest 5 minute interval\n        combined_df.index = combined_df.index.round(\"5min\")\n\n        # Handle duplicate times by taking mean\n        combined_df = combined_df.groupby(level=0).mean()\n\n        # Create complete 5-minute interval index\n        full_index = pd.date_range(\n            start=combined_df.index.min(), end=combined_df.index.max(), freq=\"5min\"\n        )\n\n        # Reindex to include all intervals\n        combined_df = combined_df.reindex(full_index)\n\n        # Get primary column name\n        primary_col = next(col for col in combined_df.columns if \"primary\" in col)\n\n        # Create missing flags for each column\n        missing_flags = pd.DataFrame(index=combined_df.index)\n        missing_flags[\"missing\"] = combined_df[primary_col].isna()\n\n        # Handle interpolation for each column\n        for col in combined_df.columns:\n            # Save original missing mask BEFORE interpolation so we can\n            # reliably revert fills that span larger-than-allowed gaps.\n            orig_missing = combined_df[col].isna()\n\n            # Create groups of consecutive non-missing values (group id per block)\n            gap_groups = (~orig_missing).cumsum()\n\n            # Count missing entries per group (these are the gap sizes)\n            # Use sum on boolean series which counts True as 1\n            gap_size = orig_missing.groupby(gap_groups).sum()\n\n            # Identify gap groups that are larger than interpolation_limit\n            large_gaps = gap_size[gap_size &gt; interpolation_limit].index\n\n            # Interpolate all gaps initially (honours pandas 'limit')\n            combined_df[col] = combined_df[col].interpolate(\n                method=\"linear\",\n                limit=interpolation_limit,\n                limit_direction=\"forward\",\n            )\n\n            # Revert interpolated values back to NaN for the large gaps we\n            # identified using the original missing mask. We use the saved\n            # orig_missing mask here because after interpolation those\n            # positions are no longer NaN.\n            if len(large_gaps) &gt; 0:\n                reset_mask = orig_missing &amp; gap_groups.isin(large_gaps)\n                combined_df.loc[reset_mask, col] = np.nan\n\n            # Clip values to valid range\n            combined_df[col] = combined_df[col].clip(lower=39.64, upper=360.36)\n\n        # Create mmol/L columns for each mg/dL column\n        for col in combined_df.columns.copy():\n            mmol_col = f\"{col}_mmol\"\n            combined_df[mmol_col] = combined_df[col] * 0.0555\n            column_units[mmol_col] = Unit.MMOL\n            column_units[col] = Unit.MGDL\n\n        # Add the missing flags\n        combined_df[\"missing\"] = missing_flags[\"missing\"]\n\n        # Track stats about the processed data\n        total_readings = len(combined_df)\n        missing_primary = combined_df[\"missing\"].sum()\n        total_na = combined_df[\"cgm_primary\"].isna().sum()\n        initial_completeness_percent = (\n            (total_readings - missing_primary) / total_readings\n        ) * 100\n        remaining_completeness_percent = (\n            (total_readings - total_na) / total_readings\n        ) * 100\n        processing_notes.extend(\n            [\n                f\"Processed {total_readings} total CGM readings\",\n                f\"Found {missing_primary} missing or interpolated values in primary data\",\n                f\"Found {total_na} missing values after interpolation\",\n                f\"Initial CGM dataset completeness: {initial_completeness_percent:.2f}%\",\n                f\"CGM completeness after interpolation: {remaining_completeness_percent:.2f}%\",\n            ]\n        )\n\n        return ProcessedTypeData(\n            dataframe=combined_df,\n            source_units=column_units,\n            processing_notes=processing_notes,\n        )\n\n    except Exception as e:\n        raise ProcessingError(f\"Error processing CGM data: {str(e)}\") from e\n</code></pre>"},{"location":"api/processors/#src.processors.InsulinProcessor","title":"<code>InsulinProcessor</code>","text":"<p>               Bases: <code>BaseTypeProcessor</code></p> <p>Processes insulin dose data with classification from meta data if available.</p> Source code in <code>src/processors/insulin.py</code> <pre><code>@DataProcessor.register_processor(DataType.INSULIN)\nclass InsulinProcessor(BaseTypeProcessor):\n    \"\"\"Processes insulin dose data with classification from meta data if available.\"\"\"\n\n    def _extract_meta_info(self, meta_value: str) -&gt; Tuple[bool, bool, Optional[str]]:\n        \"\"\"Extract insulin type information from meta JSON.\n\n        Returns:\n            Tuple of (is_bolus, is_basal, insulin_type)\n        \"\"\"\n        try:\n            meta_data = json.loads(meta_value)\n            if meta_data and isinstance(meta_data, list):\n                insulin = meta_data[0].get(\"insulin\", \"\").lower()\n                if \"novorapid\" in insulin:\n                    return True, False, \"novorapid\"\n                if \"levemir\" in insulin:\n                    return False, True, \"levemir\"\n        except (json.JSONDecodeError, IndexError, KeyError, AttributeError):\n            pass\n\n        return False, False, None\n\n    def process_type(\n        self,\n        columns: List[ColumnData],\n        bolus_limit: float = 8.0,\n        max_limit: float = 15.0,\n    ) -&gt; ProcessedTypeData:\n        \"\"\"Process insulin data and classify doses.\n\n        Args:\n            columns: List of ColumnData containing insulin data and metadata\n            bolus_limit: Maximum insulin units to classify as bolus\n            max_limit: Maximum valid insulin dose\n        \"\"\"\n        processing_notes = []\n\n        try:\n            # Find insulin dose and meta columns\n            dose_cols = [col for col in columns if col.data_type == DataType.INSULIN]\n            meta_cols = [\n                col for col in columns if col.data_type == DataType.INSULIN_META\n            ]\n\n            if not any(col.is_primary for col in dose_cols):\n                raise ProcessingError(\"No primary insulin dose column found\")\n\n            # initialise result DataFrame with dose data\n            result_df = pd.DataFrame()\n            result_units = {}\n\n            # Process dose data first\n            for col in dose_cols:\n                df = col.dataframe.copy()\n\n                # Keep only positive doses\n                valid_mask = df[\"value\"] &gt; 0.0\n                df = df[valid_mask]\n\n                if len(df) &gt; 0:\n                    processing_notes.append(f\"Found {len(df)} positive doses\")\n\n                    # Add dose column\n                    result_df[\"dose\"] = df[\"value\"]\n                    result_units[\"dose\"] = col.unit\n\n                    # Initial classification based on dose\n                    result_df[\"is_bolus\"] = df[\"value\"] &lt;= bolus_limit\n                    result_df[\"is_basal\"] = (df[\"value\"] &gt; bolus_limit) &amp; (\n                        df[\"value\"] &lt;= max_limit\n                    )\n                    result_df[\"type\"] = \"\"  # Will be filled by metadata if available\n\n                    # Track classification stats\n                    processing_notes.extend(\n                        [\n                            \"Initial dose-based classification:\",\n                            f\"- {result_df['is_bolus'].sum()} doses classified as bolus (\u2264{bolus_limit}U)\",\n                            f\"- {result_df['is_basal'].sum()} doses classified as basal (&gt;{bolus_limit}U)\",\n                            f\"- Dropped {(df['value'] &gt; max_limit).sum()} doses exceeding {max_limit}U\",\n                        ]\n                    )\n\n            # Update classification with metadata if available\n            if meta_cols and not result_df.empty:\n                meta_updates = 0\n                for col in meta_cols:\n                    for idx, meta_value in col.dataframe[\"value\"].items():\n                        if idx in result_df.index:\n                            is_bolus, is_basal, insulin_type = self._extract_meta_info(\n                                meta_value\n                            )\n                            if insulin_type:\n                                result_df.loc[idx, \"is_bolus\"] = is_bolus\n                                result_df.loc[idx, \"is_basal\"] = is_basal\n                                result_df.loc[idx, \"type\"] = insulin_type\n                                meta_updates += 1\n\n                if meta_updates &gt; 0:\n                    processing_notes.append(\n                        f\"Updated {meta_updates} classifications using metadata\"\n                    )\n\n            # Final stats\n            processing_notes.append(\n                f\"Final dataset contains {len(result_df)} insulin records\"\n            )\n\n            return ProcessedTypeData(\n                dataframe=result_df,\n                source_units=result_units,\n                processing_notes=processing_notes,\n            )\n\n        except Exception as e:\n            raise ProcessingError(f\"Error processing insulin data: {str(e)}\") from e\n</code></pre>"},{"location":"api/processors/#src.processors.InsulinProcessor.process_type","title":"<code>process_type(columns: List[ColumnData], bolus_limit: float = 8.0, max_limit: float = 15.0) -&gt; ProcessedTypeData</code>","text":"<p>Process insulin data and classify doses.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>List[ColumnData]</code> <p>List of ColumnData containing insulin data and metadata</p> required <code>bolus_limit</code> <code>float</code> <p>Maximum insulin units to classify as bolus</p> <code>8.0</code> <code>max_limit</code> <code>float</code> <p>Maximum valid insulin dose</p> <code>15.0</code> Source code in <code>src/processors/insulin.py</code> <pre><code>def process_type(\n    self,\n    columns: List[ColumnData],\n    bolus_limit: float = 8.0,\n    max_limit: float = 15.0,\n) -&gt; ProcessedTypeData:\n    \"\"\"Process insulin data and classify doses.\n\n    Args:\n        columns: List of ColumnData containing insulin data and metadata\n        bolus_limit: Maximum insulin units to classify as bolus\n        max_limit: Maximum valid insulin dose\n    \"\"\"\n    processing_notes = []\n\n    try:\n        # Find insulin dose and meta columns\n        dose_cols = [col for col in columns if col.data_type == DataType.INSULIN]\n        meta_cols = [\n            col for col in columns if col.data_type == DataType.INSULIN_META\n        ]\n\n        if not any(col.is_primary for col in dose_cols):\n            raise ProcessingError(\"No primary insulin dose column found\")\n\n        # initialise result DataFrame with dose data\n        result_df = pd.DataFrame()\n        result_units = {}\n\n        # Process dose data first\n        for col in dose_cols:\n            df = col.dataframe.copy()\n\n            # Keep only positive doses\n            valid_mask = df[\"value\"] &gt; 0.0\n            df = df[valid_mask]\n\n            if len(df) &gt; 0:\n                processing_notes.append(f\"Found {len(df)} positive doses\")\n\n                # Add dose column\n                result_df[\"dose\"] = df[\"value\"]\n                result_units[\"dose\"] = col.unit\n\n                # Initial classification based on dose\n                result_df[\"is_bolus\"] = df[\"value\"] &lt;= bolus_limit\n                result_df[\"is_basal\"] = (df[\"value\"] &gt; bolus_limit) &amp; (\n                    df[\"value\"] &lt;= max_limit\n                )\n                result_df[\"type\"] = \"\"  # Will be filled by metadata if available\n\n                # Track classification stats\n                processing_notes.extend(\n                    [\n                        \"Initial dose-based classification:\",\n                        f\"- {result_df['is_bolus'].sum()} doses classified as bolus (\u2264{bolus_limit}U)\",\n                        f\"- {result_df['is_basal'].sum()} doses classified as basal (&gt;{bolus_limit}U)\",\n                        f\"- Dropped {(df['value'] &gt; max_limit).sum()} doses exceeding {max_limit}U\",\n                    ]\n                )\n\n        # Update classification with metadata if available\n        if meta_cols and not result_df.empty:\n            meta_updates = 0\n            for col in meta_cols:\n                for idx, meta_value in col.dataframe[\"value\"].items():\n                    if idx in result_df.index:\n                        is_bolus, is_basal, insulin_type = self._extract_meta_info(\n                            meta_value\n                        )\n                        if insulin_type:\n                            result_df.loc[idx, \"is_bolus\"] = is_bolus\n                            result_df.loc[idx, \"is_basal\"] = is_basal\n                            result_df.loc[idx, \"type\"] = insulin_type\n                            meta_updates += 1\n\n            if meta_updates &gt; 0:\n                processing_notes.append(\n                    f\"Updated {meta_updates} classifications using metadata\"\n                )\n\n        # Final stats\n        processing_notes.append(\n            f\"Final dataset contains {len(result_df)} insulin records\"\n        )\n\n        return ProcessedTypeData(\n            dataframe=result_df,\n            source_units=result_units,\n            processing_notes=processing_notes,\n        )\n\n    except Exception as e:\n        raise ProcessingError(f\"Error processing insulin data: {str(e)}\") from e\n</code></pre>"},{"location":"api/processors/#src.processors.NotesProcessor","title":"<code>NotesProcessor</code>","text":"<p>               Bases: <code>BaseTypeProcessor</code></p> <p>Processes text notes/comments data.</p> Source code in <code>src/processors/notes.py</code> <pre><code>@DataProcessor.register_processor(DataType.NOTES)\nclass NotesProcessor(BaseTypeProcessor):\n    \"\"\"Processes text notes/comments data.\"\"\"\n\n    def process_type(\n        self,\n        columns: List[ColumnData],\n    ) -&gt; ProcessedTypeData:\n        \"\"\"Process notes data ensuring safe string storage.\n\n        Args:\n            columns: List of ColumnData containing notes columns\n\n        Returns:\n            ProcessedTypeData containing processed notes\n        \"\"\"\n        processing_notes = []\n\n        try:\n            # Validate we have at least one primary column\n            if not any(col.is_primary for col in columns):\n                raise ProcessingError(\"No primary notes column found\")\n\n            # Sort columns to ensure primary first\n            sorted_columns = sorted(columns, key=lambda x: (not x.is_primary))\n\n            # initialise result dataframe\n            result_df = pd.DataFrame(index=pd.DatetimeIndex([]))\n\n            # Process each column\n            for idx, col_data in enumerate(sorted_columns):\n                # Generate column name\n                col_name = self._generate_column_name(\n                    DataType.NOTES, col_data.is_primary, idx\n                )\n\n                # Get the notes series\n                notes_series = col_data.dataframe[\"value\"]\n\n                # Replace None with pd.NA for better handling\n                notes_series = notes_series.replace([None], pd.NA)\n\n                # Convert non-NA values to string and strip whitespace\n                notes_series = notes_series.apply(\n                    lambda x: x.strip() if pd.notna(x) else pd.NA\n                )\n\n                # Remove empty strings\n                notes_series = notes_series.replace({\"\": pd.NA})\n\n                # Add to result DataFrame only if we have any valid notes\n                if not notes_series.isna().all():\n                    result_df[col_name] = notes_series\n\n                    # Track stats\n                    valid_notes = notes_series.notna()\n                    processing_notes.append(\n                        f\"Column {col_name}: found {valid_notes.sum()} valid notes\"\n                    )\n\n            # Drop rows where all values are NA\n            if not result_df.empty:\n                result_df = result_df.dropna(how=\"all\")\n\n            processing_notes.append(\n                f\"Final dataset contains {len(result_df)} notes entries\"\n            )\n\n            return ProcessedTypeData(\n                dataframe=result_df,\n                source_units={},  # No units for text data\n                processing_notes=processing_notes,\n            )\n\n        except Exception as e:\n            raise ProcessingError(f\"Error processing notes data: {str(e)}\") from e\n</code></pre>"},{"location":"api/processors/#src.processors.NotesProcessor.process_type","title":"<code>process_type(columns: List[ColumnData]) -&gt; ProcessedTypeData</code>","text":"<p>Process notes data ensuring safe string storage.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>List[ColumnData]</code> <p>List of ColumnData containing notes columns</p> required <p>Returns:</p> Type Description <code>ProcessedTypeData</code> <p>ProcessedTypeData containing processed notes</p> Source code in <code>src/processors/notes.py</code> <pre><code>def process_type(\n    self,\n    columns: List[ColumnData],\n) -&gt; ProcessedTypeData:\n    \"\"\"Process notes data ensuring safe string storage.\n\n    Args:\n        columns: List of ColumnData containing notes columns\n\n    Returns:\n        ProcessedTypeData containing processed notes\n    \"\"\"\n    processing_notes = []\n\n    try:\n        # Validate we have at least one primary column\n        if not any(col.is_primary for col in columns):\n            raise ProcessingError(\"No primary notes column found\")\n\n        # Sort columns to ensure primary first\n        sorted_columns = sorted(columns, key=lambda x: (not x.is_primary))\n\n        # initialise result dataframe\n        result_df = pd.DataFrame(index=pd.DatetimeIndex([]))\n\n        # Process each column\n        for idx, col_data in enumerate(sorted_columns):\n            # Generate column name\n            col_name = self._generate_column_name(\n                DataType.NOTES, col_data.is_primary, idx\n            )\n\n            # Get the notes series\n            notes_series = col_data.dataframe[\"value\"]\n\n            # Replace None with pd.NA for better handling\n            notes_series = notes_series.replace([None], pd.NA)\n\n            # Convert non-NA values to string and strip whitespace\n            notes_series = notes_series.apply(\n                lambda x: x.strip() if pd.notna(x) else pd.NA\n            )\n\n            # Remove empty strings\n            notes_series = notes_series.replace({\"\": pd.NA})\n\n            # Add to result DataFrame only if we have any valid notes\n            if not notes_series.isna().all():\n                result_df[col_name] = notes_series\n\n                # Track stats\n                valid_notes = notes_series.notna()\n                processing_notes.append(\n                    f\"Column {col_name}: found {valid_notes.sum()} valid notes\"\n                )\n\n        # Drop rows where all values are NA\n        if not result_df.empty:\n            result_df = result_df.dropna(how=\"all\")\n\n        processing_notes.append(\n            f\"Final dataset contains {len(result_df)} notes entries\"\n        )\n\n        return ProcessedTypeData(\n            dataframe=result_df,\n            source_units={},  # No units for text data\n            processing_notes=processing_notes,\n        )\n\n    except Exception as e:\n        raise ProcessingError(f\"Error processing notes data: {str(e)}\") from e\n</code></pre>"},{"location":"api/readers/","title":"Readers API","text":""},{"location":"api/readers/#src.readers","title":"<code>src.readers</code>","text":"<p>Reader initialization and registration.</p>"},{"location":"api/readers/#src.readers.__all__","title":"<code>__all__ = ['BaseReader', 'SQLiteReader', 'CSVReader', 'XMLReader']</code>  <code>module-attribute</code>","text":""},{"location":"api/readers/#src.readers.BaseReader","title":"<code>BaseReader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all file format readers.</p> <p>This class provides core functionality for reading diabetes device data files and automatic reader selection based on file types. It handles timestamp processing, data validation, and resource management.</p> Source code in <code>src/readers/base.py</code> <pre><code>class BaseReader(ABC):\n    \"\"\"Abstract base class for all file format readers.\n\n    This class provides core functionality for reading diabetes device data files\n    and automatic reader selection based on file types. It handles timestamp processing,\n    data validation, and resource management.\n    \"\"\"\n\n    _readers: Dict[FileType, Type[\"BaseReader\"]] = {}\n\n    @classmethod\n    def register(cls, file_type: FileType) -&gt; Callable[[Type[T]], Type[T]]:\n        \"\"\"Register a reader class for a specific file type.\n\n        Args:\n            file_type: FileType enum value to associate with the reader\n\n        Returns:\n            Callable: Decorator function that registers the reader class\n        \"\"\"\n\n        def wrapper(reader_cls: Type[T]) -&gt; Type[T]:\n            cls._readers[file_type] = reader_cls\n            return reader_cls\n\n        return wrapper\n\n    @classmethod\n    def get_reader_for_format(cls, fmt: DeviceFormat, file_path: Path) -&gt; \"BaseReader\":\n        \"\"\"Get appropriate reader instance for the detected format.\n\n        Args:\n            fmt: Detected device format specification\n            file_path: Path to the data file\n\n        Returns:\n            Instance of appropriate reader class\n\n        Raises:\n            ReaderError: If no reader is registered for the file type\n        \"\"\"\n        for file_config in fmt.files:\n            if Path(file_path).match(file_config.name_pattern):\n                reader_cls = cls._readers.get(file_config.file_type)\n                if reader_cls is None:\n                    raise ReaderError(\n                        f\"No reader registered for file type: {file_config.file_type.value}\"\n                    )\n                return reader_cls(file_path, file_config)\n\n        raise ReaderError(f\"No matching file configuration found for {file_path}\")\n\n    def __init__(self, path: Path, file_config: FileConfig):\n        \"\"\"initialise reader with file path and configuration.\n\n        Args:\n            path: Path to the data file\n            file_config: Configuration for the file format\n\n        Raises:\n            ValueError: If file does not exist\n        \"\"\"\n        if not path.exists():\n            raise ValueError(f\"File not found: {path}\")\n\n        self.file_path = path\n        self.file_config = file_config\n\n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Cleanup resources if needed.\"\"\"\n        self._cleanup()\n\n    def _cleanup(self):\n        \"\"\"Override this method in derived classes if cleanup is needed.\"\"\"\n\n    @abstractmethod\n    def read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n        \"\"\"Read and process a single table according to its structure.\n\n        This method must be implemented by each specific reader.\n        \"\"\"\n\n    def read_all_tables(self) -&gt; Dict[str, TableData]:\n        \"\"\"Read and process all tables defined in the file configuration.\"\"\"\n        results = {}\n        for table_config in self.file_config.tables:\n            table_data = self.read_table(table_config)\n            if table_data is not None:\n                if table_data.missing_required_columns:\n                    logger.debug(\n                        \"Table %s missing required data in columns: %s\",\n                        table_data.name,\n                        table_data.missing_required_columns,\n                    )\n                results[table_data.name] = table_data\n            else:\n                logger.error(\"Failed to process table: %s\", table_config.name)\n\n        return results\n\n    def detect_timestamp_format(self, series: pd.Series) -&gt; Tuple[TimestampType, dict]:\n        \"\"\"Detect timestamp format using a small deterministic heuristic.\n\n        Strategy (simple &amp; fast):\n\n        - sample up to 50 non-null values\n        - detect numeric epochs first\n        - look for obvious ISO-like markers (T, Z, timezone offsets)\n        - for hyphen/slash date starts try explicit day-first formats if day&gt;12 found\n        - attempt a compact list of explicit formats with a high-acceptance threshold\n        - fallback to pandas inference (utc=True) if explicit formats fail\n\n        Returns (TimestampType, parse_kwargs) where parse_kwargs is suitable for\n        passing to pd.to_datetime (e.g., {'format': ..., 'utc': True} or {'dayfirst': True,'utc':True}).\n        \"\"\"\n        try:\n            sample = series.dropna().astype(str).str.strip().head(50)\n            if sample.empty:\n                logger.warning(\"No non-null timestamps found in sample\")\n                return TimestampType.UNKNOWN, {}\n\n            # numeric epochs\n            numeric = pd.to_numeric(sample, errors=\"coerce\")\n            if numeric.notna().any():\n                nums = numeric.dropna().astype(float)\n                if (nums &gt; 1e8).all() and (nums &lt; 1e12).all():\n                    return TimestampType.UNIX_SECONDS, {\"unit\": \"s\", \"utc\": True}\n                if (nums &gt; 1e11).all() and (nums &lt; 1e15).all():\n                    return TimestampType.UNIX_MILLISECONDS, {\"unit\": \"ms\", \"utc\": True}\n\n            # quick ISO-like heuristic\n            joined = \" \".join(sample.head(10).tolist()).upper()\n            if \"T\" in joined and (\"Z\" in joined or \"+\" in joined):\n                return TimestampType.ISO_8601, {\"utc\": True}\n\n            # Check for hyphen/slash date leading pattern (d/m/Y or m/d/Y)\n            dayfirst_candidate = False\n            sep_match = sample.str.match(r\"^\\s*\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\")\n            if sep_match.any():\n                # If any day &gt; 12 then dayfirst is almost certainly correct\n                for v in sample:\n                    m = re.match(r\"^(\\d{1,2})[-/](\\d{1,2})[-/](\\d{2,4})\", v)\n                    if m:\n                        day = int(m.group(1))\n                        if day &gt; 12:\n                            dayfirst_candidate = True\n                            break\n\n            # Small explicit formats list (keep compact)\n            explicit = [\n                \"%d-%m-%Y %H:%M\",\n                \"%d/%m/%Y %H:%M\",\n                \"%Y-%m-%d %H:%M:%S\",\n                \"%Y-%m-%d %H:%M\",\n                \"%d-%m-%Y %H:%M:%S\",\n                \"%m/%d/%Y %H:%M\",\n                \"%Y-%m-%dT%H:%M:%S%z\",\n                \"%Y-%m-%dT%H:%M:%SZ\",\n                \"%d-%m-%Y %I:%M %p\",\n            ]\n\n            sample_norm = sample.str.replace(r\"\\s+UTC$|\\s+GMT$\", \"\", regex=True)\n            sample_norm = sample_norm.str.replace(r\"\\s+\", \" \", regex=True)\n\n            # If dayfirst is obvious, try day-first explicit formats first\n            if dayfirst_candidate:\n                for fmt in [\"%d-%m-%Y %H:%M\", \"%d/%m/%Y %H:%M\", \"%d-%m-%Y %H:%M:%S\"]:\n                    parsed = pd.to_datetime(\n                        sample_norm, format=fmt, errors=\"coerce\", utc=True\n                    )\n                    if parsed.notna().mean() &gt;= 0.95:\n                        return TimestampType.ISO_8601, {\"format\": fmt, \"utc\": True}\n\n            # Try compact explicit list\n            for fmt in explicit:\n                parsed = pd.to_datetime(\n                    sample_norm, format=fmt, errors=\"coerce\", utc=True\n                )\n                if parsed.notna().mean() &gt;= 0.95:\n                    return TimestampType.ISO_8601, {\"format\": fmt, \"utc\": True}\n\n            # If we saw a hyphen/slash pattern but not decisive, prefer dayfirst inference\n            if sep_match.any() and not dayfirst_candidate:\n                parsed_dayfirst = pd.to_datetime(\n                    sample, dayfirst=True, utc=True, errors=\"coerce\"\n                )\n                if parsed_dayfirst.notna().mean() &gt;= 0.9:\n                    return TimestampType.ISO_8601, {\"dayfirst\": True, \"utc\": True}\n\n            # Last resort: pandas inference\n            with warnings.catch_warnings():\n                warnings.filterwarnings(\n                    \"ignore\",\n                    message=(\n                        \"Could not infer format, so each element will be parsed individually, falling back to `dateutil`\"\n                    ),\n                )\n                inferred = pd.to_datetime(sample, utc=True, errors=\"coerce\")\n            if inferred.notna().mean() &gt;= 0.9:\n                return TimestampType.ISO_8601, {\"utc\": True}\n\n            return TimestampType.UNKNOWN, {}\n\n        except TimestampProcessingError as e:\n            logger.error(\"Error during timestamp detection: %s\", e)\n            return TimestampType.UNKNOWN, {}\n\n    def _convert_timestamp_to_utc(\n        self, df: pd.DataFrame, timestamp_column: str\n    ) -&gt; Tuple[pd.DataFrame, TimestampType]:\n        \"\"\"Convert timestamp column to UTC datetime and set as index.\"\"\"\n        fmt, parse_kwargs = self.detect_timestamp_format(df[timestamp_column])\n\n        if fmt == TimestampType.UNKNOWN:\n            raise TimestampProcessingError(\n                f\"Could not detect timestamp format for column {timestamp_column}\"\n            )\n\n        try:\n            # Epoch handling\n            if fmt in (TimestampType.UNIX_SECONDS, TimestampType.UNIX_MILLISECONDS):\n                unit = parse_kwargs.get(\"unit\", \"s\")\n                df[timestamp_column] = pd.to_datetime(\n                    df[timestamp_column], unit=unit, utc=True\n                )\n            elif fmt == TimestampType.ISO_8601:\n                # If an explicit format was provided, try it first but with coercion\n                if \"format\" in parse_kwargs:\n                    parsed = pd.to_datetime(\n                        df[timestamp_column].astype(str),\n                        format=parse_kwargs[\"format\"],\n                        errors=\"coerce\",\n                        utc=True,\n                    )\n                    success = parsed.notna().mean()\n                    if success &gt;= 0.9:\n                        df[timestamp_column] = parsed\n                    else:\n                        # fallback to pandas with any provided flags (dayfirst/utc)\n                        kwargs = {\n                            k: v for k, v in parse_kwargs.items() if k != \"format\"\n                        }\n                        df[timestamp_column] = pd.to_datetime(\n                            df[timestamp_column].astype(str), errors=\"coerce\", **kwargs\n                        )\n                else:\n                    # use parse kwargs (e.g., dayfirst) or generic inference\n                    df[timestamp_column] = pd.to_datetime(\n                        df[timestamp_column].astype(str),\n                        errors=\"coerce\",\n                        **parse_kwargs,\n                    )\n\n            # Ensure timezone-awareness and set index\n            if df[timestamp_column].dt.tz is None:\n                df[timestamp_column] = df[timestamp_column].dt.tz_localize(\"UTC\")\n\n            return df.set_index(timestamp_column).sort_index(), fmt\n\n        except Exception as e:\n            logger.error(\"Error converting timestamps: %s\", e)\n            raise TimestampProcessingError(\n                f\"Invalid timestamp column: {timestamp_column}\"\n            ) from e\n\n    def _validate_required_data(\n        self, df: pd.DataFrame, columns: List[ColumnMapping]\n    ) -&gt; List[str]:\n        \"\"\"Check for missing data in required columns.\"\"\"\n        missing_required = []\n        for col in columns:\n            if (\n                col.requirement == ColumnRequirement.REQUIRED_WITH_DATA\n                and col.source_name in df.columns\n                and df[col.source_name].isna().all()\n            ):\n                missing_required.append(col.source_name)\n        return missing_required\n\n    @staticmethod\n    def _validate_identifier(identifier: str) -&gt; bool:\n        \"\"\"Validate that an identifier only contains safe characters.\"\"\"\n        return all(c.isalnum() or c in [\"_\", \".\"] for c in identifier)\n</code></pre>"},{"location":"api/readers/#src.readers.BaseReader.file_path","title":"<code>file_path = path</code>  <code>instance-attribute</code>","text":""},{"location":"api/readers/#src.readers.BaseReader.file_config","title":"<code>file_config = file_config</code>  <code>instance-attribute</code>","text":""},{"location":"api/readers/#src.readers.BaseReader.register","title":"<code>register(file_type: FileType) -&gt; Callable[[Type[T]], Type[T]]</code>  <code>classmethod</code>","text":"<p>Register a reader class for a specific file type.</p> <p>Parameters:</p> Name Type Description Default <code>file_type</code> <code>FileType</code> <p>FileType enum value to associate with the reader</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable[[Type[T]], Type[T]]</code> <p>Decorator function that registers the reader class</p> Source code in <code>src/readers/base.py</code> <pre><code>@classmethod\ndef register(cls, file_type: FileType) -&gt; Callable[[Type[T]], Type[T]]:\n    \"\"\"Register a reader class for a specific file type.\n\n    Args:\n        file_type: FileType enum value to associate with the reader\n\n    Returns:\n        Callable: Decorator function that registers the reader class\n    \"\"\"\n\n    def wrapper(reader_cls: Type[T]) -&gt; Type[T]:\n        cls._readers[file_type] = reader_cls\n        return reader_cls\n\n    return wrapper\n</code></pre>"},{"location":"api/readers/#src.readers.BaseReader.get_reader_for_format","title":"<code>get_reader_for_format(fmt: DeviceFormat, file_path: Path) -&gt; BaseReader</code>  <code>classmethod</code>","text":"<p>Get appropriate reader instance for the detected format.</p> <p>Parameters:</p> Name Type Description Default <code>fmt</code> <code>DeviceFormat</code> <p>Detected device format specification</p> required <code>file_path</code> <code>Path</code> <p>Path to the data file</p> required <p>Returns:</p> Type Description <code>BaseReader</code> <p>Instance of appropriate reader class</p> <p>Raises:</p> Type Description <code>ReaderError</code> <p>If no reader is registered for the file type</p> Source code in <code>src/readers/base.py</code> <pre><code>@classmethod\ndef get_reader_for_format(cls, fmt: DeviceFormat, file_path: Path) -&gt; \"BaseReader\":\n    \"\"\"Get appropriate reader instance for the detected format.\n\n    Args:\n        fmt: Detected device format specification\n        file_path: Path to the data file\n\n    Returns:\n        Instance of appropriate reader class\n\n    Raises:\n        ReaderError: If no reader is registered for the file type\n    \"\"\"\n    for file_config in fmt.files:\n        if Path(file_path).match(file_config.name_pattern):\n            reader_cls = cls._readers.get(file_config.file_type)\n            if reader_cls is None:\n                raise ReaderError(\n                    f\"No reader registered for file type: {file_config.file_type.value}\"\n                )\n            return reader_cls(file_path, file_config)\n\n    raise ReaderError(f\"No matching file configuration found for {file_path}\")\n</code></pre>"},{"location":"api/readers/#src.readers.BaseReader.__init__","title":"<code>__init__(path: Path, file_config: FileConfig)</code>","text":"<p>initialise reader with file path and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the data file</p> required <code>file_config</code> <code>FileConfig</code> <p>Configuration for the file format</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If file does not exist</p> Source code in <code>src/readers/base.py</code> <pre><code>def __init__(self, path: Path, file_config: FileConfig):\n    \"\"\"initialise reader with file path and configuration.\n\n    Args:\n        path: Path to the data file\n        file_config: Configuration for the file format\n\n    Raises:\n        ValueError: If file does not exist\n    \"\"\"\n    if not path.exists():\n        raise ValueError(f\"File not found: {path}\")\n\n    self.file_path = path\n    self.file_config = file_config\n</code></pre>"},{"location":"api/readers/#src.readers.BaseReader.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry.</p> Source code in <code>src/readers/base.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    return self\n</code></pre>"},{"location":"api/readers/#src.readers.BaseReader.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Cleanup resources if needed.</p> Source code in <code>src/readers/base.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Cleanup resources if needed.\"\"\"\n    self._cleanup()\n</code></pre>"},{"location":"api/readers/#src.readers.BaseReader.read_table","title":"<code>read_table(table_structure: TableStructure) -&gt; Optional[TableData]</code>  <code>abstractmethod</code>","text":"<p>Read and process a single table according to its structure.</p> <p>This method must be implemented by each specific reader.</p> Source code in <code>src/readers/base.py</code> <pre><code>@abstractmethod\ndef read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n    \"\"\"Read and process a single table according to its structure.\n\n    This method must be implemented by each specific reader.\n    \"\"\"\n</code></pre>"},{"location":"api/readers/#src.readers.BaseReader.read_all_tables","title":"<code>read_all_tables() -&gt; Dict[str, TableData]</code>","text":"<p>Read and process all tables defined in the file configuration.</p> Source code in <code>src/readers/base.py</code> <pre><code>def read_all_tables(self) -&gt; Dict[str, TableData]:\n    \"\"\"Read and process all tables defined in the file configuration.\"\"\"\n    results = {}\n    for table_config in self.file_config.tables:\n        table_data = self.read_table(table_config)\n        if table_data is not None:\n            if table_data.missing_required_columns:\n                logger.debug(\n                    \"Table %s missing required data in columns: %s\",\n                    table_data.name,\n                    table_data.missing_required_columns,\n                )\n            results[table_data.name] = table_data\n        else:\n            logger.error(\"Failed to process table: %s\", table_config.name)\n\n    return results\n</code></pre>"},{"location":"api/readers/#src.readers.BaseReader.detect_timestamp_format","title":"<code>detect_timestamp_format(series: pd.Series) -&gt; Tuple[TimestampType, dict]</code>","text":"<p>Detect timestamp format using a small deterministic heuristic.</p> <p>Strategy (simple &amp; fast):</p> <ul> <li>sample up to 50 non-null values</li> <li>detect numeric epochs first</li> <li>look for obvious ISO-like markers (T, Z, timezone offsets)</li> <li>for hyphen/slash date starts try explicit day-first formats if day&gt;12 found</li> <li>attempt a compact list of explicit formats with a high-acceptance threshold</li> <li>fallback to pandas inference (utc=True) if explicit formats fail</li> </ul> <p>Returns (TimestampType, parse_kwargs) where parse_kwargs is suitable for passing to pd.to_datetime (e.g., {'format': ..., 'utc': True} or {'dayfirst': True,'utc':True}).</p> Source code in <code>src/readers/base.py</code> <pre><code>def detect_timestamp_format(self, series: pd.Series) -&gt; Tuple[TimestampType, dict]:\n    \"\"\"Detect timestamp format using a small deterministic heuristic.\n\n    Strategy (simple &amp; fast):\n\n    - sample up to 50 non-null values\n    - detect numeric epochs first\n    - look for obvious ISO-like markers (T, Z, timezone offsets)\n    - for hyphen/slash date starts try explicit day-first formats if day&gt;12 found\n    - attempt a compact list of explicit formats with a high-acceptance threshold\n    - fallback to pandas inference (utc=True) if explicit formats fail\n\n    Returns (TimestampType, parse_kwargs) where parse_kwargs is suitable for\n    passing to pd.to_datetime (e.g., {'format': ..., 'utc': True} or {'dayfirst': True,'utc':True}).\n    \"\"\"\n    try:\n        sample = series.dropna().astype(str).str.strip().head(50)\n        if sample.empty:\n            logger.warning(\"No non-null timestamps found in sample\")\n            return TimestampType.UNKNOWN, {}\n\n        # numeric epochs\n        numeric = pd.to_numeric(sample, errors=\"coerce\")\n        if numeric.notna().any():\n            nums = numeric.dropna().astype(float)\n            if (nums &gt; 1e8).all() and (nums &lt; 1e12).all():\n                return TimestampType.UNIX_SECONDS, {\"unit\": \"s\", \"utc\": True}\n            if (nums &gt; 1e11).all() and (nums &lt; 1e15).all():\n                return TimestampType.UNIX_MILLISECONDS, {\"unit\": \"ms\", \"utc\": True}\n\n        # quick ISO-like heuristic\n        joined = \" \".join(sample.head(10).tolist()).upper()\n        if \"T\" in joined and (\"Z\" in joined or \"+\" in joined):\n            return TimestampType.ISO_8601, {\"utc\": True}\n\n        # Check for hyphen/slash date leading pattern (d/m/Y or m/d/Y)\n        dayfirst_candidate = False\n        sep_match = sample.str.match(r\"^\\s*\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}\")\n        if sep_match.any():\n            # If any day &gt; 12 then dayfirst is almost certainly correct\n            for v in sample:\n                m = re.match(r\"^(\\d{1,2})[-/](\\d{1,2})[-/](\\d{2,4})\", v)\n                if m:\n                    day = int(m.group(1))\n                    if day &gt; 12:\n                        dayfirst_candidate = True\n                        break\n\n        # Small explicit formats list (keep compact)\n        explicit = [\n            \"%d-%m-%Y %H:%M\",\n            \"%d/%m/%Y %H:%M\",\n            \"%Y-%m-%d %H:%M:%S\",\n            \"%Y-%m-%d %H:%M\",\n            \"%d-%m-%Y %H:%M:%S\",\n            \"%m/%d/%Y %H:%M\",\n            \"%Y-%m-%dT%H:%M:%S%z\",\n            \"%Y-%m-%dT%H:%M:%SZ\",\n            \"%d-%m-%Y %I:%M %p\",\n        ]\n\n        sample_norm = sample.str.replace(r\"\\s+UTC$|\\s+GMT$\", \"\", regex=True)\n        sample_norm = sample_norm.str.replace(r\"\\s+\", \" \", regex=True)\n\n        # If dayfirst is obvious, try day-first explicit formats first\n        if dayfirst_candidate:\n            for fmt in [\"%d-%m-%Y %H:%M\", \"%d/%m/%Y %H:%M\", \"%d-%m-%Y %H:%M:%S\"]:\n                parsed = pd.to_datetime(\n                    sample_norm, format=fmt, errors=\"coerce\", utc=True\n                )\n                if parsed.notna().mean() &gt;= 0.95:\n                    return TimestampType.ISO_8601, {\"format\": fmt, \"utc\": True}\n\n        # Try compact explicit list\n        for fmt in explicit:\n            parsed = pd.to_datetime(\n                sample_norm, format=fmt, errors=\"coerce\", utc=True\n            )\n            if parsed.notna().mean() &gt;= 0.95:\n                return TimestampType.ISO_8601, {\"format\": fmt, \"utc\": True}\n\n        # If we saw a hyphen/slash pattern but not decisive, prefer dayfirst inference\n        if sep_match.any() and not dayfirst_candidate:\n            parsed_dayfirst = pd.to_datetime(\n                sample, dayfirst=True, utc=True, errors=\"coerce\"\n            )\n            if parsed_dayfirst.notna().mean() &gt;= 0.9:\n                return TimestampType.ISO_8601, {\"dayfirst\": True, \"utc\": True}\n\n        # Last resort: pandas inference\n        with warnings.catch_warnings():\n            warnings.filterwarnings(\n                \"ignore\",\n                message=(\n                    \"Could not infer format, so each element will be parsed individually, falling back to `dateutil`\"\n                ),\n            )\n            inferred = pd.to_datetime(sample, utc=True, errors=\"coerce\")\n        if inferred.notna().mean() &gt;= 0.9:\n            return TimestampType.ISO_8601, {\"utc\": True}\n\n        return TimestampType.UNKNOWN, {}\n\n    except TimestampProcessingError as e:\n        logger.error(\"Error during timestamp detection: %s\", e)\n        return TimestampType.UNKNOWN, {}\n</code></pre>"},{"location":"api/readers/#src.readers.CSVReader","title":"<code>CSVReader</code>","text":"<p>               Bases: <code>BaseReader</code></p> <p>Reads and processes CSV files according to the provided format configuration.</p> Source code in <code>src/readers/csv.py</code> <pre><code>@BaseReader.register(FileType.CSV)\nclass CSVReader(BaseReader):\n    \"\"\"Reads and processes CSV files according to the provided format configuration.\"\"\"\n\n    def __init__(self, path: Path, file_config: FileConfig):\n        super().__init__(path, file_config)\n        self._data = None\n\n    def _cleanup(self):\n        \"\"\"Cleanup any held resources.\"\"\"\n        self._data = None\n\n    def read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n        \"\"\"Read and process a single table according to its structure.\n\n        For CSV files, we treat each file as a single table, reading all data at once\n        and caching it for subsequent operations if needed.\n        \"\"\"\n        try:\n            # Read data if not already cached\n            if self._data is None:\n                try:\n                    # If the file config/table declares a header_row, use it to skip preamble\n                    header_row = None\n                    if self.file_config and self.file_config.tables:\n                        header_row = getattr(\n                            self.file_config.tables[0], \"header_row\", None\n                        )\n\n                    if header_row is None:\n                        self._data = pd.read_csv(\n                            self.file_path,\n                            encoding=\"utf-8\",\n                            low_memory=False,  # Prevent mixed type inference warnings\n                        )\n                    else:\n                        # skip rows up to header_row so that header_row becomes the header (first read line)\n                        self._data = pd.read_csv(\n                            self.file_path,\n                            encoding=\"utf-8\",\n                            low_memory=False,\n                            header=0,\n                            skiprows=range(header_row),\n                        )\n\n                    # Normalize column names: strip whitespace from headers\n                    self._data.columns = [\n                        c.strip() if isinstance(c, str) else c\n                        for c in self._data.columns\n                    ]\n                except Exception as e:\n                    raise FileAccessError(f\"Failed to read CSV file: {e}\") from e\n\n            if self._data.empty:\n                raise DataExistsError(f\"No data found in CSV file {self.file_path}\")\n\n            # Get required columns\n            columns_to_read = [\n                col.source_name\n                for col in table_structure.columns\n                if col.requirement != ColumnRequirement.CONFIRMATION_ONLY\n            ]\n            columns_to_read.append(table_structure.timestamp_column)\n\n            # Check for missing columns\n            missing_columns = [\n                col for col in columns_to_read if col not in self._data.columns\n            ]\n            if missing_columns:\n                logger.error(\n                    \"Required columns missing from CSV: %s\", \", \".join(missing_columns)\n                )\n                return None\n\n            # Select only needed columns and make a copy\n            df = self._data[columns_to_read].copy()\n\n            # Process timestamps\n            df, fmt = self._convert_timestamp_to_utc(\n                df, table_structure.timestamp_column\n            )\n\n            # Validate required data\n            missing_required = self._validate_required_data(df, table_structure.columns)\n\n            return TableData(\n                name=table_structure.name,\n                dataframe=df,\n                missing_required_columns=missing_required,\n                timestamp_type=fmt,\n            )\n\n        except DataValidationError as e:\n            logger.error(\"Validation error: %s\", e)\n            return None\n        except DataExistsError as e:\n            logger.error(\"No data error: %s\", e)\n            return None\n        except DataProcessingError as e:\n            logger.error(\"Processing error: %s\", e)\n            return None\n        except ProcessingError as e:\n            logger.error(\"Unexpected error processing CSV: %s\", e)\n            return None\n</code></pre>"},{"location":"api/readers/#src.readers.CSVReader.__init__","title":"<code>__init__(path: Path, file_config: FileConfig)</code>","text":"Source code in <code>src/readers/csv.py</code> <pre><code>def __init__(self, path: Path, file_config: FileConfig):\n    super().__init__(path, file_config)\n    self._data = None\n</code></pre>"},{"location":"api/readers/#src.readers.CSVReader.read_table","title":"<code>read_table(table_structure: TableStructure) -&gt; Optional[TableData]</code>","text":"<p>Read and process a single table according to its structure.</p> <p>For CSV files, we treat each file as a single table, reading all data at once and caching it for subsequent operations if needed.</p> Source code in <code>src/readers/csv.py</code> <pre><code>def read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n    \"\"\"Read and process a single table according to its structure.\n\n    For CSV files, we treat each file as a single table, reading all data at once\n    and caching it for subsequent operations if needed.\n    \"\"\"\n    try:\n        # Read data if not already cached\n        if self._data is None:\n            try:\n                # If the file config/table declares a header_row, use it to skip preamble\n                header_row = None\n                if self.file_config and self.file_config.tables:\n                    header_row = getattr(\n                        self.file_config.tables[0], \"header_row\", None\n                    )\n\n                if header_row is None:\n                    self._data = pd.read_csv(\n                        self.file_path,\n                        encoding=\"utf-8\",\n                        low_memory=False,  # Prevent mixed type inference warnings\n                    )\n                else:\n                    # skip rows up to header_row so that header_row becomes the header (first read line)\n                    self._data = pd.read_csv(\n                        self.file_path,\n                        encoding=\"utf-8\",\n                        low_memory=False,\n                        header=0,\n                        skiprows=range(header_row),\n                    )\n\n                # Normalize column names: strip whitespace from headers\n                self._data.columns = [\n                    c.strip() if isinstance(c, str) else c\n                    for c in self._data.columns\n                ]\n            except Exception as e:\n                raise FileAccessError(f\"Failed to read CSV file: {e}\") from e\n\n        if self._data.empty:\n            raise DataExistsError(f\"No data found in CSV file {self.file_path}\")\n\n        # Get required columns\n        columns_to_read = [\n            col.source_name\n            for col in table_structure.columns\n            if col.requirement != ColumnRequirement.CONFIRMATION_ONLY\n        ]\n        columns_to_read.append(table_structure.timestamp_column)\n\n        # Check for missing columns\n        missing_columns = [\n            col for col in columns_to_read if col not in self._data.columns\n        ]\n        if missing_columns:\n            logger.error(\n                \"Required columns missing from CSV: %s\", \", \".join(missing_columns)\n            )\n            return None\n\n        # Select only needed columns and make a copy\n        df = self._data[columns_to_read].copy()\n\n        # Process timestamps\n        df, fmt = self._convert_timestamp_to_utc(\n            df, table_structure.timestamp_column\n        )\n\n        # Validate required data\n        missing_required = self._validate_required_data(df, table_structure.columns)\n\n        return TableData(\n            name=table_structure.name,\n            dataframe=df,\n            missing_required_columns=missing_required,\n            timestamp_type=fmt,\n        )\n\n    except DataValidationError as e:\n        logger.error(\"Validation error: %s\", e)\n        return None\n    except DataExistsError as e:\n        logger.error(\"No data error: %s\", e)\n        return None\n    except DataProcessingError as e:\n        logger.error(\"Processing error: %s\", e)\n        return None\n    except ProcessingError as e:\n        logger.error(\"Unexpected error processing CSV: %s\", e)\n        return None\n</code></pre>"},{"location":"api/readers/#src.readers.SQLiteReader","title":"<code>SQLiteReader</code>","text":"<p>               Bases: <code>BaseReader</code></p> <p>Reads and processes SQLite files according to the provided format configuration.</p> Source code in <code>src/readers/sqlite.py</code> <pre><code>@BaseReader.register(FileType.SQLITE)\nclass SQLiteReader(BaseReader):\n    \"\"\"Reads and processes SQLite files according to the provided format configuration.\"\"\"\n\n    def __init__(self, path: Path, file_config: FileConfig):\n        super().__init__(path, file_config)\n        self._engine = None\n\n    @property\n    def engine(self):\n        \"\"\"Lazy initialisation of database engine.\"\"\"\n        if self._engine is None:\n            self._engine = create_engine(f\"sqlite:///{self.file_path}\")\n        return self._engine\n\n    def _cleanup(self):\n        \"\"\"Cleanup database connections.\"\"\"\n        if self._engine is not None:\n            self._engine.dispose()\n            self._engine = None\n\n    def read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n        \"\"\"Read and process a single table according to its structure.\"\"\"\n        try:\n            # Validate identifiers\n            if not self._validate_identifier(table_structure.name):\n                raise DataValidationError(f\"Invalid table name: {table_structure.name}\")\n\n            # Read only needed columns\n            columns_to_read = [\n                col.source_name\n                for col in table_structure.columns\n                if col.requirement != ColumnRequirement.CONFIRMATION_ONLY\n            ]\n            columns_to_read.append(table_structure.timestamp_column)\n\n            # Validate column names\n            for col in columns_to_read:\n                if not self._validate_identifier(col):\n                    raise DataValidationError(f\"Invalid column name: {col}\")\n\n            # Create query with quoted identifiers for SQLite\n            quoted_columns = [f'\"{col}\"' for col in columns_to_read]\n            query = text(\n                f\"\"\"\n                SELECT {', '.join(quoted_columns)}\n                FROM \"{table_structure.name}\"\n                ORDER BY \"{table_structure.timestamp_column}\"\n            \"\"\"\n            )\n\n            # Execute query within connection context\n            with self.engine.connect() as conn:\n                df = pd.read_sql_query(query, conn)\n\n            # Process timestamps\n            df, fmt = self._convert_timestamp_to_utc(\n                df, table_structure.timestamp_column\n            )\n\n            # Validate required data\n            missing_required = self._validate_required_data(df, table_structure.columns)\n\n            return TableData(\n                name=table_structure.name,\n                dataframe=df,\n                missing_required_columns=missing_required,\n                timestamp_type=fmt,\n            )\n\n        except DataValidationError as e:\n            # Handle specific ValueErrors such as invalid table or column names\n            logger.error(\"ValueError: %s\", e)\n            return None\n        except SQLAlchemyError as e:\n            # Handle any database-related errors (e.g., connection, query execution)\n            logger.error(\n                \"SQLAlchemyError processing table %s: %s\", table_structure.name, e\n            )\n            return None\n        except DataExistsError as e:\n            # Handle case where there is no data in the result set\n            logger.error(\n                \"EmptyDataError processing table %s: %s\", table_structure.name, e\n            )\n            return None\n        except ReaderError as e:\n            # Catch any unexpected errors\n            logger.error(\n                \"Unexpected error processing table %s: %s\", table_structure.name, e\n            )\n            return None\n</code></pre>"},{"location":"api/readers/#src.readers.SQLiteReader.engine","title":"<code>engine</code>  <code>property</code>","text":"<p>Lazy initialisation of database engine.</p>"},{"location":"api/readers/#src.readers.SQLiteReader.__init__","title":"<code>__init__(path: Path, file_config: FileConfig)</code>","text":"Source code in <code>src/readers/sqlite.py</code> <pre><code>def __init__(self, path: Path, file_config: FileConfig):\n    super().__init__(path, file_config)\n    self._engine = None\n</code></pre>"},{"location":"api/readers/#src.readers.SQLiteReader.read_table","title":"<code>read_table(table_structure: TableStructure) -&gt; Optional[TableData]</code>","text":"<p>Read and process a single table according to its structure.</p> Source code in <code>src/readers/sqlite.py</code> <pre><code>def read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n    \"\"\"Read and process a single table according to its structure.\"\"\"\n    try:\n        # Validate identifiers\n        if not self._validate_identifier(table_structure.name):\n            raise DataValidationError(f\"Invalid table name: {table_structure.name}\")\n\n        # Read only needed columns\n        columns_to_read = [\n            col.source_name\n            for col in table_structure.columns\n            if col.requirement != ColumnRequirement.CONFIRMATION_ONLY\n        ]\n        columns_to_read.append(table_structure.timestamp_column)\n\n        # Validate column names\n        for col in columns_to_read:\n            if not self._validate_identifier(col):\n                raise DataValidationError(f\"Invalid column name: {col}\")\n\n        # Create query with quoted identifiers for SQLite\n        quoted_columns = [f'\"{col}\"' for col in columns_to_read]\n        query = text(\n            f\"\"\"\n            SELECT {', '.join(quoted_columns)}\n            FROM \"{table_structure.name}\"\n            ORDER BY \"{table_structure.timestamp_column}\"\n        \"\"\"\n        )\n\n        # Execute query within connection context\n        with self.engine.connect() as conn:\n            df = pd.read_sql_query(query, conn)\n\n        # Process timestamps\n        df, fmt = self._convert_timestamp_to_utc(\n            df, table_structure.timestamp_column\n        )\n\n        # Validate required data\n        missing_required = self._validate_required_data(df, table_structure.columns)\n\n        return TableData(\n            name=table_structure.name,\n            dataframe=df,\n            missing_required_columns=missing_required,\n            timestamp_type=fmt,\n        )\n\n    except DataValidationError as e:\n        # Handle specific ValueErrors such as invalid table or column names\n        logger.error(\"ValueError: %s\", e)\n        return None\n    except SQLAlchemyError as e:\n        # Handle any database-related errors (e.g., connection, query execution)\n        logger.error(\n            \"SQLAlchemyError processing table %s: %s\", table_structure.name, e\n        )\n        return None\n    except DataExistsError as e:\n        # Handle case where there is no data in the result set\n        logger.error(\n            \"EmptyDataError processing table %s: %s\", table_structure.name, e\n        )\n        return None\n    except ReaderError as e:\n        # Catch any unexpected errors\n        logger.error(\n            \"Unexpected error processing table %s: %s\", table_structure.name, e\n        )\n        return None\n</code></pre>"},{"location":"api/readers/#src.readers.XMLReader","title":"<code>XMLReader</code>","text":"<p>               Bases: <code>BaseReader</code></p> <p>Reads and processes XML files according to the provided format configuration.</p> Source code in <code>src/readers/xml.py</code> <pre><code>@BaseReader.register(FileType.XML)\nclass XMLReader(BaseReader):\n    \"\"\"Reads and processes XML files according to the provided format configuration.\"\"\"\n\n    def __init__(self, path: Path, file_config: FileConfig):\n        super().__init__(path, file_config)\n        self._tree = None\n        self._root = None\n\n    def _cleanup(self):\n        \"\"\"Cleanup any held resources.\"\"\"\n        self._tree = None\n        self._root = None\n\n    def _init_xml(self):\n        \"\"\"initialise XML parsing if not already done.\"\"\"\n        if self._root is None:\n            try:\n                self._tree = ET.parse(self.file_path)\n                self._root = self._tree.getroot()\n            except ET.ParseError as e:\n                raise DataExistsError(f\"Failed to parse XML file: {e}\") from e\n            except Exception as e:\n                raise DataExistsError(f\"Error reading XML file: {e}\") from e\n\n    @staticmethod\n    def _extract_value(element: ET.Element, column: str) -&gt; str:\n        \"\"\"Extract value from XML element, checking both attributes and text.\n\n        Args:\n            element: XML element to extract from\n            column: Column name to look for\n\n        Returns:\n            Value from attribute or element text\n        \"\"\"\n        # Check attributes first\n        if column in element.attrib:\n            return element.attrib[column]\n\n        # Then check child elements\n        child = element.find(column)\n        if child is not None:\n            return child.text if child.text else \"\"\n\n        return \"\"\n\n    def read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n        \"\"\"Read and process a single table according to its structure.\n\n        For XML files, each table is expected to be contained within elements\n        matching the table name or a configured xpath.\n        \"\"\"\n        try:\n            self._init_xml()\n\n            # Get required columns\n            columns_to_read = [\n                col.source_name\n                for col in table_structure.columns\n                if col.requirement != ColumnRequirement.CONFIRMATION_ONLY\n            ]\n            columns_to_read.append(table_structure.timestamp_column)\n\n            # Find all elements for this table\n            table_elements = self._root.findall(f\".//{table_structure.name}\")\n            if not table_elements:\n                logger.error(\"No elements found for table: %s\", table_structure.name)\n                return None\n\n            # Extract data for each column\n            data: Dict[str, List[str]] = {col: [] for col in columns_to_read}\n\n            for element in table_elements:\n                for column in columns_to_read:\n                    value = self._extract_value(element, column)\n                    data[column].append(value)\n\n            # Convert to DataFrame\n            df = pd.DataFrame(data)\n\n            if df.empty:\n                raise DataExistsError(f\"No data found in table {table_structure.name}\")\n\n            # Process timestamps\n            df, fmt = self._convert_timestamp_to_utc(\n                df, table_structure.timestamp_column\n            )\n\n            # Validate required data\n            missing_required = self._validate_required_data(df, table_structure.columns)\n\n            return TableData(\n                name=table_structure.name,\n                dataframe=df,\n                missing_required_columns=missing_required,\n                timestamp_type=fmt,\n            )\n\n        except DataValidationError as e:\n            logger.error(\"Validation error: %s\", e)\n            return None\n        except DataExistsError as e:\n            logger.error(\"No data error: %s\", e)\n            return None\n        except DataProcessingError as e:\n            logger.error(\"Processing error: %s\", e)\n            return None\n        except ReaderError as e:\n            logger.error(\"Unexpected error processing XML: %s\", e)\n            return None\n</code></pre>"},{"location":"api/readers/#src.readers.XMLReader.__init__","title":"<code>__init__(path: Path, file_config: FileConfig)</code>","text":"Source code in <code>src/readers/xml.py</code> <pre><code>def __init__(self, path: Path, file_config: FileConfig):\n    super().__init__(path, file_config)\n    self._tree = None\n    self._root = None\n</code></pre>"},{"location":"api/readers/#src.readers.XMLReader.read_table","title":"<code>read_table(table_structure: TableStructure) -&gt; Optional[TableData]</code>","text":"<p>Read and process a single table according to its structure.</p> <p>For XML files, each table is expected to be contained within elements matching the table name or a configured xpath.</p> Source code in <code>src/readers/xml.py</code> <pre><code>def read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n    \"\"\"Read and process a single table according to its structure.\n\n    For XML files, each table is expected to be contained within elements\n    matching the table name or a configured xpath.\n    \"\"\"\n    try:\n        self._init_xml()\n\n        # Get required columns\n        columns_to_read = [\n            col.source_name\n            for col in table_structure.columns\n            if col.requirement != ColumnRequirement.CONFIRMATION_ONLY\n        ]\n        columns_to_read.append(table_structure.timestamp_column)\n\n        # Find all elements for this table\n        table_elements = self._root.findall(f\".//{table_structure.name}\")\n        if not table_elements:\n            logger.error(\"No elements found for table: %s\", table_structure.name)\n            return None\n\n        # Extract data for each column\n        data: Dict[str, List[str]] = {col: [] for col in columns_to_read}\n\n        for element in table_elements:\n            for column in columns_to_read:\n                value = self._extract_value(element, column)\n                data[column].append(value)\n\n        # Convert to DataFrame\n        df = pd.DataFrame(data)\n\n        if df.empty:\n            raise DataExistsError(f\"No data found in table {table_structure.name}\")\n\n        # Process timestamps\n        df, fmt = self._convert_timestamp_to_utc(\n            df, table_structure.timestamp_column\n        )\n\n        # Validate required data\n        missing_required = self._validate_required_data(df, table_structure.columns)\n\n        return TableData(\n            name=table_structure.name,\n            dataframe=df,\n            missing_required_columns=missing_required,\n            timestamp_type=fmt,\n        )\n\n    except DataValidationError as e:\n        logger.error(\"Validation error: %s\", e)\n        return None\n    except DataExistsError as e:\n        logger.error(\"No data error: %s\", e)\n        return None\n    except DataProcessingError as e:\n        logger.error(\"Processing error: %s\", e)\n        return None\n    except ReaderError as e:\n        logger.error(\"Unexpected error processing XML: %s\", e)\n        return None\n</code></pre>"},{"location":"api/core/aligner/","title":"Aligner API","text":""},{"location":"api/core/aligner/#src.core.aligner","title":"<code>src.core.aligner</code>","text":"<p>Aligner for diabetes data.</p> <p>This module provides functionality to align different types of diabetes data to a reference timeline, defaulting to CGM readings but supporting other reference types.</p>"},{"location":"api/core/aligner/#src.core.aligner.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/core/aligner/#src.core.aligner.AlignmentResult","title":"<code>AlignmentResult</code>  <code>dataclass</code>","text":"<p>Results of the alignment process.</p> Source code in <code>src/core/aligner.py</code> <pre><code>@dataclass\nclass AlignmentResult:\n    \"\"\"Results of the alignment process.\"\"\"\n\n    dataframe: pd.DataFrame  # The aligned timeseries data\n    start_time: pd.Timestamp  # Start of aligned timeline\n    end_time: pd.Timestamp  # End of aligned timeline\n    frequency: str  # Alignment frequency\n    processing_notes: List[str]  # Notes about the alignment process\n    source_units: Dict[str, Unit] = field(\n        default_factory=dict\n    )  # Maintain source units in aligned data\n</code></pre>"},{"location":"api/core/aligner/#src.core.aligner.AlignmentResult.dataframe","title":"<code>dataframe: pd.DataFrame</code>  <code>instance-attribute</code>","text":""},{"location":"api/core/aligner/#src.core.aligner.AlignmentResult.start_time","title":"<code>start_time: pd.Timestamp</code>  <code>instance-attribute</code>","text":""},{"location":"api/core/aligner/#src.core.aligner.AlignmentResult.end_time","title":"<code>end_time: pd.Timestamp</code>  <code>instance-attribute</code>","text":""},{"location":"api/core/aligner/#src.core.aligner.AlignmentResult.frequency","title":"<code>frequency: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/core/aligner/#src.core.aligner.AlignmentResult.processing_notes","title":"<code>processing_notes: List[str]</code>  <code>instance-attribute</code>","text":""},{"location":"api/core/aligner/#src.core.aligner.AlignmentResult.source_units","title":"<code>source_units: Dict[str, Unit] = field(default_factory=dict)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/aligner/#src.core.aligner.AlignmentResult.__init__","title":"<code>__init__(dataframe: pd.DataFrame, start_time: pd.Timestamp, end_time: pd.Timestamp, frequency: str, processing_notes: List[str], source_units: Dict[str, Unit] = dict()) -&gt; None</code>","text":""},{"location":"api/core/aligner/#src.core.aligner.Aligner","title":"<code>Aligner</code>","text":"<p>Aligns diabetes data to a reference timeline.</p> Source code in <code>src/core/aligner.py</code> <pre><code>class Aligner:\n    \"\"\"Aligns diabetes data to a reference timeline.\"\"\"\n\n    def _validate_timeline(self, reference_data: pd.DataFrame, freq: str) -&gt; None:\n        \"\"\"Validate that reference data provides a valid timeline.\"\"\"\n        if reference_data.empty:\n            raise AlignmentError(\"Reference data is empty\")\n\n        if not isinstance(reference_data.index, pd.DatetimeIndex):\n            raise AlignmentError(\"Reference data must have DatetimeIndex\")\n\n        if not reference_data.index.is_monotonic_increasing:\n            raise AlignmentError(\n                \"Reference data index must be monotonically increasing\"\n            )\n\n        # Check if the data mostly follows the expected frequency\n        time_diffs = reference_data.index.to_series().diff()\n        modal_diff = time_diffs.mode()[0]\n        expected_diff = pd.Timedelta(freq)\n\n        if modal_diff != expected_diff:\n            raise AlignmentError(\n                f\"Reference data frequency {modal_diff} does not match expected {freq}\"\n            )\n\n    def _collect_processing_notes(\n        self, processed_data: Dict[DataType, ProcessedTypeData]\n    ) -&gt; List[str]:\n        \"\"\"Collect processing notes from all data types.\"\"\"\n        all_notes = []\n        for data_type, data in processed_data.items():\n            all_notes.extend([f\"{data_type.name} Processing Notes:\"])\n            all_notes.extend([f\"  {note}\" for note in data.processing_notes])\n        return all_notes\n\n    def _align_bgm(\n        self, df: pd.DataFrame, reference_index: pd.DatetimeIndex, freq: str\n    ) -&gt; pd.DataFrame:\n        \"\"\"Align blood glucose meter data.\n\n        Args:\n            df: DataFrame containing BGM data\n            reference_index: Reference timeline to align to\n            freq: Frequency for alignment\n\n        Returns:\n            DataFrame aligned to reference timeline with averaged values\n        \"\"\"\n        df = df.copy()\n        df.index = df.index.round(freq)\n\n        # Identify value columns and their corresponding clipped flag columns\n        value_cols = [\n            col\n            for col in df.columns\n            if not col.endswith(\"_clipped\") and not col.endswith(\"_mmol\")\n        ]\n        clipped_cols = [f\"{col}_clipped\" for col in value_cols]\n        mmol_cols = [f\"{col}_mmol\" for col in value_cols]\n\n        # initialise result DataFrame\n        result = pd.DataFrame(index=reference_index)\n\n        # Process each set of related columns (value, clipped flag, mmol)\n        for value_col, clipped_col, mmol_col in zip(\n            value_cols, clipped_cols, mmol_cols\n        ):\n            # Calculate means for values within each interval\n            values = df[value_col].resample(freq).mean()\n\n            # For clipped flags, if any reading in the interval was clipped, mark as clipped\n            clipped = df[clipped_col].resample(freq).any()\n\n            # Calculate means for mmol values\n            mmol_values = df[mmol_col].resample(freq).mean()\n\n            # Add to result\n            result[value_col] = values\n            result[clipped_col] = clipped\n            result[mmol_col] = mmol_values\n\n        return result\n\n    def _align_insulin(\n        self, df: pd.DataFrame, reference_index: pd.DatetimeIndex, freq: str\n    ) -&gt; pd.DataFrame:\n        \"\"\"Align insulin data.\"\"\"\n        df = df.copy()\n        df.index = df.index.round(freq)\n\n        # Split and sum basal/bolus doses separately\n        basal_doses = df[\"dose\"].where(df[\"is_basal\"], 0)\n        bolus_doses = df[\"dose\"].where(df[\"is_bolus\"], 0)\n\n        # Resample each type\n        result = pd.DataFrame(\n            {\n                \"basal_dose\": basal_doses.resample(freq).sum(),\n                \"bolus_dose\": bolus_doses.resample(freq).sum(),\n            }\n        )\n\n        return result.reindex(reference_index).fillna(0)\n\n    def _align_carbs(\n        self, df: pd.DataFrame, reference_index: pd.DatetimeIndex, freq: str\n    ) -&gt; pd.DataFrame:\n        \"\"\"Align carbohydrate data.\"\"\"\n        df = df.copy()\n        df.index = df.index.round(freq)\n\n        result = df[\"carbs_primary\"].resample(freq).sum()\n        return (\n            pd.DataFrame({\"carbs_primary\": result}).reindex(reference_index).fillna(0)\n        )\n\n    def _align_notes(\n        self, df: pd.DataFrame, reference_index: pd.DatetimeIndex, freq: str\n    ) -&gt; pd.DataFrame:\n        \"\"\"Align notes data.\"\"\"\n        df = df.copy()\n        df.index = df.index.round(freq)\n\n        result = df[\"notes_primary\"].resample(freq).last()\n        return pd.DataFrame({\"notes_primary\": result}).reindex(reference_index)\n\n    def align(\n        self,\n        processed_data: Dict[DataType, ProcessedTypeData],\n        reference_df: pd.DataFrame = None,\n        freq: str = \"5min\",\n    ) -&gt; AlignmentResult:\n        \"\"\"Align all data to a reference timeline.\n\n        Args:\n            processed_data: Dictionary of processed data by DataType\n            reference_df: DataFrame to use as reference timeline. If None, uses CGM data.\n            freq: Expected frequency of data\n\n        Returns:\n            AlignmentResult containing aligned data and metadata\n        \"\"\"\n        # Get reference DataFrame (default to CGM if not specified)\n        if reference_df is None:\n            cgm_data = processed_data.get(DataType.CGM)\n            if not cgm_data or cgm_data.dataframe.empty:\n                raise AlignmentError(\"No CGM data available for alignment\")\n            reference_df = cgm_data.dataframe\n\n        # Validate timeline\n        self._validate_timeline(reference_df, freq)\n        reference_index = reference_df.index\n\n        # Track alignment process\n        processing_notes = []\n        aligned_dfs = []\n\n        # Change 'missing' column to clearer name for combined data\n        reference_df.rename(columns={\"missing\": \"missing_cgm\"}, inplace=True)\n\n        # Always include reference data first\n        aligned_dfs.append(reference_df)\n        processing_notes.append(\"Reference timeline established\")\n\n        # Define alignment methods for each data type\n        type_methods = {\n            DataType.BGM: self._align_bgm,\n            DataType.INSULIN: self._align_insulin,\n            DataType.CARBS: self._align_carbs,\n            DataType.NOTES: self._align_notes,\n        }\n\n        # Align other available data\n        for data_type, processed in processed_data.items():\n            if processed.dataframe is not reference_df:  # Skip reference data\n                try:\n                    align_method = type_methods.get(data_type)\n                    if align_method:\n                        aligned_df = align_method(\n                            processed.dataframe, reference_index, freq\n                        )\n                        aligned_dfs.append(aligned_df)\n                        processing_notes.append(\n                            f\"Successfully aligned {data_type.name} data\"\n                        )\n                except AlignmentError as e:\n                    logger.error(\"Error aligning %s: %s\", data_type.name, str(e))\n                    processing_notes.append(\n                        f\"Failed to align {data_type.name}: {str(e)}\"\n                    )\n\n        # Combine all aligned data\n        combined_df = pd.concat(aligned_dfs, axis=1)\n\n        return AlignmentResult(\n            dataframe=combined_df,\n            start_time=reference_index[0],\n            end_time=reference_index[-1],\n            frequency=freq,\n            processing_notes=[\n                *self._collect_processing_notes(processed_data),\n                *processing_notes,\n            ],\n            source_units={\n                col: unit\n                for data in processed_data.values()\n                for col, unit in data.source_units.items()\n            },\n        )\n</code></pre>"},{"location":"api/core/aligner/#src.core.aligner.Aligner.align","title":"<code>align(processed_data: Dict[DataType, ProcessedTypeData], reference_df: pd.DataFrame = None, freq: str = '5min') -&gt; AlignmentResult</code>","text":"<p>Align all data to a reference timeline.</p> <p>Parameters:</p> Name Type Description Default <code>processed_data</code> <code>Dict[DataType, ProcessedTypeData]</code> <p>Dictionary of processed data by DataType</p> required <code>reference_df</code> <code>DataFrame</code> <p>DataFrame to use as reference timeline. If None, uses CGM data.</p> <code>None</code> <code>freq</code> <code>str</code> <p>Expected frequency of data</p> <code>'5min'</code> <p>Returns:</p> Type Description <code>AlignmentResult</code> <p>AlignmentResult containing aligned data and metadata</p> Source code in <code>src/core/aligner.py</code> <pre><code>def align(\n    self,\n    processed_data: Dict[DataType, ProcessedTypeData],\n    reference_df: pd.DataFrame = None,\n    freq: str = \"5min\",\n) -&gt; AlignmentResult:\n    \"\"\"Align all data to a reference timeline.\n\n    Args:\n        processed_data: Dictionary of processed data by DataType\n        reference_df: DataFrame to use as reference timeline. If None, uses CGM data.\n        freq: Expected frequency of data\n\n    Returns:\n        AlignmentResult containing aligned data and metadata\n    \"\"\"\n    # Get reference DataFrame (default to CGM if not specified)\n    if reference_df is None:\n        cgm_data = processed_data.get(DataType.CGM)\n        if not cgm_data or cgm_data.dataframe.empty:\n            raise AlignmentError(\"No CGM data available for alignment\")\n        reference_df = cgm_data.dataframe\n\n    # Validate timeline\n    self._validate_timeline(reference_df, freq)\n    reference_index = reference_df.index\n\n    # Track alignment process\n    processing_notes = []\n    aligned_dfs = []\n\n    # Change 'missing' column to clearer name for combined data\n    reference_df.rename(columns={\"missing\": \"missing_cgm\"}, inplace=True)\n\n    # Always include reference data first\n    aligned_dfs.append(reference_df)\n    processing_notes.append(\"Reference timeline established\")\n\n    # Define alignment methods for each data type\n    type_methods = {\n        DataType.BGM: self._align_bgm,\n        DataType.INSULIN: self._align_insulin,\n        DataType.CARBS: self._align_carbs,\n        DataType.NOTES: self._align_notes,\n    }\n\n    # Align other available data\n    for data_type, processed in processed_data.items():\n        if processed.dataframe is not reference_df:  # Skip reference data\n            try:\n                align_method = type_methods.get(data_type)\n                if align_method:\n                    aligned_df = align_method(\n                        processed.dataframe, reference_index, freq\n                    )\n                    aligned_dfs.append(aligned_df)\n                    processing_notes.append(\n                        f\"Successfully aligned {data_type.name} data\"\n                    )\n            except AlignmentError as e:\n                logger.error(\"Error aligning %s: %s\", data_type.name, str(e))\n                processing_notes.append(\n                    f\"Failed to align {data_type.name}: {str(e)}\"\n                )\n\n    # Combine all aligned data\n    combined_df = pd.concat(aligned_dfs, axis=1)\n\n    return AlignmentResult(\n        dataframe=combined_df,\n        start_time=reference_index[0],\n        end_time=reference_index[-1],\n        frequency=freq,\n        processing_notes=[\n            *self._collect_processing_notes(processed_data),\n            *processing_notes,\n        ],\n        source_units={\n            col: unit\n            for data in processed_data.values()\n            for col, unit in data.source_units.items()\n        },\n    )\n</code></pre>"},{"location":"api/core/data-types/","title":"Data Types API","text":"<p>Module Location</p> <p><code>src/core/data_types.py</code></p>"},{"location":"api/core/data-types/#src.core.data_types","title":"<code>src.core.data_types</code>","text":"<p>Core data type definitions for diabetes data processing.</p> <p>This module defines the core data types and structures used for processing diabetes device data exports. It supports multiple file formats, different units of measurement, and various data types commonly found in diabetes management tools.</p> The structure allows for <ul> <li>Multiple files in a single format</li> <li>Multiple data types per table</li> <li>Different file types (SQLite, CSV, etc.)</li> <li>Flexible column mapping</li> <li>Primary/secondary data distinction</li> </ul>"},{"location":"api/core/data-types/#src.core.data_types.FileType","title":"<code>FileType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Supported file types for diabetes data.</p> Source code in <code>src/core/data_types.py</code> <pre><code>class FileType(Enum):\n    \"\"\"Supported file types for diabetes data.\"\"\"\n\n    SQLITE = \"sqlite\"\n    CSV = \"csv\"\n    JSON = \"json\"\n    XML = \"xml\"\n</code></pre>"},{"location":"api/core/data-types/#src.core.data_types.FileType.SQLITE","title":"<code>SQLITE = 'sqlite'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.FileType.CSV","title":"<code>CSV = 'csv'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.FileType.JSON","title":"<code>JSON = 'json'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.FileType.XML","title":"<code>XML = 'xml'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.DataType","title":"<code>DataType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Core diabetes data types.</p> Source code in <code>src/core/data_types.py</code> <pre><code>class DataType(Enum):\n    \"\"\"Core diabetes data types.\"\"\"\n\n    # CGM Data\n    CGM = auto()  # Continuous glucose monitoring data\n\n    # BGM Data\n    BGM = auto()  # Blood glucose meter readings\n\n    # Treatment Data\n    INSULIN = auto()  # Insulin doses\n    INSULIN_META = auto()  # Insulin metadata eg brand\n    CARBS = auto()  # Carbohydrate intake\n    NOTES = auto()  # Text notes/comments\n</code></pre>"},{"location":"api/core/data-types/#src.core.data_types.DataType.CGM","title":"<code>CGM = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.DataType.BGM","title":"<code>BGM = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.DataType.INSULIN","title":"<code>INSULIN = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.DataType.INSULIN_META","title":"<code>INSULIN_META = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.DataType.CARBS","title":"<code>CARBS = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.DataType.NOTES","title":"<code>NOTES = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.TimestampType","title":"<code>TimestampType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Common types of timestamp format, ensuring correct conversion</p> Source code in <code>src/core/data_types.py</code> <pre><code>class TimestampType(Enum):\n    \"\"\"Common types of timestamp format, ensuring correct conversion\"\"\"\n\n    UNIX_SECONDS = \"unix_seconds\"\n    UNIX_MILLISECONDS = \"unix_milliseconds\"\n    UNIX_MICROSECONDS = \"unix_microseconds\"\n    ISO_8601 = \"iso_8601\"\n    UNKNOWN = \"unknown\"\n</code></pre>"},{"location":"api/core/data-types/#src.core.data_types.TimestampType.UNIX_SECONDS","title":"<code>UNIX_SECONDS = 'unix_seconds'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.TimestampType.UNIX_MILLISECONDS","title":"<code>UNIX_MILLISECONDS = 'unix_milliseconds'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.TimestampType.UNIX_MICROSECONDS","title":"<code>UNIX_MICROSECONDS = 'unix_microseconds'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.TimestampType.ISO_8601","title":"<code>ISO_8601 = 'iso_8601'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.TimestampType.UNKNOWN","title":"<code>UNKNOWN = 'unknown'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.ColumnRequirement","title":"<code>ColumnRequirement</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Defines how column should be validated and if data reading is required</p> Source code in <code>src/core/data_types.py</code> <pre><code>class ColumnRequirement(Enum):\n    \"\"\"Defines how column should be validated and if data reading is required\"\"\"\n\n    CONFIRMATION_ONLY = auto()  # Just needs to exist - no data read\n    REQUIRED_WITH_DATA = auto()  # Must exist - data read &amp; fail if not\n    REQUIRED_NULLABLE = auto()  # Must exist, can have all missing values - data read\n    OPTIONAL = auto()  # May or may not exist - data read\n</code></pre>"},{"location":"api/core/data-types/#src.core.data_types.ColumnRequirement.CONFIRMATION_ONLY","title":"<code>CONFIRMATION_ONLY = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.ColumnRequirement.REQUIRED_WITH_DATA","title":"<code>REQUIRED_WITH_DATA = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.ColumnRequirement.REQUIRED_NULLABLE","title":"<code>REQUIRED_NULLABLE = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.ColumnRequirement.OPTIONAL","title":"<code>OPTIONAL = auto()</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.Unit","title":"<code>Unit</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Supported units of measurement.</p> Source code in <code>src/core/data_types.py</code> <pre><code>class Unit(Enum):\n    \"\"\"Supported units of measurement.\"\"\"\n\n    MGDL = \"mg/dL\"  # Blood glucose in mg/dL\n    MMOL = \"mmol/L\"  # Blood glucose in mmol/L\n    UNITS = \"U\"  # Insulin units\n    GRAMS = \"g\"  # Carbohydrates in grams\n</code></pre>"},{"location":"api/core/data-types/#src.core.data_types.Unit.MGDL","title":"<code>MGDL = 'mg/dL'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.Unit.MMOL","title":"<code>MMOL = 'mmol/L'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.Unit.UNITS","title":"<code>UNITS = 'U'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.Unit.GRAMS","title":"<code>GRAMS = 'g'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.ColumnMapping","title":"<code>ColumnMapping</code>  <code>dataclass</code>","text":"<p>Maps source columns to standardized data types.</p> <p>Parameters:</p> Name Type Description Default <code>source_name</code> <code>str</code> <p>Original column name in the data source</p> required <code>data_type</code> <code>Optional[DataType]</code> <p>Type of data this column contains (if applicable - Any column can be used for confirming device.)</p> <code>None</code> <code>unit</code> <code>Optional[Unit]</code> <p>Unit of measurement (if applicable)</p> <code>None</code> <code>requirement</code> <code>ColumnRequirement</code> <p>Type of requirement - default = REQUIRED_WITH_DATA</p> <code>REQUIRED_WITH_DATA</code> <code>is_primary</code> <code>bool</code> <p>Whether this is the primary column - default = True</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; glucose_column = ColumnMapping(\n...     source_name=\"calculated_value\",\n...     data_type=DataType.CGM,\n...     unit=Unit.MGDL,\n... )\n&gt;&gt;&gt; raw_glucose = ColumnMapping(\n...     source_name=\"raw_data\",\n...     data_type=DataType.CGM,\n...     requirement=ColumnRequirement.REQUIRED_NULLABLE,\n...     is_primary=False\n... )\n</code></pre> Source code in <code>src/core/data_types.py</code> <pre><code>@dataclass\nclass ColumnMapping:\n    \"\"\"Maps source columns to standardized data types.\n\n    Args:\n        source_name: Original column name in the data source\n        data_type: Type of data this column contains (if applicable - Any column can be used for confirming device.)\n        unit: Unit of measurement (if applicable)\n        requirement: Type of requirement - default = REQUIRED_WITH_DATA\n        is_primary: Whether this is the primary column - default = True\n\n    Examples:\n        &gt;&gt;&gt; glucose_column = ColumnMapping(\n        ...     source_name=\"calculated_value\",\n        ...     data_type=DataType.CGM,\n        ...     unit=Unit.MGDL,\n        ... )\n        &gt;&gt;&gt; raw_glucose = ColumnMapping(\n        ...     source_name=\"raw_data\",\n        ...     data_type=DataType.CGM,\n        ...     requirement=ColumnRequirement.REQUIRED_NULLABLE,\n        ...     is_primary=False\n        ... )\n    \"\"\"\n\n    source_name: str\n    data_type: Optional[DataType] = None\n    unit: Optional[Unit] = None\n    requirement: ColumnRequirement = ColumnRequirement.REQUIRED_WITH_DATA\n    is_primary: bool = True\n</code></pre>"},{"location":"api/core/data-types/#src.core.data_types.ColumnMapping.source_name","title":"<code>source_name: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.ColumnMapping.data_type","title":"<code>data_type: Optional[DataType] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.ColumnMapping.unit","title":"<code>unit: Optional[Unit] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.ColumnMapping.requirement","title":"<code>requirement: ColumnRequirement = ColumnRequirement.REQUIRED_WITH_DATA</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.ColumnMapping.is_primary","title":"<code>is_primary: bool = True</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.ColumnMapping.__init__","title":"<code>__init__(source_name: str, data_type: Optional[DataType] = None, unit: Optional[Unit] = None, requirement: ColumnRequirement = ColumnRequirement.REQUIRED_WITH_DATA, is_primary: bool = True) -&gt; None</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.TableStructure","title":"<code>TableStructure</code>  <code>dataclass</code>","text":"<p>Defines the structure of a data table.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Table name in the data source (empty string for CSV files)</p> required <code>timestamp_column</code> <code>str</code> <p>Name of the timestamp column</p> required <code>columns</code> <code>List[ColumnMapping]</code> <p>List of column mappings</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; bgreadings = TableStructure(\n...     name=\"bgreadings\",\n...     timestamp_column=\"timestamp\",\n...     columns=[\n...         ColumnMapping(\n...             source_name=\"calculated_value\",\n...             data_type=DataType.CGM,\n...             unit=Unit.MGDL\n...         ),\n...         ColumnMapping(\n...             source_name=\"raw_data\",\n...             data_type=DataType.CGM,\n...             requirement=ColumnRequirement.REQUIRED_NULLABLE,\n...             is_primary=False\n...         )\n...     ]\n... )\n</code></pre> Source code in <code>src/core/data_types.py</code> <pre><code>@dataclass\nclass TableStructure:\n    \"\"\"Defines the structure of a data table.\n\n    Args:\n        name: Table name in the data source (empty string for CSV files)\n        timestamp_column: Name of the timestamp column\n        columns: List of column mappings\n\n    Examples:\n        &gt;&gt;&gt; bgreadings = TableStructure(\n        ...     name=\"bgreadings\",\n        ...     timestamp_column=\"timestamp\",\n        ...     columns=[\n        ...         ColumnMapping(\n        ...             source_name=\"calculated_value\",\n        ...             data_type=DataType.CGM,\n        ...             unit=Unit.MGDL\n        ...         ),\n        ...         ColumnMapping(\n        ...             source_name=\"raw_data\",\n        ...             data_type=DataType.CGM,\n        ...             requirement=ColumnRequirement.REQUIRED_NULLABLE,\n        ...             is_primary=False\n        ...         )\n        ...     ]\n        ... )\n    \"\"\"\n\n    name: str\n    timestamp_column: str\n    columns: List[ColumnMapping]\n    header_row: Optional[int] = (\n        None  # 0-based index of the header row in CSV files (None = auto-detect)\n    )\n\n    def validate_columns(self):\n        \"\"\"Validate that table has at least one column defined.\n\n        Raises:\n            FormatValidationError: If table has no columns defined\n        \"\"\"\n        if not self.columns:\n            raise FormatValidationError(\n                f\"Table {self.name} must have at least one column defined\",\n                details={\"table_name\": self.name, \"columns_count\": 0},\n            )\n\n    def validate_unique_source_names(self):\n        \"\"\"Validate that all column names are unique.\n\n        Raises:\n            FormatValidationError: If duplicate column names are found\n        \"\"\"\n        column_names = [col.source_name for col in self.columns]\n        unique_names = set(column_names)\n        if len(column_names) != len(unique_names):\n            duplicates = [name for name in unique_names if column_names.count(name) &gt; 1]\n            raise FormatValidationError(\n                f\"Duplicate column names in table {self.name}\",\n                details={\"table_name\": self.name, \"duplicate_columns\": duplicates},\n            )\n\n    def validate_primary_columns(self):\n        \"\"\"Validate that each data type has at most one primary column.\n\n        Raises:\n            FormatValidationError: If multiple primary columns exist for any data type\n        \"\"\"\n        for data_type in DataType:\n            primary_columns = [\n                col.source_name\n                for col in self.columns\n                if col.data_type == data_type and col.is_primary\n            ]\n            if len(primary_columns) &gt; 1:\n                raise FormatValidationError(\n                    f\"Multiple primary columns for {data_type.value} in table {self.name}\",\n                    details={\n                        \"table_name\": self.name,\n                        \"data_type\": data_type.value,\n                        \"primary_columns\": primary_columns,\n                    },\n                )\n\n    def __post_init__(self):\n        self.validate_columns()\n        self.validate_unique_source_names()\n        self.validate_primary_columns()\n</code></pre>"},{"location":"api/core/data-types/#src.core.data_types.TableStructure.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.TableStructure.timestamp_column","title":"<code>timestamp_column: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.TableStructure.columns","title":"<code>columns: List[ColumnMapping]</code>  <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.TableStructure.header_row","title":"<code>header_row: Optional[int] = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.TableStructure.__init__","title":"<code>__init__(name: str, timestamp_column: str, columns: List[ColumnMapping], header_row: Optional[int] = None) -&gt; None</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.TableStructure.validate_columns","title":"<code>validate_columns()</code>","text":"<p>Validate that table has at least one column defined.</p> <p>Raises:</p> Type Description <code>FormatValidationError</code> <p>If table has no columns defined</p> Source code in <code>src/core/data_types.py</code> <pre><code>def validate_columns(self):\n    \"\"\"Validate that table has at least one column defined.\n\n    Raises:\n        FormatValidationError: If table has no columns defined\n    \"\"\"\n    if not self.columns:\n        raise FormatValidationError(\n            f\"Table {self.name} must have at least one column defined\",\n            details={\"table_name\": self.name, \"columns_count\": 0},\n        )\n</code></pre>"},{"location":"api/core/data-types/#src.core.data_types.TableStructure.validate_unique_source_names","title":"<code>validate_unique_source_names()</code>","text":"<p>Validate that all column names are unique.</p> <p>Raises:</p> Type Description <code>FormatValidationError</code> <p>If duplicate column names are found</p> Source code in <code>src/core/data_types.py</code> <pre><code>def validate_unique_source_names(self):\n    \"\"\"Validate that all column names are unique.\n\n    Raises:\n        FormatValidationError: If duplicate column names are found\n    \"\"\"\n    column_names = [col.source_name for col in self.columns]\n    unique_names = set(column_names)\n    if len(column_names) != len(unique_names):\n        duplicates = [name for name in unique_names if column_names.count(name) &gt; 1]\n        raise FormatValidationError(\n            f\"Duplicate column names in table {self.name}\",\n            details={\"table_name\": self.name, \"duplicate_columns\": duplicates},\n        )\n</code></pre>"},{"location":"api/core/data-types/#src.core.data_types.TableStructure.validate_primary_columns","title":"<code>validate_primary_columns()</code>","text":"<p>Validate that each data type has at most one primary column.</p> <p>Raises:</p> Type Description <code>FormatValidationError</code> <p>If multiple primary columns exist for any data type</p> Source code in <code>src/core/data_types.py</code> <pre><code>def validate_primary_columns(self):\n    \"\"\"Validate that each data type has at most one primary column.\n\n    Raises:\n        FormatValidationError: If multiple primary columns exist for any data type\n    \"\"\"\n    for data_type in DataType:\n        primary_columns = [\n            col.source_name\n            for col in self.columns\n            if col.data_type == data_type and col.is_primary\n        ]\n        if len(primary_columns) &gt; 1:\n            raise FormatValidationError(\n                f\"Multiple primary columns for {data_type.value} in table {self.name}\",\n                details={\n                    \"table_name\": self.name,\n                    \"data_type\": data_type.value,\n                    \"primary_columns\": primary_columns,\n                },\n            )\n</code></pre>"},{"location":"api/core/data-types/#src.core.data_types.TableStructure.__post_init__","title":"<code>__post_init__()</code>","text":"Source code in <code>src/core/data_types.py</code> <pre><code>def __post_init__(self):\n    self.validate_columns()\n    self.validate_unique_source_names()\n    self.validate_primary_columns()\n</code></pre>"},{"location":"api/core/data-types/#src.core.data_types.FileConfig","title":"<code>FileConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a specific file in a device format.</p> <p>Parameters:</p> Name Type Description Default <code>name_pattern</code> <code>str</code> <p>Pattern to match filename (e.g., \"*.sqlite\", \"glucose.csv\")</p> required <code>file_type</code> <code>FileType</code> <p>Type of the data file</p> required <code>tables</code> <code>List[TableStructure]</code> <p>List of table structures in the file</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; sqlite_file = FileConfig(\n...     name_pattern=\"*.sqlite\",\n...     file_type=FileType.SQLITE,\n...     tables=[bgreadings]  # TableStructure from previous example\n... )\n</code></pre> Source code in <code>src/core/data_types.py</code> <pre><code>@dataclass\nclass FileConfig:\n    \"\"\"Configuration for a specific file in a device format.\n\n    Args:\n        name_pattern: Pattern to match filename (e.g., \"*.sqlite\", \"glucose.csv\")\n        file_type: Type of the data file\n        tables: List of table structures in the file\n\n    Examples:\n        &gt;&gt;&gt; sqlite_file = FileConfig(\n        ...     name_pattern=\"*.sqlite\",\n        ...     file_type=FileType.SQLITE,\n        ...     tables=[bgreadings]  # TableStructure from previous example\n        ... )\n    \"\"\"\n\n    name_pattern: str\n    file_type: FileType\n    tables: List[TableStructure]\n\n    def __post_init__(self):\n        \"\"\"Validate file configuration after initialization.\n\n        Raises:\n            FormatValidationError: If file configuration is invalid\n        \"\"\"\n        if not self.tables:\n            raise FormatValidationError(\n                f\"File {self.name_pattern} must have at least one table defined\",\n                details={\"file_pattern\": self.name_pattern},\n            )\n\n        # For CSV files, ensure only one table with empty name\n        if self.file_type == FileType.CSV:\n            if len(self.tables) &gt; 1:\n                raise FormatValidationError(\n                    \"CSV files can only have one table structure\",\n                    details={\n                        \"file_pattern\": self.name_pattern,\n                        \"tables_count\": len(self.tables),\n                    },\n                )\n            if self.tables[0].name != \"\":\n                raise FormatValidationError(\n                    f\"CSV file table name should be empty string for file {self.name_pattern}\",\n                    details={\n                        \"file_pattern\": self.name_pattern,\n                        \"table_name\": self.tables[0].name,\n                    },\n                )\n</code></pre>"},{"location":"api/core/data-types/#src.core.data_types.FileConfig.name_pattern","title":"<code>name_pattern: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.FileConfig.file_type","title":"<code>file_type: FileType</code>  <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.FileConfig.tables","title":"<code>tables: List[TableStructure]</code>  <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.FileConfig.__init__","title":"<code>__init__(name_pattern: str, file_type: FileType, tables: List[TableStructure]) -&gt; None</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.FileConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate file configuration after initialization.</p> <p>Raises:</p> Type Description <code>FormatValidationError</code> <p>If file configuration is invalid</p> Source code in <code>src/core/data_types.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate file configuration after initialization.\n\n    Raises:\n        FormatValidationError: If file configuration is invalid\n    \"\"\"\n    if not self.tables:\n        raise FormatValidationError(\n            f\"File {self.name_pattern} must have at least one table defined\",\n            details={\"file_pattern\": self.name_pattern},\n        )\n\n    # For CSV files, ensure only one table with empty name\n    if self.file_type == FileType.CSV:\n        if len(self.tables) &gt; 1:\n            raise FormatValidationError(\n                \"CSV files can only have one table structure\",\n                details={\n                    \"file_pattern\": self.name_pattern,\n                    \"tables_count\": len(self.tables),\n                },\n            )\n        if self.tables[0].name != \"\":\n            raise FormatValidationError(\n                f\"CSV file table name should be empty string for file {self.name_pattern}\",\n                details={\n                    \"file_pattern\": self.name_pattern,\n                    \"table_name\": self.tables[0].name,\n                },\n            )\n</code></pre>"},{"location":"api/core/data-types/#src.core.data_types.DeviceFormat","title":"<code>DeviceFormat</code>  <code>dataclass</code>","text":"<p>Complete format specification for a diabetes device data export.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the device/format</p> required <code>files</code> <code>List[FileConfig]</code> <p>List of file configurations</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; xdrip_format = DeviceFormat(\n...     name=\"xdrip_sqlite\",\n...     files=[sqlite_file]  # FileConfig from previous example\n... )\n</code></pre> Source code in <code>src/core/data_types.py</code> <pre><code>@dataclass\nclass DeviceFormat:\n    \"\"\"Complete format specification for a diabetes device data export.\n\n    Args:\n        name: Name of the device/format\n        files: List of file configurations\n\n    Examples:\n        &gt;&gt;&gt; xdrip_format = DeviceFormat(\n        ...     name=\"xdrip_sqlite\",\n        ...     files=[sqlite_file]  # FileConfig from previous example\n        ... )\n    \"\"\"\n\n    name: str\n    files: List[FileConfig]\n\n    def __post_init__(self):\n        \"\"\"Validate device format after initialization.\n\n        Raises:\n            FormatValidationError: If device format is invalid\n        \"\"\"\n        if not self.files:\n            raise FormatValidationError(\n                f\"Device format {self.name} must have at least one file defined\",\n                details={\"format_name\": self.name},\n            )\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation including available data types.\"\"\"\n        types = set()\n        for file_config in self.files:\n            for table in file_config.tables:\n                for column in table.columns:\n                    if column.is_primary:\n                        types.add(column.data_type.name)\n        return f\"{self.name} - Available data: {', '.join(sorted(types))}\"\n</code></pre>"},{"location":"api/core/data-types/#src.core.data_types.DeviceFormat.name","title":"<code>name: str</code>  <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.DeviceFormat.files","title":"<code>files: List[FileConfig]</code>  <code>instance-attribute</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.DeviceFormat.__init__","title":"<code>__init__(name: str, files: List[FileConfig]) -&gt; None</code>","text":""},{"location":"api/core/data-types/#src.core.data_types.DeviceFormat.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate device format after initialization.</p> <p>Raises:</p> Type Description <code>FormatValidationError</code> <p>If device format is invalid</p> Source code in <code>src/core/data_types.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate device format after initialization.\n\n    Raises:\n        FormatValidationError: If device format is invalid\n    \"\"\"\n    if not self.files:\n        raise FormatValidationError(\n            f\"Device format {self.name} must have at least one file defined\",\n            details={\"format_name\": self.name},\n        )\n</code></pre>"},{"location":"api/core/data-types/#src.core.data_types.DeviceFormat.__str__","title":"<code>__str__() -&gt; str</code>","text":"<p>String representation including available data types.</p> Source code in <code>src/core/data_types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation including available data types.\"\"\"\n    types = set()\n    for file_config in self.files:\n        for table in file_config.tables:\n            for column in table.columns:\n                if column.is_primary:\n                    types.add(column.data_type.name)\n    return f\"{self.name} - Available data: {', '.join(sorted(types))}\"\n</code></pre>"},{"location":"api/core/data-types/#key-components","title":"Key Components","text":""},{"location":"api/core/data-types/#filetype","title":"FileType","text":"<pre><code>class FileType(Enum):\n    \"\"\"Supported file types for diabetes data.\"\"\"\n    SQLITE = \"sqlite\"\n    CSV = \"csv\"\n    JSON = \"json\"\n    XML = \"xml\"\n</code></pre> <p>Usage</p> <p>Used to specify and validate input file types during format detection.</p>"},{"location":"api/core/data-types/#datatype","title":"DataType","text":"<pre><code>class DataType(Enum):\n    \"\"\"Core diabetes data types.\"\"\"\n    CGM = auto()        # Continuous glucose monitoring data\n    BGM = auto()        # Blood glucose meter readings\n    INSULIN = auto()    # Insulin doses\n    INSULIN_META = auto() # Insulin metadata\n    CARBS = auto()      # Carbohydrate intake\n    NOTES = auto()      # Text notes/comments\n</code></pre> <p>Example</p> <pre><code>from src.core.data_types import DataType\n\n# Check if data is CGM reading\nif column.data_type == DataType.CGM:\n    process_cgm_data(column)\n</code></pre>"},{"location":"api/core/data-types/#timestamptype","title":"TimestampType","text":"<pre><code>class TimestampType(Enum):\n    \"\"\"Common types of timestamp format.\"\"\"\n    UNIX_SECONDS = \"unix_seconds\"\n    UNIX_MILLISECONDS = \"unix_milliseconds\"\n    UNIX_MICROSECONDS = \"unix_microseconds\"\n    ISO_8601 = \"iso_8601\"\n    UNKNOWN = \"unknown\"\n</code></pre> <p>Important</p> <p>All timestamps are converted to UTC during processing.</p>"},{"location":"api/core/data-types/#columnrequirement","title":"ColumnRequirement","text":"<pre><code>class ColumnRequirement(Enum):\n    \"\"\"Defines column validation requirements.\"\"\"\n    CONFIRMATION_ONLY = auto()    # Just needs to exist\n    REQUIRED_WITH_DATA = auto()   # Must exist with data\n    REQUIRED_NULLABLE = auto()    # Can have missing values\n    OPTIONAL = auto()             # May not exist\n</code></pre>"},{"location":"api/core/data-types/#unit","title":"Unit","text":"<pre><code>class Unit(Enum):\n    \"\"\"Supported units of measurement.\"\"\"\n    MGDL = \"mg/dL\"    # Blood glucose\n    MMOL = \"mmol/L\"   # Blood glucose\n    UNITS = \"U\"       # Insulin\n    GRAMS = \"g\"       # Carbohydrates\n</code></pre>"},{"location":"api/core/data-types/#columnmapping","title":"ColumnMapping","text":"<pre><code>@dataclass\nclass ColumnMapping:\n    \"\"\"Maps source columns to standardised data types.\n\n    Args:\n        source_name: Original column name\n        data_type: Column data type\n        unit: Unit of measurement\n        requirement: Validation requirement\n        is_primary: Primary column flag\n    \"\"\"\n    source_name: str\n    data_type: Optional[DataType] = None\n    unit: Optional[Unit] = None\n    requirement: ColumnRequirement = ColumnRequirement.REQUIRED_WITH_DATA\n    is_primary: bool = True\n</code></pre> <p>ColumnMapping Example</p> <pre><code>    glucose = ColumnMapping(\n        source_name=\"calculated_value\",\n        data_type=DataType.CGM,\n        unit=Unit.MGDL\n    )\n</code></pre>"},{"location":"api/core/data-types/#tablestructure","title":"TableStructure","text":"<pre><code>@dataclass\nclass TableStructure:\n    \"\"\"Defines data table structure.\n\n    Args:\n        name: Table name\n        timestamp_column: Timestamp column name\n        columns: Column mappings\n\n    Methods:\n        validate_columns(): Ensures table has columns\n        validate_unique_source_names(): Checks for duplicates\n        validate_primary_columns(): Validates primary columns\n    \"\"\"\n    name: str\n    timestamp_column: str\n    columns: List[ColumnMapping]\n</code></pre> <p>Validation Methods</p> <p>All validation methods raise <code>FormatValidationError</code> on failure</p>"},{"location":"api/core/data-types/#fileconfig","title":"FileConfig","text":"<pre><code>@dataclass\nclass FileConfig:\n    \"\"\"Configuration for device format file.\n\n    Args:\n        name_pattern: Filename pattern\n        file_type: File type enum\n        tables: Table structures\n\n    Validates:\n        - At least one table\n        - CSV files have one unnamed table\n    \"\"\"\n    name_pattern: str\n    file_type: FileType\n    tables: List[TableStructure]\n</code></pre>"},{"location":"api/core/data-types/#deviceformat","title":"DeviceFormat","text":"<pre><code>@dataclass\nclass DeviceFormat:\n    \"\"\"Complete device format specification.\n\n    Args:\n        name: Format name\n        files: File configurations\n\n    Methods:\n        __str__: Returns format name and data types\n    \"\"\"\n    name: str\n    files: List[FileConfig]\n</code></pre> <p>Complete Format Example</p> <pre><code>    xdrip_format = DeviceFormat(\n        name=\"xdrip_sqlite\",\n        files=[\n            FileConfig(\n                name_pattern=\"*.sqlite\",\n                file_type=FileType.SQLITE,\n                tables=[\n                    TableStructure(\n                        name=\"BgReadings\",\n                        timestamp_column=\"timestamp\",\n                        columns=[\n                            ColumnMapping(\n                                source_name=\"calculated_value\",\n                                data_type=DataType.CGM,\n                                unit=Unit.MGDL\n                            )\n                        ]\n                    )\n                ]\n            )\n        ]\n    )\n</code></pre>"},{"location":"api/core/exceptions/","title":"Exceptions API","text":""},{"location":"api/core/exceptions/#src.core.exceptions","title":"<code>src.core.exceptions</code>","text":"<p>This module provides all exceptions used within the system. All exceptions inherit from the base class CGMProcessorError or one of the subsequent children classes.</p>"},{"location":"api/core/exceptions/#src.core.exceptions.CGMProcessorError","title":"<code>CGMProcessorError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Base exception for all CGM processor errors.</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class CGMProcessorError(Exception):\n    \"\"\"Base exception for all CGM processor errors.\"\"\"\n\n    def __init__(self, message: str, details: Optional[Dict[str, Any]] = None):\n        super().__init__(message)\n        self.details = details or {}\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.CGMProcessorError.details","title":"<code>details = details or {}</code>  <code>instance-attribute</code>","text":""},{"location":"api/core/exceptions/#src.core.exceptions.CGMProcessorError.__init__","title":"<code>__init__(message: str, details: Optional[Dict[str, Any]] = None)</code>","text":"Source code in <code>src/core/exceptions.py</code> <pre><code>def __init__(self, message: str, details: Optional[Dict[str, Any]] = None):\n    super().__init__(message)\n    self.details = details or {}\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.FileError","title":"<code>FileError</code>","text":"<p>               Bases: <code>CGMProcessorError</code></p> <p>Base class for file-related errors.</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class FileError(CGMProcessorError):\n    \"\"\"Base class for file-related errors.\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.FileAccessError","title":"<code>FileAccessError</code>","text":"<p>               Bases: <code>FileError</code></p> <p>Raised when there's an error accessing a file.</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class FileAccessError(FileError):\n    \"\"\"Raised when there's an error accessing a file.\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.FileExtensionError","title":"<code>FileExtensionError</code>","text":"<p>               Bases: <code>FileError</code></p> <p>Raised When file extension is not supported.</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class FileExtensionError(FileError):\n    \"\"\"Raised When file extension is not supported.\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.FileParseError","title":"<code>FileParseError</code>","text":"<p>               Bases: <code>FileError</code></p> <p>Raised when there's an error parsing file contents.</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class FileParseError(FileError):\n    \"\"\"Raised when there's an error parsing file contents.\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.FormatError","title":"<code>FormatError</code>","text":"<p>               Bases: <code>CGMProcessorError</code></p> <p>Base class for format-related errors.</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class FormatError(CGMProcessorError):\n    \"\"\"Base class for format-related errors.\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.FormatDetectionError","title":"<code>FormatDetectionError</code>","text":"<p>               Bases: <code>FormatError</code></p> <p>Raised when there's an error detecting file format.</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class FormatDetectionError(FormatError):\n    \"\"\"Raised when there's an error detecting file format.\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.FormatLoadingError","title":"<code>FormatLoadingError</code>","text":"<p>               Bases: <code>FormatError</code></p> <p>Raised when a format file can`t be loaded.</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class FormatLoadingError(FormatError):\n    \"\"\"Raised when a format file can`t be loaded.\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.FormatValidationError","title":"<code>FormatValidationError</code>","text":"<p>               Bases: <code>FormatError</code></p> <p>Raised when there's an error validating format definition.</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class FormatValidationError(FormatError):\n    \"\"\"Raised when there's an error validating format definition.\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.DeviceFormatError","title":"<code>DeviceFormatError</code>","text":"<p>               Bases: <code>FormatError</code></p> <p>Raised for device-specific format issues.</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class DeviceFormatError(FormatError):\n    \"\"\"Raised for device-specific format issues.\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.ReaderError","title":"<code>ReaderError</code>","text":"<p>               Bases: <code>CGMProcessorError</code></p> <p>Base class for reader related errors.</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class ReaderError(CGMProcessorError):\n    \"\"\"Base class for reader related errors.\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.ProcessingError","title":"<code>ProcessingError</code>","text":"<p>               Bases: <code>CGMProcessorError</code></p> <p>Base class for data processing errors.</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class ProcessingError(CGMProcessorError):\n    \"\"\"Base class for data processing errors.\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.DataProcessingError","title":"<code>DataProcessingError</code>","text":"<p>               Bases: <code>ProcessingError</code></p> <p>Raised when there's an error processing data.</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class DataProcessingError(ProcessingError):\n    \"\"\"Raised when there's an error processing data.\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.TimestampProcessingError","title":"<code>TimestampProcessingError</code>","text":"<p>               Bases: <code>ProcessingError</code></p> <p>Raised when there is a timestamp format issues</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class TimestampProcessingError(ProcessingError):\n    \"\"\"Raised when there is a timestamp format issues\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.AlignmentError","title":"<code>AlignmentError</code>","text":"<p>               Bases: <code>ProcessingError</code></p> <p>Raised when there is an error aligning datasets</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class AlignmentError(ProcessingError):\n    \"\"\"Raised when there is an error aligning datasets\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.DataExistsError","title":"<code>DataExistsError</code>","text":"<p>               Bases: <code>FileError</code></p> <p>Raised when the reader returns no data</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class DataExistsError(FileError):\n    \"\"\"Raised when the reader returns no data\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.ValidationError","title":"<code>ValidationError</code>","text":"<p>               Bases: <code>CGMProcessorError</code></p> <p>Base class for validation errors.</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class ValidationError(CGMProcessorError):\n    \"\"\"Base class for validation errors.\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.DataValidationError","title":"<code>DataValidationError</code>","text":"<p>               Bases: <code>ValidationError</code></p> <p>Raised when there's an error validating data.</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class DataValidationError(ValidationError):\n    \"\"\"Raised when there's an error validating data.\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.ExportError","title":"<code>ExportError</code>","text":"<p>               Bases: <code>CGMProcessorError</code></p> <p>Base class for export errors</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class ExportError(CGMProcessorError):\n    \"\"\"Base class for export errors\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.DataQualityError","title":"<code>DataQualityError</code>","text":"<p>               Bases: <code>ProcessingError</code></p> <p>Raised when data quality checks fail (e.g., too many gaps, noise).</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class DataQualityError(ProcessingError):\n    \"\"\"Raised when data quality checks fail (e.g., too many gaps, noise).\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.TimeAlignmentError","title":"<code>TimeAlignmentError</code>","text":"<p>               Bases: <code>ProcessingError</code></p> <p>Raised when there are issues aligning different data streams (e.g., CGM with insulin).</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class TimeAlignmentError(ProcessingError):\n    \"\"\"Raised when there are issues aligning different data streams (e.g., CGM with insulin).\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.UnitConversionError","title":"<code>UnitConversionError</code>","text":"<p>               Bases: <code>ProcessingError</code></p> <p>Raised for unit conversion issues (e.g., mg/dL to mmol/L).</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class UnitConversionError(ProcessingError):\n    \"\"\"Raised for unit conversion issues (e.g., mg/dL to mmol/L).\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.MetricCalculationError","title":"<code>MetricCalculationError</code>","text":"<p>               Bases: <code>ProcessingError</code></p> <p>Raised when there are issues calculating diabetes metrics.</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class MetricCalculationError(ProcessingError):\n    \"\"\"Raised when there are issues calculating diabetes metrics.\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.CalibrationError","title":"<code>CalibrationError</code>","text":"<p>               Bases: <code>ProcessingError</code></p> <p>Raised for sensor calibration related issues.</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class CalibrationError(ProcessingError):\n    \"\"\"Raised for sensor calibration related issues.\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#src.core.exceptions.DataGapError","title":"<code>DataGapError</code>","text":"<p>               Bases: <code>DataQualityError</code></p> <p>Raised when data gaps exceed acceptable thresholds.</p> Source code in <code>src/core/exceptions.py</code> <pre><code>class DataGapError(DataQualityError):\n    \"\"\"Raised when data gaps exceed acceptable thresholds.\"\"\"\n</code></pre>"},{"location":"api/core/format-registry/","title":"Format Registry API","text":""},{"location":"api/core/format-registry/#src.core.format_registry","title":"<code>src.core.format_registry</code>","text":"<p>Format Registry for Diabetes Device Data Formats.</p> <p>This module provides functionality to discover and load device format definitions from the devices directory. It supports dynamic loading of formats and maintains a registry of available formats for use in format detection.</p> The registry <ul> <li>Recursively scans the devices directory</li> <li>Validates format definitions</li> <li>Provides access to available formats</li> <li>Handles format loading errors gracefully</li> </ul> Example <p>registry = FormatRegistry() formats = registry.formats xdrip_format = registry.get_format(\"xdrip_sqlite\")</p>"},{"location":"api/core/format-registry/#src.core.format_registry.logger","title":"<code>logger = logging.getLogger(__name__)</code>  <code>module-attribute</code>","text":""},{"location":"api/core/format-registry/#src.core.format_registry.parser","title":"<code>parser = argparse.ArgumentParser(description='Diabetes Data Format Detection Tool')</code>  <code>module-attribute</code>","text":""},{"location":"api/core/format-registry/#src.core.format_registry.args","title":"<code>args = parser.parse_args()</code>  <code>module-attribute</code>","text":""},{"location":"api/core/format-registry/#src.core.format_registry.registry","title":"<code>registry = FormatRegistry()</code>  <code>module-attribute</code>","text":""},{"location":"api/core/format-registry/#src.core.format_registry.file_path","title":"<code>file_path = Path(args.file_path)</code>  <code>module-attribute</code>","text":""},{"location":"api/core/format-registry/#src.core.format_registry.matching_formats","title":"<code>matching_formats = registry.get_formats_for_file(file_path)</code>  <code>module-attribute</code>","text":""},{"location":"api/core/format-registry/#src.core.format_registry.status","title":"<code>status = 'primary' if col.is_primary else 'secondary'</code>  <code>module-attribute</code>","text":""},{"location":"api/core/format-registry/#src.core.format_registry.FormatRegistry","title":"<code>FormatRegistry</code>","text":"<p>Registry for device formats that dynamically loads from devices directory.</p> The registry maintains a collection of device formats and provides methods to <ul> <li>Load formats from the devices directory</li> <li>Validate format definitions</li> <li>Access registered formats</li> <li>Filter formats by file type or data type</li> </ul> <p>Attributes:</p> Name Type Description <code>formats</code> <code>List[DeviceFormat]</code> <p>List of all registered formats</p> Example <p>registry = FormatRegistry() sqlite_formats = registry.get_formats_by_type(FileType.SQLITE) cgm_formats = registry.get_formats_with_data_type(DataType.CGM)</p> Source code in <code>src/core/format_registry.py</code> <pre><code>class FormatRegistry:\n    \"\"\"Registry for device formats that dynamically loads from devices directory.\n\n    The registry maintains a collection of device formats and provides methods to:\n        - Load formats from the devices directory\n        - Validate format definitions\n        - Access registered formats\n        - Filter formats by file type or data type\n\n    Attributes:\n        formats (List[DeviceFormat]): List of all registered formats\n\n    Example:\n        &gt;&gt;&gt; registry = FormatRegistry()\n        &gt;&gt;&gt; sqlite_formats = registry.get_formats_by_type(FileType.SQLITE)\n        &gt;&gt;&gt; cgm_formats = registry.get_formats_with_data_type(DataType.CGM)\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"initialise the format registry and load available formats.\"\"\"\n        self._formats: Dict[str, DeviceFormat] = {}\n        self._load_formats()\n        logger.debug(\"Initialised FormatRegistry with %d formats\", len(self._formats))\n\n    def _load_formats(self) -&gt; None:\n        \"\"\"Load all format definitions from the devices directory structure.\n\n        Raises:\n            FileAccessError: If manufacturers directory cannot be accessed\n            FormatLoadingError: If there's an error loading format files\n        \"\"\"\n        manufacturers_dir = Path(__file__).parent / \"devices\"\n        if not manufacturers_dir.exists():\n            raise FileAccessError(\n                \"Manufacturers directory not found\",\n                details={\"directory\": str(manufacturers_dir)},\n            )\n\n        try:\n            # Recursively find all Python files\n            for format_file in manufacturers_dir.rglob(\"*.py\"):\n                if format_file.stem == \"__init__\":\n                    continue\n\n                try:\n                    self._load_format_file(format_file)\n                except FormatLoadingError as e:\n                    logger.error(\n                        \"Error loading format file: %s Details: %s\", str(e), e.details\n                    )\n                    continue\n\n        except Exception as e:\n            # Only wrap non-FileAccessError exceptions in FormatLoadingError\n            if isinstance(e, FileAccessError):\n                raise\n            raise FormatLoadingError(\n                \"Failed to load formats\", details={\"error\": str(e)}\n            ) from e\n\n    def _load_format_file(self, path: Path) -&gt; None:\n        \"\"\"Load and process a single format definition file.\n\n        Args:\n            path: Path to the format definition file\n\n        Raises:\n            FormatLoadingError: If there's an error loading the file\n        \"\"\"\n        try:\n            module_name = f\"devices.{path.parent.name}.{path.stem}\"\n            spec = importlib.util.spec_from_file_location(module_name, path)\n\n            if spec is None or spec.loader is None:\n                raise FormatLoadingError(\n                    \"Failed to create module spec\", details={\"path\": str(path)}\n                )\n\n            module = importlib.util.module_from_spec(spec)\n            sys.modules[module_name] = module\n            spec.loader.exec_module(module)\n\n            # Find and validate DeviceFormat instances\n            for attr_name in dir(module):\n                attr = getattr(module, attr_name)\n                if isinstance(attr, DeviceFormat):\n                    self._validate_and_register_format(attr, path)\n\n        except Exception as e:\n            raise FormatLoadingError(\n                f\"Error loading {path}\", details={\"error\": str(e)}\n            ) from e\n\n    def _validate_and_register_format(\n        self, format_def: DeviceFormat, source_file: Path\n    ) -&gt; None:\n        \"\"\"Validate and register a format definition.\n\n        Args:\n            format_def: The format definition to validate and register\n            source_file: Path to the source file (for error reporting)\n\n        Raises:\n            FormatValidationError: If the format fails validation\n        \"\"\"\n        try:\n            # Format name validation\n            if not format_def.name:\n                raise FormatValidationError(\n                    \"Format name cannot be empty\",\n                    details={\"source_file\": str(source_file)},\n                )\n\n            # File configuration validation\n            if not format_def.files:\n                raise FormatValidationError(\n                    f\"No files defined in format {format_def.name}\",\n                    details={\n                        \"format_name\": format_def.name,\n                        \"source_file\": str(source_file),\n                    },\n                )\n\n            for config in format_def.files:\n                if not config.tables:\n                    raise FormatValidationError(\n                        f\"No tables defined in file {config.name_pattern}\",\n                        details={\n                            \"format_name\": format_def.name,\n                            \"file_pattern\": config.name_pattern,\n                        },\n                    )\n\n            # Register the format\n            format_key = f\"{format_def.name}_{format_def.files[0].file_type.value}\"\n            if format_key in self._formats:\n                logger.warning(\"Overwriting existing format: %s\", format_key)\n            self._formats[format_key] = format_def\n            logger.debug(\"Registered format: %s from %s\", format_key, source_file)\n\n        except FormatValidationError as e:\n            logger.error(\n                \"Validation failed for format in %s: %s Details: %s\",\n                source_file,\n                str(e),\n                e.details,\n            )\n            raise\n\n    @property\n    def formats(self) -&gt; List[DeviceFormat]:\n        \"\"\"Get list of all registered formats.\n\n        Returns:\n            List of all registered DeviceFormat instances\n        \"\"\"\n        return list(self._formats.values())\n\n    def get_format(self, name: str) -&gt; Optional[DeviceFormat]:\n        \"\"\"Get a specific format by name.\n\n        Args:\n            name: Name of the format to retrieve\n\n        Returns:\n            The requested DeviceFormat or None if not found\n        \"\"\"\n        return self._formats.get(name)\n\n    def get_formats_by_type(self, file_type: FileType) -&gt; List[DeviceFormat]:\n        \"\"\"Get all formats that handle a specific file type.\n\n        Args:\n            file_type: The file type to filter by\n\n        Returns:\n            List of formats that support the specified file type\n        \"\"\"\n        return [\n            fmt\n            for fmt in self._formats.values()\n            if any(f.file_type == file_type for f in fmt.files)\n        ]\n\n    def get_formats_for_file(self, path: Path) -&gt; List[DeviceFormat]:\n        \"\"\"Get all formats that could match this file based on extension.\n\n        Args:\n            path: Path to the file to check\n\n        Returns:\n            List of potential matching formats\n\n        Raises:\n            FileExtensionError: If file extension is not supported\n        \"\"\"\n        try:\n            file_type = FileType(path.suffix.lower()[1:])\n            return self.get_formats_by_type(file_type)\n        except ValueError as exc:\n            logger.warning(\"Unsupported file type: %s\", path.suffix)\n            raise FileExtensionError(\n                \"Unsupported file type\",\n                details={\"file\": str(path), \"extension\": path.suffix},\n            ) from exc\n\n    def get_formats_with_data_type(self, data_type: DataType) -&gt; List[DeviceFormat]:\n        \"\"\"Get all formats that contain a specific data type.\n\n        Args:\n            data_type: The data type to filter by\n\n        Returns:\n            List of formats that contain the specified data type\n        \"\"\"\n        formats = []\n        for device_format in self._formats.values():\n            for config in device_format.files:\n                for current_table in config.tables:\n                    if any(\n                        current_column.data_type == data_type\n                        for current_column in current_table.columns\n                    ):\n                        formats.append(device_format)\n                        break\n        return formats\n\n    def get_available_data_types(self) -&gt; Set[DataType]:\n        \"\"\"Get all data types available across all formats.\n\n        Returns:\n            Set of all available data types\n        \"\"\"\n        types = set()\n        for device_format in self._formats.values():\n            for config in device_format.files:\n                for current_table in config.tables:\n                    for current_column in current_table.columns:\n                        if current_column.data_type:\n                            types.add(current_column.data_type)\n        return types\n</code></pre>"},{"location":"api/core/format-registry/#src.core.format_registry.FormatRegistry.formats","title":"<code>formats: List[DeviceFormat]</code>  <code>property</code>","text":"<p>Get list of all registered formats.</p> <p>Returns:</p> Type Description <code>List[DeviceFormat]</code> <p>List of all registered DeviceFormat instances</p>"},{"location":"api/core/format-registry/#src.core.format_registry.FormatRegistry.__init__","title":"<code>__init__()</code>","text":"<p>initialise the format registry and load available formats.</p> Source code in <code>src/core/format_registry.py</code> <pre><code>def __init__(self):\n    \"\"\"initialise the format registry and load available formats.\"\"\"\n    self._formats: Dict[str, DeviceFormat] = {}\n    self._load_formats()\n    logger.debug(\"Initialised FormatRegistry with %d formats\", len(self._formats))\n</code></pre>"},{"location":"api/core/format-registry/#src.core.format_registry.FormatRegistry.get_format","title":"<code>get_format(name: str) -&gt; Optional[DeviceFormat]</code>","text":"<p>Get a specific format by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the format to retrieve</p> required <p>Returns:</p> Type Description <code>Optional[DeviceFormat]</code> <p>The requested DeviceFormat or None if not found</p> Source code in <code>src/core/format_registry.py</code> <pre><code>def get_format(self, name: str) -&gt; Optional[DeviceFormat]:\n    \"\"\"Get a specific format by name.\n\n    Args:\n        name: Name of the format to retrieve\n\n    Returns:\n        The requested DeviceFormat or None if not found\n    \"\"\"\n    return self._formats.get(name)\n</code></pre>"},{"location":"api/core/format-registry/#src.core.format_registry.FormatRegistry.get_formats_by_type","title":"<code>get_formats_by_type(file_type: FileType) -&gt; List[DeviceFormat]</code>","text":"<p>Get all formats that handle a specific file type.</p> <p>Parameters:</p> Name Type Description Default <code>file_type</code> <code>FileType</code> <p>The file type to filter by</p> required <p>Returns:</p> Type Description <code>List[DeviceFormat]</code> <p>List of formats that support the specified file type</p> Source code in <code>src/core/format_registry.py</code> <pre><code>def get_formats_by_type(self, file_type: FileType) -&gt; List[DeviceFormat]:\n    \"\"\"Get all formats that handle a specific file type.\n\n    Args:\n        file_type: The file type to filter by\n\n    Returns:\n        List of formats that support the specified file type\n    \"\"\"\n    return [\n        fmt\n        for fmt in self._formats.values()\n        if any(f.file_type == file_type for f in fmt.files)\n    ]\n</code></pre>"},{"location":"api/core/format-registry/#src.core.format_registry.FormatRegistry.get_formats_for_file","title":"<code>get_formats_for_file(path: Path) -&gt; List[DeviceFormat]</code>","text":"<p>Get all formats that could match this file based on extension.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the file to check</p> required <p>Returns:</p> Type Description <code>List[DeviceFormat]</code> <p>List of potential matching formats</p> <p>Raises:</p> Type Description <code>FileExtensionError</code> <p>If file extension is not supported</p> Source code in <code>src/core/format_registry.py</code> <pre><code>def get_formats_for_file(self, path: Path) -&gt; List[DeviceFormat]:\n    \"\"\"Get all formats that could match this file based on extension.\n\n    Args:\n        path: Path to the file to check\n\n    Returns:\n        List of potential matching formats\n\n    Raises:\n        FileExtensionError: If file extension is not supported\n    \"\"\"\n    try:\n        file_type = FileType(path.suffix.lower()[1:])\n        return self.get_formats_by_type(file_type)\n    except ValueError as exc:\n        logger.warning(\"Unsupported file type: %s\", path.suffix)\n        raise FileExtensionError(\n            \"Unsupported file type\",\n            details={\"file\": str(path), \"extension\": path.suffix},\n        ) from exc\n</code></pre>"},{"location":"api/core/format-registry/#src.core.format_registry.FormatRegistry.get_formats_with_data_type","title":"<code>get_formats_with_data_type(data_type: DataType) -&gt; List[DeviceFormat]</code>","text":"<p>Get all formats that contain a specific data type.</p> <p>Parameters:</p> Name Type Description Default <code>data_type</code> <code>DataType</code> <p>The data type to filter by</p> required <p>Returns:</p> Type Description <code>List[DeviceFormat]</code> <p>List of formats that contain the specified data type</p> Source code in <code>src/core/format_registry.py</code> <pre><code>def get_formats_with_data_type(self, data_type: DataType) -&gt; List[DeviceFormat]:\n    \"\"\"Get all formats that contain a specific data type.\n\n    Args:\n        data_type: The data type to filter by\n\n    Returns:\n        List of formats that contain the specified data type\n    \"\"\"\n    formats = []\n    for device_format in self._formats.values():\n        for config in device_format.files:\n            for current_table in config.tables:\n                if any(\n                    current_column.data_type == data_type\n                    for current_column in current_table.columns\n                ):\n                    formats.append(device_format)\n                    break\n    return formats\n</code></pre>"},{"location":"api/core/format-registry/#src.core.format_registry.FormatRegistry.get_available_data_types","title":"<code>get_available_data_types() -&gt; Set[DataType]</code>","text":"<p>Get all data types available across all formats.</p> <p>Returns:</p> Type Description <code>Set[DataType]</code> <p>Set of all available data types</p> Source code in <code>src/core/format_registry.py</code> <pre><code>def get_available_data_types(self) -&gt; Set[DataType]:\n    \"\"\"Get all data types available across all formats.\n\n    Returns:\n        Set of all available data types\n    \"\"\"\n    types = set()\n    for device_format in self._formats.values():\n        for config in device_format.files:\n            for current_table in config.tables:\n                for current_column in current_table.columns:\n                    if current_column.data_type:\n                        types.add(current_column.data_type)\n    return types\n</code></pre>"},{"location":"contributing/formats/","title":"Contributing New Formats","text":"<p>This guide will walk you through the process of adding support for new diabetes device data formats to the project. Our system is designed to be extensible, allowing for easy addition of new data formats while maintaining consistent data handling and validation.</p>"},{"location":"contributing/formats/#overview","title":"Overview","text":"<p>The format system consists of several key components:</p> <ol> <li>Device Formats: Define how to read and interpret data from specific diabetes devices</li> <li>File Configurations: Specify file types and patterns to match</li> <li>Table Structures: Define the layout of data tables</li> <li>Column Mappings: Map source columns to standardised data types</li> </ol>"},{"location":"contributing/formats/#quick-start","title":"Quick Start","text":"<p>Here's a basic example of adding support for a new CSV format:</p> <pre><code>from src.core.data_types import (\n    ColumnMapping,\n    DataType,\n    DeviceFormat,\n    FileConfig,\n    FileType,\n    TableStructure,\n    Unit,\n)\n\nMY_DEVICE_FORMAT = DeviceFormat(\n    name=\"my_device_csv\",\n    files=[\n        FileConfig(\n            name_pattern=\"*.csv\",\n            file_type=FileType.CSV,\n            tables=[\n                TableStructure(\n                    name=\"\",  # CSV files use empty string\n                    timestamp_column=\"Time\",\n                    columns=[\n                        ColumnMapping(\n                            source_name=\"Glucose\",\n                            data_type=DataType.CGM,\n                            unit=Unit.MGDL,\n                        ),\n                    ],\n                ),\n            ],\n        )\n    ],\n)\n</code></pre>"},{"location":"contributing/formats/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"contributing/formats/#1-create-a-new-format-file","title":"1. Create a New Format File","text":"<p>Create a new Python file in the appropriate manufacturer directory under <code>devices/</code>. For example: - <code>devices/dexcom/g6_csv.py</code> - <code>devices/medtronic/guardian_export.py</code></p>"},{"location":"contributing/formats/#2-define-core-components","title":"2. Define Core Components","text":""},{"location":"contributing/formats/#device-format","title":"Device Format","text":"<pre><code>MY_DEVICE_FORMAT = DeviceFormat(\n    name=\"unique_format_name\",  # Use lowercase with underscores\n    files=[...]  # List of FileConfig objects\n)\n</code></pre>"},{"location":"contributing/formats/#file-configuration","title":"File Configuration","text":"<pre><code>FileConfig(\n    name_pattern=\"*.csv\",  # Glob pattern to match files\n    file_type=FileType.CSV,  # SQLITE, CSV, JSON, or XML\n    tables=[...]  # List of TableStructure objects\n)\n</code></pre>"},{"location":"contributing/formats/#table-structure","title":"Table Structure","text":"<pre><code>TableStructure(\n    name=\"table_name\",  # Empty string for CSV files\n    timestamp_column=\"timestamp\",  # Column containing timestamps\n    columns=[...]  # List of ColumnMapping objects\n)\n</code></pre>"},{"location":"contributing/formats/#column-mapping","title":"Column Mapping","text":"<pre><code>ColumnMapping(\n    source_name=\"original_column_name\",\n    data_type=DataType.CGM,  # Type of data in this column\n    unit=Unit.MGDL,  # Unit of measurement (if applicable)\n    requirement=ColumnRequirement.REQUIRED_WITH_DATA,  # Column requirement type\n    is_primary=True  # Whether this is primary data for the type\n)\n</code></pre>"},{"location":"contributing/formats/#3-available-data-types","title":"3. Available Data Types","text":"<p>The system supports these core data types:</p> <ul> <li><code>DataType.CGM</code>: Continuous glucose monitoring data</li> <li><code>DataType.BGM</code>: Blood glucose meter readings</li> <li><code>DataType.INSULIN</code>: Insulin doses</li> <li><code>DataType.INSULIN_META</code>: Insulin metadata (e.g., brand)</li> <li><code>DataType.CARBS</code>: Carbohydrate intake</li> <li><code>DataType.NOTES</code>: Text notes/comments</li> </ul>"},{"location":"contributing/formats/#4-column-requirements","title":"4. Column Requirements","text":"<p>Define how each column should be validated:</p> <ul> <li><code>CONFIRMATION_ONLY</code>: Column must exist but data isn't read</li> <li><code>REQUIRED_WITH_DATA</code>: Column must exist and contain data</li> <li><code>REQUIRED_NULLABLE</code>: Column must exist but can have missing values</li> <li><code>OPTIONAL</code>: Column may or may not exist</li> </ul>"},{"location":"contributing/formats/#5-units-of-measurement","title":"5. Units of Measurement","text":"<p>Available units:</p> <ul> <li><code>Unit.MGDL</code>: Blood glucose in mg/dL</li> <li><code>Unit.MMOL</code>: Blood glucose in mmol/L</li> <li><code>Unit.UNITS</code>: Insulin units</li> <li><code>Unit.GRAMS</code>: Carbohydrates in grams</li> </ul>"},{"location":"contributing/formats/#6-multiple-data-types-and-files","title":"6. Multiple Data Types and Files","text":"<p>Your format can support multiple data types and files. For example:</p> <pre><code>DeviceFormat(\n    name=\"multi_file_format\",\n    files=[\n        FileConfig(\n            name_pattern=\"glucose.csv\",\n            file_type=FileType.CSV,\n            tables=[\n                TableStructure(\n                    name=\"\",\n                    timestamp_column=\"Time\",\n                    columns=[\n                        ColumnMapping(\n                            source_name=\"Glucose\",\n                            data_type=DataType.CGM,\n                            unit=Unit.MGDL,\n                        ),\n                    ],\n                ),\n            ],\n        ),\n        FileConfig(\n            name_pattern=\"insulin.csv\",\n            file_type=FileType.CSV,\n            tables=[\n                TableStructure(\n                    name=\"\",\n                    timestamp_column=\"Time\",\n                    columns=[\n                        ColumnMapping(\n                            source_name=\"Dose\",\n                            data_type=DataType.INSULIN,\n                            unit=Unit.UNITS,\n                        ),\n                        ColumnMapping(\n                            source_name=\"Type\",\n                            data_type=DataType.INSULIN_META,\n                        ),\n                    ],\n                ),\n            ],\n        ),\n    ],\n)\n</code></pre>"},{"location":"contributing/formats/#7-primary-vs-secondary-data","title":"7. Primary vs Secondary Data","text":"<p>For each data type, you can have one primary column and multiple secondary columns:</p> <pre><code>TableStructure(\n    name=\"\",\n    timestamp_column=\"Time\",\n    columns=[\n        ColumnMapping(\n            source_name=\"calculated_glucose\",\n            data_type=DataType.CGM,\n            unit=Unit.MGDL,\n            is_primary=True,  # Primary column\n        ),\n        ColumnMapping(\n            source_name=\"raw_glucose\",\n            data_type=DataType.CGM,\n            unit=Unit.MGDL,\n            is_primary=False,  # Secondary column\n        ),\n    ],\n)\n</code></pre>"},{"location":"contributing/formats/#8-validation-requirements","title":"8. Validation Requirements","text":"<p>Your format must pass these validations:</p> <ol> <li>At least one file must be defined</li> <li>Each file must have at least one table</li> <li>Each table must have at least one column</li> <li>Column names within a table must be unique</li> <li>Each data type can have only one primary column per table</li> <li>CSV files must have exactly one table with an empty name</li> </ol>"},{"location":"contributing/formats/#9-testing-your-format","title":"9. Testing Your Format","text":"<p>Test your format by:</p> <ol> <li>Adding it to the appropriate manufacturer directory</li> <li>Running the cli script with an example data file and debug argument: <pre><code>python -m src.cli your_test_file.csv --debug\n</code></pre></li> </ol>"},{"location":"contributing/formats/#10-common-patterns-and-best-practices","title":"10. Common Patterns and Best Practices","text":"<ol> <li> <p>Column Requirements</p> <ul> <li>Use <code>CONFIRMATION_ONLY</code> for device identification columns</li> <li>Use <code>REQUIRED_NULLABLE</code> for optional data types</li> <li>Use <code>REQUIRED_WITH_DATA</code> for essential columns</li> </ul> </li> <li> <p>Data Organisation</p> <ul> <li>Group related columns in the same table</li> <li>Keep primary/secondary relationships clear</li> <li>Document any special handling requirements</li> </ul> </li> <li> <p>File Patterns</p> <ul> <li>Use specific patterns when possible (e.g., \"export.csv\")</li> <li>Use wildcards carefully (e.g., \"*.sqlite\")</li> </ul> </li> <li> <p>Error Handling</p> <ul> <li>Define clear validation requirements</li> <li>Document any format-specific quirks</li> <li>Handle missing or nullable values appropriately</li> </ul> </li> </ol>"},{"location":"contributing/formats/#examples","title":"Examples","text":"<p>See these existing format definitions for reference:</p> <ol> <li> <p>XDrip+ SQLite Format:</p> <ul> <li>Handles multiple tables</li> <li>Shows primary/secondary relationships</li> <li>Demonstrates metadata handling</li> </ul> </li> <li> <p>LibreView CSV Format:</p> <ul> <li>Shows single CSV file handling</li> <li>Demonstrates multiple data types</li> <li>Shows device confirmation columns</li> </ul> </li> </ol>"},{"location":"contributing/formats/#making-your-format-more-robust","title":"Making Your Format More Robust","text":""},{"location":"contributing/formats/#handle-edge-cases","title":"Handle Edge Cases","text":"<p>Add <code>REQUIRED_NULLABLE</code> for columns that might be empty Document any special data formats or requirements Consider adding validation columns when needed</p>"},{"location":"contributing/formats/#improve-documentation","title":"Improve Documentation","text":"<pre><code>MY_DEVICE_FORMAT = DeviceFormat(\n    name=\"my_device_format\",\n    files=[\n        FileConfig(\n            name_pattern=\"*.csv\",\n            file_type=FileType.CSV,\n            tables=[\n                TableStructure(\n                    name=\"\",\n                    timestamp_column=\"Time\",\n                    # Document special requirements\n                    # Example: Time column format: \"YYYY-MM-DD HH:mm:ss\"\n                    columns=[...]\n                )\n            ]\n        )\n    ]\n)\n</code></pre>"},{"location":"contributing/formats/#format-verification-checklist","title":"Format Verification Checklist","text":"<p> All required columns mapped</p> <p> Units correctly specified</p> <p> Primary/secondary relationships clear</p> <p> Timestamp format documented</p> <p> Edge cases handled</p> <p> Requirements appropriate for each column</p> <p> Documentation complete</p>"},{"location":"contributing/formats/#need-help","title":"Need Help?","text":"<p>If you need assistance:</p> <ol> <li>Check existing format definitions for examples</li> <li>Review the core data types documentation</li> <li>Open an issue for format-specific questions</li> <li>Submit a draft PR for feedback</li> </ol> <p>Remember: Good format definitions are clear, well-documented, and handle edge cases appropriately.</p>"},{"location":"contributing/guide/","title":"Contributing to Diabetes Data Processing Project","text":"<p>Welcome to our diabetes data processing project! We're working to create a unified tool that can process and analyse data from various diabetes management devices and applications. Your contributions can help make diabetes data more accessible and useful for everyone.</p>"},{"location":"contributing/guide/#setting-up-your-development-environment","title":"Setting Up Your Development Environment","text":"<p>We use Poetry for dependency management to ensure consistent development environments. If you haven't used Poetry before, it's a modern Python package manager that handles dependencies and virtual environments automatically.</p> <ol> <li> <p>First, install Poetry if you haven't already:    <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre></p> </li> <li> <p>Clone the repository:    <pre><code>git clone https://github.com/[username]/diabetes-data-processing.git\ncd diabetes-data-processing\n</code></pre></p> </li> <li> <p>Install dependencies using Poetry:    <pre><code>poetry install\n</code></pre>    This will create a virtual environment and install all required dependencies.</p> </li> </ol> <p>If you prefer using pip, you can alternatively:    <pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npip install -r requirements.txt\npip install -r requirements-dev.txt\n</code></pre></p>"},{"location":"contributing/guide/#code-style-and-quality-standards","title":"Code Style and Quality Standards","text":"<p>Our project maintains high code quality standards through consistent style and thorough testing. We follow these core principles:</p> <ol> <li>Readability takes precedence over clever solutions</li> <li>Consistency across the entire codebase</li> <li>Well-documented and maintainable code</li> <li>Pragmatic approach to style rules</li> </ol>"},{"location":"contributing/guide/#code-style-specifications","title":"Code Style Specifications","text":"<p>We follow standard Python naming conventions:</p> <ul> <li>Functions and variables use <code>snake_case</code></li> <li>Classes use <code>PascalCase</code></li> <li>Constants use <code>UPPER_CASE</code></li> <li>Protected/private attributes use <code>_leading_underscore</code></li> </ul> <p>File and Function Structure:</p> <ul> <li>Maximum line length: 100 characters</li> <li>Maximum file length: 1000 lines</li> <li>Functions should have no more than 5 arguments</li> <li>Classes should have no more than 7 attributes and 20 public methods</li> </ul> <p>Documentation Requirements:</p> <ul> <li>All modules must have docstrings</li> <li>Public methods require docstrings; private methods (starting with <code>_</code>) do not</li> <li>Type hints are required for function parameters and return values</li> </ul> <p>We use pre-commit hooks to maintain code quality. After setting up your environment, install the pre-commit hooks: <pre><code>poetry run pre-commit install\n</code></pre></p>"},{"location":"contributing/guide/#how-you-can-help","title":"How You Can Help","text":""},{"location":"contributing/guide/#1-share-sample-data-files","title":"1. Share Sample Data Files","text":"<p>We currently need sample data files from various diabetes management devices and applications. This helps us understand different data formats and ensure our tool works correctly. Please use our New Device Format Submission issue on Github.</p>"},{"location":"contributing/guide/#currently-supported-formats","title":"Currently Supported Formats:","text":"<ul> <li>XDrip+ SQLite backup files</li> </ul>"},{"location":"contributing/guide/#priority-formats-wed-like-to-support","title":"Priority Formats We'd Like to Support:","text":"<ul> <li>Dexcom G6/G7 exports</li> <li>Freestyle Libre 1/2/3 exports</li> <li>Medtronic pump data</li> <li>Tandem t:slim pump data</li> <li>Nightscout database exports</li> <li>Tidepool platform exports</li> </ul>"},{"location":"contributing/guide/#data-submission-guidelines","title":"Data Submission Guidelines","text":"<p>When submitting sample files:</p> <ol> <li>Remove Personal Information:</li> <li>Anonymise all personal identifiers</li> <li>Replace actual names with placeholder text</li> <li>Remove medical ID numbers and device serial numbers</li> <li> <p>Ensure timestamps are within a reasonable timeframe (e.g., last 7 days)</p> </li> <li> <p>Prepare Your Files:</p> </li> <li>Export data using standard export features</li> <li>Document your export process</li> <li>Note software/firmware versions</li> <li> <p>Include relevant export settings</p> </li> <li> <p>Submit Your Contribution:</p> </li> <li>Create an issue with tag <code>new-format-sample</code></li> <li>Attach anonymised sample files</li> <li>Include device/application details and export method</li> </ol>"},{"location":"contributing/guide/#2-format-definition-contributions","title":"2. Format Definition Contributions","text":"<p>To contribute a new format definition:</p> <ol> <li>Study our existing XDrip+ SQLite format as a template:</li> </ol> <pre><code>XDRIP_SQLITE_FORMAT = DeviceFormat(\n    name=\"xdrip_sqlite\",\n    files=[\n        FileConfig(\n            name_pattern=\"*.sqlite\",\n            file_type=FileType.SQLITE,\n            tables=[...]\n        )\n    ],\n)\n</code></pre> <ol> <li>Create your format definition:</li> <li>Place new formats in <code>src/core/devices/</code></li> <li>Include comprehensive documentation</li> <li>Define table structures and column mappings</li> <li>Specify data types and units</li> <li> <p>Document timestamp formats</p> </li> <li> <p>Submit your contribution:</p> </li> <li>Fork the repository</li> <li>Create a branch: <code>git checkout -b new-format/[device-name]</code></li> <li>Add your format definition</li> <li>Include sample data files that demonstrate the format structure</li> <li>Create a pull request</li> </ol>"},{"location":"contributing/guide/#quality-assurance","title":"Quality Assurance","text":"<p>When contributing new code or format definitions, ensure quality through these steps:</p> <ol> <li> <p>Run the test suite:    <pre><code>poetry run pytest\n</code></pre></p> </li> <li> <p>Ensure code quality:    <pre><code>poetry run pylint src/\npoetry run black src/\npoetry run isort src/\n</code></pre></p> </li> </ol>"},{"location":"contributing/guide/#error-handling","title":"Error Handling","text":"<p>When adding new functionality: - Use specific exception classes from <code>core/exceptions.py</code> - Avoid catching generic exceptions - Add meaningful error messages - Document error conditions in docstrings</p>"},{"location":"contributing/guide/#commit-guidelines","title":"Commit Guidelines","text":"<p>Write clear, descriptive commit messages:</p> <pre><code>Add Dexcom G6 format definition\n\n- Implements basic CSV parsing for Dexcom clarity exports\n- Adds unit conversion for mmol/L to mg/dL\n- Includes timestamp standardisation\n- Fixes #123\n</code></pre>"},{"location":"contributing/guide/#getting-help","title":"Getting Help","text":"<ul> <li>Create an issue with tag <code>question</code> for general queries</li> <li>Join our discussions in the GitHub Discussions tab</li> <li>Check existing issues and pull requests for similar topics</li> </ul>"},{"location":"contributing/guide/#code-of-conduct","title":"Code of Conduct","text":"<p>We are committed to providing a welcoming and inclusive experience for everyone. Please:</p> <ul> <li>Be respectful and considerate in communications</li> <li>Focus on the technical aspects of contributions</li> <li>Help others learn and grow</li> <li>Report inappropriate behaviour to project maintainers</li> </ul>"},{"location":"contributing/guide/#licence","title":"Licence","text":"<p>By contributing to this project, you agree that your contributions will be licenced under the same terms as the project (see our licence page).</p> <p>Thank you for helping make diabetes data more accessible and useful for everyone!</p>"},{"location":"contributing/license/","title":"License","text":"<p>This project and all attached documentation are available using the MIT licence.</p> <p>Our licence file can be found in the root directory of the Github repo. View full licence here or review the copy below.</p> <p>MIT licence</p> <p>Copyright (c) 2025 Warren Bebbington</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"getting-started/","title":"Overview","text":"Getting Started with CGM Data Processor <p>Process your diabetes device data in minutes</p>"},{"location":"getting-started/#overview","title":"\ud83c\udfaf Overview","text":"<p>CGM Data Processor is a Python framework that helps you:</p> <ul> <li>Import data from multiple diabetes management systems</li> <li>Process and align CGM readings with treatment data</li> <li>Export cleaned, validated datasets for analysis</li> </ul>"},{"location":"getting-started/#requirements","title":"\ud83d\udccb Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>Supported data formats: SQLite, CSV, XML</li> <li>Basic Python knowledge for custom analysis</li> </ul>"},{"location":"getting-started/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<ul> <li>Installation Guide - Set up the package</li> <li>Basic Usage - Process your first dataset</li> <li>Data Import - Learn about supported formats</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"Installation <p>Set up CGM Data Processor for development</p>"},{"location":"getting-started/installation/#simple-install","title":"Simple install","text":"<pre><code># Clone repository\ngit clone https://github.com/Warren8824/cgm-data-processor.git\ncd cgm-data-processor\n\n# Install dependencies using pip\npip install -r requirements.txt\n\n# Or using Poetry \npoetry install\n</code></pre> <p>And as simple as that the system is ready to use. - Check out our Basic Usage page.</p>"},{"location":"getting-started/installation/#development-setup","title":"\ud83d\udee0\ufe0f Development Setup","text":"<ul> <li>Python 3.10+ required</li> <li>Poetry for dependency management (Preferred)</li> <li>Git for version control</li> </ul> <pre><code># Clone repository\ngit clone https://github.com/Warren8824/cgm-data-processor.git\ncd cgm-data-processor\n\n# Install Poetry (if not installed)\ncurl -sSL https://install.python-poetry.org | python3 -\n\n# Create and activate new environment\n\n# macOS/Linux:\npoetry env activate\nsource $(poetry env info --path)/bin/activate\n\n# Windows(Powershell):\npoetry env activate\n(Invoke-Expression \"$(poetry env info --path)\\Scripts\\Activate\")\n\n# Install development dependencies\n\n# Using poetry:\npoetry install --with dev\n\n# or using venv:\npip install -r requirements-dev.txt\n\n\n# Setup pre-commit hooks\npoetry run pre-commit install\n\n# Run tests\npoetry run pytest\n</code></pre> <p>\u2705 Verify Installation</p> <pre><code>from src.core.format_registry import FormatRegistry\n\n# Should print available formats\nregistry = FormatRegistry()\nprint(registry.formats)\n</code></pre>"},{"location":"getting-started/quickstart/basic/","title":"Basic Usage","text":"Basic Usage <p>Process and analyse your diabetes data</p>"},{"location":"getting-started/quickstart/basic/#command-line-usage","title":"\ud83d\ude80 Command Line Usage","text":"<ul> <li>Basic: <code>python -m src.cli data.sqlite</code></li> <li>Custom output: <code>python -m src.cli data.sqlite --output my_folder</code></li> <li>Debug mode: <code>python -m src.cli data.sqlite --debug</code></li> </ul>"},{"location":"getting-started/quickstart/basic/#processing-options","title":"\u2699\ufe0f Processing Options","text":"<pre><code>python -m src.cli data.sqlite \\\n    --interpolation-limit 6   # Max CGM gaps to fill (6 = 30 mins)\n    --bolus-limit 10.0       # Max bolus insulin units\n    --max-dose 20.0          # Max valid insulin dose\n    --output ./my_analysis   # Output location\n</code></pre>"},{"location":"getting-started/quickstart/basic/#parameter-guide","title":"\ud83d\udcca Parameter Guide","text":"<ul> <li><code>interpolation-limit</code>: Gaps larger than this won't be filled (default: 4 = 20 mins)</li> <li><code>bolus-limit</code>: Doses above this classified as basal (default: 8.0 units)</li> <li><code>max-dose</code>: Doses above this flagged as invalid (default: 15.0 units)</li> </ul>"},{"location":"getting-started/quickstart/import/","title":"Data Import","text":"Data Import <p>Supported formats and data sources</p>"},{"location":"getting-started/quickstart/import/#supported-devices","title":"\ud83d\udcf1 Supported Devices","text":"<ul> <li>XDrip+ (SQLite format)</li> <li>Dexcom (CSV format - coming soon)</li> <li>Freestyle Libre (CSV format - coming soon)</li> </ul>"},{"location":"getting-started/quickstart/import/#xdrip-export-guide","title":"\ud83d\udcbe XDrip+ Export Guide","text":"<ul> <li>Open XDrip+</li> <li>Navigate to Settings \u2192 Data Export</li> <li>Select \"Export Database\"</li> <li>Save the .sqlite file</li> </ul>"},{"location":"getting-started/quickstart/import/#data-requirements","title":"\ud83d\udd0d Data Requirements","text":"<ul> <li>CGM readings with timestamps</li> <li>Treatment records (insulin, carbs)</li> <li>No missing required columns</li> <li>Valid data ranges for readings</li> </ul>"},{"location":"getting-started/quickstart/processing/","title":"Data Processing","text":"Data Processing <p>Understanding and configuring data processing options</p>"},{"location":"getting-started/quickstart/processing/#processing-parameters","title":"\ud83d\udd04 Processing Parameters","text":"<ul> <li>CGM Gap Handling: How many missing readings to interpolate</li> <li>Insulin Classification: Thresholds for bolus vs basal</li> <li>Data Validation: Maximum valid insulin doses</li> </ul>"},{"location":"getting-started/quickstart/processing/#configuration-examples","title":"\u2699\ufe0f Configuration Examples","text":"<pre><code># Conservative gap filling (15 mins max)\npython -m src.cli data.sqlite --interpolation-limit 3\n\n# Higher insulin thresholds\npython -m src.cli data.sqlite --bolus-limit 12.0 --max-dose 25.0\n\n# Strict validation\npython -m src.cli data.sqlite --bolus-limit 6.0 --max-dose 12.0\n</code></pre>"},{"location":"getting-started/quickstart/processing/#output-structure","title":"\ud83d\udcca Output Structure","text":"<ul> <li>complete_dataset/: Full processed data with applied parameters</li> <li>monthly/: Split data maintaining processing settings</li> <li>processing_notes.json: Configuration and quality metrics</li> </ul> Example Output Structure:  <pre><code>data/exports\n\u251c\u2500\u2500 2023-06-03_to_2024-09-28_complete\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 aligned_data.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 carbs.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cgm.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 insulin.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 notes.csv\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 processing_notes.json\n\u2514\u2500\u2500 monthly\n    \u251c\u2500\u2500 2023-06\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 aligned_data.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 carbs.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 cgm.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 insulin.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 notes.csv\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 processing_notes.json\n    \u251c\u2500\u2500 2023-07\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 aligned_data.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 carbs.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 cgm.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 insulin.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 notes.csv\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 processing_notes.json\n</code></pre>"},{"location":"user-guide/data-types/","title":"Data Types & Processing","text":"Data Types <p>Core data types and their processing</p>"},{"location":"user-guide/data-types/#cgm-data","title":"\ud83d\udcc8 CGM Data","text":"<ul> <li>5-minute glucose readings</li> <li>Gap detection and interpolation</li> <li>Units: mg/dL and mmol/L</li> <li>Quality metrics: completeness, noise levels</li> </ul>"},{"location":"user-guide/data-types/#insulin-data","title":"\ud83d\udc89 Insulin Data","text":"<ul> <li>Bolus: Meal and correction doses</li> <li>Basal: Long-acting background insulin</li> <li>Automatic classification based on dose size</li> <li>Metadata support for insulin types</li> </ul>"},{"location":"user-guide/data-types/#carbohydrate-data","title":"\ud83c\udf4e Carbohydrate Data","text":"<ul> <li>Meal entries in grams</li> <li>Timestamp alignment with insulin</li> <li>Minimum 1g threshold</li> </ul>"},{"location":"user-guide/data-types/#notes-data","title":"\ud83d\udcdd Notes Data","text":"<ul> <li>Treatment notes and events</li> <li>Timestamped annotations</li> <li>Flexible text storage</li> </ul>"},{"location":"user-guide/errors/","title":"Error Handling","text":"Error Handling <p>Common errors and troubleshooting</p>"},{"location":"user-guide/errors/#common-errors","title":"\u274c Common Errors","text":"<ul> <li>FormatDetectionError: File format not recognised</li> <li>DataProcessingError: Invalid or corrupt data</li> <li>AlignmentError: Cannot align datasets</li> <li>FileAccessError: Cannot read input file</li> </ul>"},{"location":"user-guide/errors/#troubleshooting","title":"\ud83d\udd0d Troubleshooting","text":"<pre><code># Enable debug mode for detailed errors\npython -m src.cli data.sqlite --debug\n\n# Common debug output:\n\u2713 Format Detection Successful\n\u2717 Data Reading Failed: Missing required columns\n   Details: Table 'BgReadings' missing 'calculated_value'\n</code></pre>"},{"location":"user-guide/errors/#data-validation-errors","title":"\ud83d\udeab Data Validation Errors","text":"<ul> <li>Invalid glucose values (outside 40-400 mg/dL)</li> <li>Insulin doses exceeding max_dose</li> <li>Missing timestamps or required fields</li> <li>Duplicate timestamps in CGM data</li> </ul>"},{"location":"user-guide/formats/","title":"Supported Formats","text":"Supported File Formats <p>Supported diabetes device data formats</p>"},{"location":"user-guide/formats/#xdrip-sqlite","title":"\ud83d\udcf1 XDrip+ SQLite","text":"<ul> <li>Default SQLite database from XDrip+</li> <li>Contains BgReadings and Treatments tables</li> <li>Full CGM and treatment data support</li> <li>Export: Settings \u2192 Data Export \u2192 Export Database</li> </ul>"},{"location":"user-guide/formats/#schema-structure","title":"\ud83d\udcd1 Schema Structure","text":"<pre><code>-- BgReadings Table\ntimestamp          -- UTC timestamp\ncalculated_value   -- Glucose in mg/dL\nraw_data          -- Raw sensor data\n\n-- Treatments Table\ntimestamp    -- UTC timestamp\ninsulin      -- Insulin dose in units\ninsulinJSON  -- Insulin type metadata\ncarbs       -- Carbohydrates in grams\nnotes       -- Treatment notes\n</code></pre>"},{"location":"user-guide/formats/#other-formats-coming-soon","title":"\ud83d\udd04 Other Formats (Coming Soon)","text":"<ul> <li>Dexcom CSV Export</li> <li>Freestyle Libre CSV</li> <li>Nightscout Data</li> </ul>"},{"location":"user-guide/formats/#add-custom-file-formats","title":"Add Custom File Formats","text":"<p>Check out our Format Contribution guidelines.</p>"}]}