{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CGM Data Processor","text":"<p>A powerful Python tool for processing and analyzing Continuous Glucose Monitoring (CGM) data from various diabetes devices. This tool automatically detects file formats, processes data, and aligns multiple data streams for comprehensive diabetes data analysis.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Automatic Format Detection: Intelligently identifies data formats from different CGM devices</li> <li>Multi-Device Support: <ul> <li>Dexcom CGM systems</li> <li>Libre CGM systems</li> <li>XDrip+ data</li> </ul> </li> <li>Flexible Data Processing:<ul> <li>CGM readings</li> <li>Insulin doses</li> <li>Carbohydrate intake</li> <li>Notes and events</li> </ul> </li> <li>Smart Data Alignment: Automatically aligns different data streams by timestamp</li> <li>Extensible Architecture: Easy to add support for new devices and data formats</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"<pre><code>pip install cgm-data-processor\n</code></pre>"},{"location":"#basic-usage","title":"Basic Usage","text":"<p>Process a single diabetes device data file:</p> <pre><code>cgm-process path/to/your/file.csv\n</code></pre> <p>Enable detailed analysis with debug mode:</p> <pre><code>cgm-process path/to/your/file.csv --debug\n</code></pre>"},{"location":"#processing-pipeline","title":"Processing Pipeline","text":"<ol> <li>Format Detection: Automatically identifies the source device and data format</li> <li>Data Reading: Extracts data using format-specific readers</li> <li>Data Processing: Processes and validates data streams</li> <li>Data Alignment: Aligns multiple data streams by timestamp</li> <li>Analysis: Provides detailed data analysis in debug mode</li> </ol> <pre><code>graph LR\n    A[Your Data File] --&gt; B[Format Detection]\n    B --&gt; C[Data Reading]\n    C --&gt; D[Processing]\n    D --&gt; E[Alignment]\n    E --&gt; F[Export]</code></pre>"},{"location":"#project-status","title":"Project Status","text":"<p>The project is under active development. Current focus areas:</p> <ul> <li>Implementation of data exporters</li> <li>Enhanced data visualization</li> <li>Additional device format support</li> </ul>"},{"location":"#documentation","title":"Documentation","text":"<ul> <li>Getting Started Guide</li> <li>Supported Formats</li> <li>API Reference</li> <li>Development Guide</li> </ul>"},{"location":"api/","title":"API Reference Overview","text":"<p>The CGM Data Processor is built around a modular architecture that handles diabetes device data processing through distinct stages. This overview explains the core architecture, components, and data flow.</p>"},{"location":"api/#architecture-overview","title":"Architecture Overview","text":"<p>The project is organized into several key components:</p> <pre><code>graph TB\n    A[File Input] --&gt; B[Format Detection]\n    B --&gt; C[Data Reading]\n    C --&gt; D1[CGM Processing]\n    C --&gt; D2[Insulin Processing]\n    C --&gt; D3[Carbs Processing]\n    C --&gt; D4[Notes Processing]\n    D1 --&gt; E[Processed Data]\n    D2 --&gt; E\n    D3 --&gt; E\n    D4 --&gt; E\n    E --&gt; G[Data Export]\n    E --&gt; F[Data Alignment]\n    F --&gt; G</code></pre>"},{"location":"api/#core-components","title":"Core Components","text":""},{"location":"api/#1-data-types-system","title":"1. Data Types System","text":"<ul> <li>Defines fundamental data structures through <code>data_types.py</code></li> <li>Provides enums for supported data types:<ul> <li>Continuous Glucose Monitoring (CGM)</li> <li>Blood Glucose Monitoring (BGM)</li> <li>Insulin doses</li> <li>Carbohydrate intake</li> <li>Notes and events</li> </ul> </li> <li>Handles unit management and data validation</li> </ul>"},{"location":"api/#2-format-registry","title":"2. Format Registry","text":"<ul> <li>Manages device format definitions</li> <li>Provides format detection and validation</li> <li>Supports dynamic loading of new device formats</li> <li>Maps data sources to internal representations</li> </ul>"},{"location":"api/#3-processing-pipeline","title":"3. Processing Pipeline","text":"<ul> <li>Data reading via format-specific readers</li> <li>Type-specific data processing</li> <li>Data alignment and synchronization</li> <li>Error handling and validation</li> </ul>"},{"location":"api/#data-flow","title":"Data Flow","text":"<ol> <li> <p>Format Detection <pre><code>from src.file_parser.format_registry import FormatRegistry\nfrom src.file_parser.format_detector import FormatDetector\n\nregistry = FormatRegistry()\ndetector = FormatDetector(registry)\ndetected_format = detector.detect_format(file_path)\n</code></pre></p> </li> <li> <p>Data Reading <pre><code>from src.readers.base import BaseReader\n\nreader = BaseReader.get_reader_for_format(detected_format, file_path)\ntable_data = reader.read_all_tables()\n</code></pre></p> </li> <li> <p>Data Processing <pre><code>from src.processors import DataProcessor\n\nprocessor = DataProcessor()\nprocessed_data = processor.process_tables(table_data, table_configs)\n</code></pre></p> </li> <li> <p>Data Alignment <pre><code>from src.core.aligner import Aligner\n\naligner = Aligner()\naligned_data = aligner.align(processed_data)\n</code></pre></p> </li> </ol>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>The project uses a hierarchical exception system:</p> <pre><code>CGMProcessorError\n\u251c\u2500\u2500 FileError\n\u2502   \u251c\u2500\u2500 FileAccessError\n\u2502   \u251c\u2500\u2500 FileExtensionError\n\u2502   \u2514\u2500\u2500 FileParseError\n\u251c\u2500\u2500 FormatError\n\u2502   \u251c\u2500\u2500 FormatDetectionError\n\u2502   \u251c\u2500\u2500 FormatLoadingError\n\u2502   \u2514\u2500\u2500 FormatValidationError\n\u251c\u2500\u2500 ProcessingError\n\u2502   \u251c\u2500\u2500 DataProcessingError\n\u2502   \u251c\u2500\u2500 AlignmentError\n\u2502   \u2514\u2500\u2500 DataQualityError\n\u2514\u2500\u2500 ValidationError\n    \u2514\u2500\u2500 DataValidationError\n</code></pre>"},{"location":"api/#type-processing-system","title":"Type Processing System","text":"<p>The processor system uses a registry pattern for handling different data types:</p> <pre><code>from src.core.data_types import DataType\nfrom src.processors.base import DataProcessor, BaseTypeProcessor\n\n@DataProcessor.register_processor(DataType.CGM)\nclass CGMProcessor(BaseTypeProcessor):\n    def process_type(self, columns: List[ColumnData]) -&gt; ProcessedTypeData:\n        # Processing implementation\n        pass\n</code></pre>"},{"location":"api/#key-concepts","title":"Key Concepts","text":"<ol> <li>Device Formats</li> <li>Each device format is defined through a <code>DeviceFormat</code> class</li> <li>Formats specify file types, table structures, and column mappings</li> <li> <p>Support for multiple files and tables within a format</p> </li> <li> <p>Data Type Processing</p> </li> <li>Each data type has a dedicated processor</li> <li>Processors handle unit conversion and validation</li> <li> <p>Support for primary and secondary data columns</p> </li> <li> <p>Data Alignment</p> </li> <li>Time-based alignment of different data streams</li> <li>Configurable alignment frequency</li> <li> <p>Handles missing data and gaps</p> </li> <li> <p>Validation</p> </li> <li>Format validation during loading</li> <li>Data validation during processing</li> <li>Unit compatibility checking</li> </ol>"},{"location":"api/#next-steps","title":"Next Steps","text":"<p>Explore specific components in detail:</p> <ul> <li>Data Types Reference - Core data structures and types</li> <li>Format Registry - Format management and detection</li> <li>Data Processing - Data processing system</li> <li>Error Handling - Complete exception hierarchy</li> </ul>"},{"location":"api/core/alignment/","title":"Data Alignment API Reference","text":"<p>The Alignment system provides functionality to synchronize different types of diabetes data (CGM, insulin, carbs, notes) to a common timeline, enabling integrated analysis of diabetes management data.</p>"},{"location":"api/core/alignment/#core-components","title":"Core Components","text":"AlignmentResultAligner <pre><code>@dataclass\nclass AlignmentResult:\n    dataframe: pd.DataFrame    # The aligned timeseries data\n    start_time: pd.Timestamp  # Start of aligned timeline\n    end_time: pd.Timestamp    # End of aligned timeline\n    frequency: str            # Alignment frequency\n    processing_notes: List[str] # Notes about the alignment process\n</code></pre> <pre><code>class Aligner:\n    def align(\n        self,\n        processed_data: Dict[DataType, ProcessedTypeData],\n        reference_df: pd.DataFrame = None,\n        freq: str = \"5min\",\n    ) -&gt; AlignmentResult:\n        \"\"\"Align all data to a reference timeline.\"\"\"\n</code></pre>"},{"location":"api/core/alignment/#alignment-process","title":"Alignment Process","text":""},{"location":"api/core/alignment/#timeline-validation","title":"Timeline Validation","text":"<p>Timeline Requirements</p> <p>The reference timeline must meet these criteria:</p> <pre><code>def _validate_timeline(self, reference_data: pd.DataFrame, freq: str) -&gt; None:\n    # 1. Must not be empty\n    if reference_data.empty:\n        raise AlignmentError(\"Reference data is empty\")\n\n    # 2. Must have DatetimeIndex\n    if not isinstance(reference_data.index, pd.DatetimeIndex):\n        raise AlignmentError(\"Reference data must have DatetimeIndex\")\n\n    # 3. Must be monotonically increasing\n    if not reference_data.index.is_monotonic_increasing:\n        raise AlignmentError(\"Reference data index must be monotonically increasing\")\n\n    # 4. Must match expected frequency\n    time_diffs = reference_data.index.to_series().diff()\n    modal_diff = time_diffs.mode()[0]\n    expected_diff = pd.Timedelta(freq)\n\n    if modal_diff != expected_diff:\n        raise AlignmentError(\n            f\"Reference data frequency {modal_diff} does not match expected {freq}\"\n        )\n</code></pre>"},{"location":"api/core/alignment/#data-type-alignment-strategies","title":"Data Type Alignment Strategies","text":"InsulinCarbsNotes <pre><code>def _align_insulin(\n    self, df: pd.DataFrame, reference_index: pd.DatetimeIndex, freq: str\n) -&gt; pd.DataFrame:\n    \"\"\"Align insulin data.\"\"\"\n    # Round timestamps to alignment frequency\n    df.index = df.index.round(freq)\n\n    # Split and sum basal/bolus doses separately\n    basal_doses = df[\"dose\"].where(df[\"is_basal\"], 0)\n    bolus_doses = df[\"dose\"].where(df[\"is_bolus\"], 0)\n\n    # Resample to reference frequency\n    result = pd.DataFrame({\n        \"basal_dose\": basal_doses.resample(freq).sum(),\n        \"bolus_dose\": bolus_doses.resample(freq).sum(),\n    })\n\n    return result.reindex(reference_index).fillna(0)\n</code></pre> <pre><code>def _align_carbs(\n    self, df: pd.DataFrame, reference_index: pd.DatetimeIndex, freq: str\n) -&gt; pd.DataFrame:\n    \"\"\"Align carbohydrate data.\"\"\"\n    # Round timestamps\n    df.index = df.index.round(freq)\n\n    # Sum carbs in each interval\n    result = df[\"carbs_primary\"].resample(freq).sum()\n    return pd.DataFrame({\"carbs_primary\": result}).reindex(reference_index).fillna(0)\n</code></pre> <pre><code>def _align_notes(\n    self, df: pd.DataFrame, reference_index: pd.DatetimeIndex, freq: str\n) -&gt; pd.DataFrame:\n    \"\"\"Align notes data.\"\"\"\n    # Round timestamps\n    df.index = df.index.round(freq)\n\n    # Keep last note in each interval\n    result = df[\"notes_primary\"].resample(freq).last()\n    return pd.DataFrame({\"notes_primary\": result}).reindex(reference_index)\n</code></pre>"},{"location":"api/core/alignment/#usage-examples","title":"Usage Examples","text":""},{"location":"api/core/alignment/#basic-alignment","title":"Basic Alignment","text":"<pre><code>from src.core.aligner import Aligner\nfrom src.processors import DataProcessor\n\n# Process data\nprocessor = DataProcessor()\nprocessed_data = processor.process_tables(table_data, table_configs)\n\n# Align data\naligner = Aligner()\naligned_result = aligner.align(processed_data, freq=\"5min\")\n\n# Access aligned data\naligned_df = aligned_result.dataframe\nprint(f\"Aligned from {aligned_result.start_time} to {aligned_result.end_time}\")\nprint(f\"Using {aligned_result.frequency} frequency\")\n</code></pre>"},{"location":"api/core/alignment/#custom-reference-timeline","title":"Custom Reference Timeline","text":"<pre><code># Use custom reference timeline instead of CGM\ncustom_timeline = pd.DataFrame(\n    index=pd.date_range(start=\"2024-01-01\", end=\"2024-01-02\", freq=\"15min\")\n)\n\naligned_result = aligner.align(\n    processed_data,\n    reference_df=custom_timeline,\n    freq=\"15min\"\n)\n</code></pre>"},{"location":"api/core/alignment/#data-type-handling","title":"Data Type Handling","text":"<p>Alignment Strategies</p> <p>Each data type is handled differently based on its characteristics:</p> <ul> <li>CGM Data: Used as reference timeline by default</li> <li>Insulin Doses: Split into basal/bolus and summed within intervals</li> <li>Carbohydrates: Summed within intervals</li> <li>Notes: Last value within each interval is kept</li> </ul>"},{"location":"api/core/alignment/#column-naming","title":"Column Naming","text":"<p>The aligned DataFrame includes these columns:</p> <ul> <li>CGM data (reference):<ul> <li><code>cgm_primary</code></li> <li><code>cgm_primary_mmol</code></li> <li><code>cgm_2</code></li> <li><code>cgm_2_mmol</code></li> <li><code>missing_cgm</code></li> </ul> </li> <li>Insulin doses:<ul> <li><code>basal_dose</code></li> <li><code>bolus_dose</code></li> </ul> </li> <li>Carbohydrates:<ul> <li><code>carbs_primary</code></li> <li><code>carbs_2</code></li> </ul> </li> <li>Notes:<ul> <li><code>notes_primary</code></li> </ul> </li> </ul>"},{"location":"api/core/alignment/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    aligned_result = aligner.align(processed_data)\nexcept AlignmentError as e:\n    print(f\"Alignment failed: {str(e)}\")\n</code></pre>"},{"location":"api/core/alignment/#best-practices","title":"Best Practices","text":"<p>Alignment Tips</p> <ol> <li> <p>Reference Timeline</p> <ul> <li>Use CGM data as reference when available</li> <li>Ensure reference data has consistent frequency</li> <li>Validate timeline before alignment</li> </ul> </li> <li> <p>Frequency Selection</p> <ul> <li>Match frequency to reference data collection rate</li> <li>Consider memory usage for long time periods</li> <li>Standard frequencies: \"5min\", \"15min\", \"30min\", \"1H\"</li> </ul> </li> <li> <p>Data Preparation</p> <ul> <li>Clean and validate data before alignment</li> <li>Handle missing values appropriately</li> <li>Consider timezone consistency</li> </ul> </li> <li> <p>Performance</p> <ul> <li>Limit alignment range for large datasets</li> <li>Pre-filter unnecessary data</li> <li>Cache aligned results when appropriate</li> </ul> </li> </ol>"},{"location":"api/core/alignment/#integration-with-processors","title":"Integration with Processors","text":"<p>The alignment system works with processed data from type-specific processors:</p> <pre><code># Type processors prepare data for alignment\n@DataProcessor.register_processor(DataType.CGM)\nclass CGMProcessor(BaseTypeProcessor):\n    def process_type(self, columns: List[ColumnData]) -&gt; ProcessedTypeData:\n        # Process and prepare CGM data\n        ...\n\n@DataProcessor.register_processor(DataType.INSULIN)\nclass InsulinProcessor(BaseTypeProcessor):\n    def process_type(self, columns: List[ColumnData]) -&gt; ProcessedTypeData:\n        # Process and prepare insulin data\n        ...\n\n# Alignment uses the processed results\naligner = Aligner()\naligned_data = aligner.align(processed_results)\n</code></pre>"},{"location":"api/core/data-types/","title":"Data Types API Reference","text":"<p>The <code>core.data_types</code> module provides the foundational type system for handling diabetes device data. It defines a comprehensive set of data structures and enums that enable type-safe processing of various diabetes-related data formats.</p>"},{"location":"api/core/data-types/#core-enums","title":"Core Enums","text":""},{"location":"api/core/data-types/#datatype","title":"DataType","text":"<p>Defines the fundamental types of diabetes data that can be processed.</p> <pre><code>from src.core.data_types import DataType\n\nclass DataType(Enum):\n    CGM = auto()         # Continuous glucose monitoring readings\n    BGM = auto()         # Blood glucose meter readings\n    INSULIN = auto()     # Insulin doses\n    INSULIN_META = auto() # Insulin metadata (e.g., brand, type)\n    CARBS = auto()       # Carbohydrate intake\n    NOTES = auto()       # Text notes/comments\n</code></pre>"},{"location":"api/core/data-types/#filetype","title":"FileType","text":"<p>Specifies supported file formats for data imports.</p> <pre><code>class FileType(Enum):\n    SQLITE = \"sqlite\"\n    CSV = \"csv\"\n    JSON = \"json\"\n    XML = \"xml\"\n</code></pre>"},{"location":"api/core/data-types/#unit","title":"Unit","text":"<p>Defines standard units of measurement for diabetes data.</p> <pre><code>class Unit(Enum):\n    MGDL = \"mg/dL\"    # Blood glucose in milligrams per deciliter\n    MMOL = \"mmol/L\"   # Blood glucose in millimoles per liter\n    UNITS = \"U\"       # Insulin units\n    GRAMS = \"g\"       # Carbohydrates in grams\n</code></pre>"},{"location":"api/core/data-types/#columnrequirement","title":"ColumnRequirement","text":"<p>Specifies validation and data requirements for columns.</p> <pre><code>class ColumnRequirement(Enum):\n    CONFIRMATION_ONLY = auto()   # Column must exist but data isn't read\n    REQUIRED_WITH_DATA = auto()  # Column must exist with valid data\n    REQUIRED_NULLABLE = auto()   # Column must exist but can have missing values\n    OPTIONAL = auto()            # Column may or may not exist\n</code></pre>"},{"location":"api/core/data-types/#data-structures","title":"Data Structures","text":""},{"location":"api/core/data-types/#columnmapping","title":"ColumnMapping","text":"<p>Maps source data columns to standardized internal representations.</p> <pre><code>@dataclass\nclass ColumnMapping:\n    source_name: str\n    data_type: Optional[DataType] = None\n    unit: Optional[Unit] = None\n    requirement: ColumnRequirement = ColumnRequirement.REQUIRED_WITH_DATA\n    is_primary: bool = True\n</code></pre> <p>Key Concepts:</p> <ul> <li><code>source_name</code>: Original column name in the source data</li> <li><code>data_type</code>: Type of data contained in the column</li> <li><code>unit</code>: Unit of measurement (if applicable)</li> <li><code>requirement</code>: Validation requirements for the column</li> <li><code>is_primary</code>: Indicates whether this is the primary column for its data type</li> </ul>"},{"location":"api/core/data-types/#tablestructure","title":"TableStructure","text":"<p>Defines the structure of a data table, including its columns and timestamp handling.</p> <pre><code>@dataclass\nclass TableStructure:\n    name: str\n    timestamp_column: str\n    columns: List[ColumnMapping]\n</code></pre> <p>Validation Methods:</p> <ul> <li><code>validate_columns()</code>: Ensures at least one column is defined</li> <li><code>validate_unique_source_names()</code>: Checks for duplicate column names</li> <li><code>validate_primary_columns()</code>: Ensures each data type has at most one primary column</li> </ul>"},{"location":"api/core/data-types/#fileconfig","title":"FileConfig","text":"<p>Specifies the configuration for a single file within a device format.</p> <pre><code>@dataclass\nclass FileConfig:\n    name_pattern: str\n    file_type: FileType\n    tables: List[TableStructure]\n</code></pre> <p>Validation Rules:</p> <ul> <li>Must have at least one table defined</li> <li>CSV files are limited to one table with an empty name</li> <li>File patterns must match supported file types</li> </ul>"},{"location":"api/core/data-types/#deviceformat","title":"DeviceFormat","text":"<p>Defines the complete format specification for a diabetes device data export.</p> <pre><code>@dataclass\nclass DeviceFormat:\n    name: str\n    files: List[FileConfig]\n</code></pre>"},{"location":"api/core/data-types/#usage-example","title":"Usage Example","text":"<p>Here's a complete example of defining an XDrip+ SQLite backup format:</p> Defining a format Example <pre><code>from src.core.data_types import (\n    ColumnMapping,\n    ColumnRequirement,\n    DataType,\n    DeviceFormat,\n    FileConfig,\n    FileType,\n    TableStructure,\n    Unit,\n)\n\nXDRIP_SQLITE_FORMAT = DeviceFormat(\n    name=\"xdrip_sqlite\",\n    files=[\n        FileConfig(\n            name_pattern=\"*.sqlite\",\n            file_type=FileType.SQLITE,\n            tables=[\n                # BgReadings table with CGM data\n                TableStructure(\n                    name=\"BgReadings\",\n                    timestamp_column=\"timestamp\",\n                    columns=[\n                        ColumnMapping(\n                            source_name=\"calculated_value\",\n                            data_type=DataType.CGM,\n                            unit=Unit.MGDL,\n                        ),\n                        ColumnMapping(\n                            source_name=\"raw_data\",\n                            data_type=DataType.CGM,\n                            is_primary=False,\n                        ),\n                    ],\n                ),\n                # Treatments table with insulin, carbs, and notes\n                TableStructure(\n                    name=\"Treatments\",\n                    timestamp_column=\"timestamp\",\n                    columns=[\n                        ColumnMapping(\n                            source_name=\"insulin\",\n                            data_type=DataType.INSULIN,\n                            unit=Unit.UNITS,\n                        ),\n                        ColumnMapping(\n                            source_name=\"carbs\",\n                            data_type=DataType.CARBS,\n                            unit=Unit.GRAMS,\n                        ),\n                        ColumnMapping(\n                            source_name=\"notes\",\n                            data_type=DataType.NOTES,\n                            requirement=ColumnRequirement.REQUIRED_NULLABLE,\n                        ),\n                    ],\n                ),\n            ],\n        )\n    ],\n)\n</code></pre>"},{"location":"api/core/data-types/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Column Requirements</p> <ul> <li>Use <code>REQUIRED_WITH_DATA</code> for essential numeric data</li> <li>Use <code>REQUIRED_NULLABLE</code> for optional metadata</li> <li>Use <code>CONFIRMATION_ONLY</code> for format validation columns</li> </ul> </li> <li> <p>Primary Columns</p> <ul> <li>Each data type should have exactly one primary column</li> <li>Secondary columns can provide additional context or raw data</li> <li>Set <code>is_primary=False</code> for supporting data columns</li> </ul> </li> <li> <p>Units</p> <ul> <li>Always specify units for numeric data</li> <li>Match units to the source data format</li> <li>Unit conversion happens during processing</li> </ul> </li> <li> <p>Validation</p> <ul> <li>All data structures perform validation on initialization</li> <li>Handle validation errors appropriately</li> <li>Use consistent naming patterns across formats</li> </ul> </li> </ol>"},{"location":"api/core/data-types/#error-handling","title":"Error Handling","text":"<p>Data type-related errors are typically raised as <code>FormatValidationError</code> with detailed error messages and context:</p> Error Handling Example <pre><code>try:\n    device_format = DeviceFormat(...)\nexcept FormatValidationError as e:\n    print(f\"Validation failed: {str(e)}\")\n    print(f\"Details: {e.details}\")\n</code></pre>"},{"location":"api/core/exceptions/","title":"Exceptions API Reference","text":"<p>The CGM Data Processor uses a hierarchical exception system to provide detailed error information. All exceptions include both an error message and optional details dictionary.</p>"},{"location":"api/core/exceptions/#exception-hierarchy","title":"Exception Hierarchy","text":"<pre><code>graph LR\n    %% Main Error\n    A[CGMProcessorError]\n\n    %% File Errors\n    B[FileError]\n    B --&gt; B1[FileAccessError]\n    B --&gt; B2[FileExtensionError]\n    B --&gt; B3[FileParseError]\n    B --&gt; B4[DataExistsError]\n\n    %% Format Errors\n    C[FormatError]\n    C --&gt; C1[FormatDetectionError]\n    C --&gt; C2[FormatLoadingError]\n    C --&gt; C3[FormatValidationError]\n    C --&gt; C4[DeviceFormatError]\n\n    %% Processing Errors\n    D[ProcessingError]\n    D --&gt; D1[DataProcessingError]\n    D --&gt; D2[TimestampProcessingError]\n    D --&gt; D3[AlignmentError]\n    D --&gt; D4[DataQualityError]\n    D --&gt; D5[TimeAlignmentError]\n    D --&gt; D6[UnitConversionError]\n    D --&gt; D7[MetricCalculationError]\n    D --&gt; D8[CalibrationError]\n    D4 --&gt; D4A[DataGapError]\n\n    %% Validation Errors\n    E[ValidationError]\n    E --&gt; E1[DataValidationError]\n\n    %% Main connections\n    A --&gt; B\n    A --&gt; C\n    A --&gt; D\n    A --&gt; E</code></pre>"},{"location":"api/core/exceptions/#base-exception","title":"Base Exception","text":"DefinitionUsage <pre><code>class CGMProcessorError(Exception):\n    \"\"\"Base exception for all CGM processor errors.\"\"\"\n\n    def __init__(self, message: str, details: Optional[Dict[str, Any]] = None):\n        super().__init__(message)\n        self.details = details or {}\n</code></pre> <pre><code>try:\n    raise CGMProcessorError(\n        \"Processing failed\",\n        details={\n            \"step\": \"validation\",\n            \"reason\": \"missing data\"\n        }\n    )\nexcept CGMProcessorError as e:\n    print(f\"Error: {str(e)}\")\n    print(f\"Details: {e.details}\")\n</code></pre>"},{"location":"api/core/exceptions/#file-related-exceptions","title":"File-Related Exceptions","text":"<p>All file-related exceptions inherit from <code>FileError</code></p> FileAccessErrorFileExtensionErrorFileParseErrorDataExistsError <p>Raised when there's an error accessing a file. <pre><code>try:\n    with open(path) as f:\n        data = f.read()\nexcept OSError as e:\n    raise FileAccessError(\n        f\"Cannot access file: {path}\",\n        details={\"error\": str(e)}\n    )\n</code></pre></p> <p>Raised when a file extension is not supported. <pre><code>if not path.suffix.lower() in ['.csv', '.sqlite', '.xml']:\n    raise FileExtensionError(\n        f\"Unsupported file type: {path.suffix}\",\n        details={\"supported_types\": ['.csv', '.sqlite', '.xml']}\n    )\n</code></pre></p> <p>Raised when there's an error parsing file contents. <pre><code>try:\n    df = pd.read_csv(path)\nexcept pd.errors.ParserError as e:\n    raise FileParseError(\n        \"Failed to parse CSV file\",\n        details={\"error\": str(e)}\n    )\n</code></pre></p> <p>Raised when the reader returns no data. <pre><code>if df.empty:\n    raise DataExistsError(\n        \"No data found in file\",\n        details={\"file\": str(path)}\n    )\n</code></pre></p>"},{"location":"api/core/exceptions/#format-related-exceptions","title":"Format-Related Exceptions","text":"<p>All format-related exceptions inherit from <code>FormatError</code></p> FormatDetectionErrorFormatLoadingErrorFormatValidationErrorDeviceFormatError <p>Raised when there's an error detecting file format. <pre><code>if not matching_formats:\n    raise FormatDetectionError(\n        \"No matching format found\",\n        details={\"attempted_formats\": attempted}\n    )\n</code></pre></p> <p>Raised when a format file can't be loaded. <pre><code>try:\n    module = importlib.import_module(format_path)\nexcept ImportError as e:\n    raise FormatLoadingError(\n        f\"Failed to load format: {format_name}\",\n        details={\"error\": str(e)}\n    )\n</code></pre></p> <p>Raised when there's an error validating format definition. <pre><code>if not format_def.files:\n    raise FormatValidationError(\n        \"Format must define at least one file\",\n        details={\"format_name\": format_def.name}\n    )\n</code></pre></p> <p>Raised for device-specific format issues. <pre><code>if not device_config.valid():\n    raise DeviceFormatError(\n        f\"Invalid device configuration: {device}\",\n        details={\"issues\": validation_errors}\n    )\n</code></pre></p>"},{"location":"api/core/exceptions/#processing-exceptions","title":"Processing Exceptions","text":"<p>Processing errors cover data handling, calculation, and quality issues</p> Data ProcessingData QualityCalculations <pre><code>class DataProcessingError(ProcessingError):\n    \"\"\"Raised when there's an error processing data.\"\"\"\n\nclass TimestampProcessingError(ProcessingError):\n    \"\"\"Raised when there are timestamp format issues.\"\"\"\n\nclass AlignmentError(ProcessingError):\n    \"\"\"Raised when there is an error aligning datasets.\"\"\"\n</code></pre> <pre><code>class DataQualityError(ProcessingError):\n    \"\"\"Raised when data quality checks fail.\"\"\"\n\nclass DataGapError(DataQualityError):\n    \"\"\"Raised when data gaps exceed thresholds.\"\"\"\n\nclass CalibrationError(ProcessingError):\n    \"\"\"Raised for sensor calibration issues.\"\"\"\n</code></pre> <pre><code>class UnitConversionError(ProcessingError):\n    \"\"\"Raised for unit conversion issues.\"\"\"\n\nclass MetricCalculationError(ProcessingError):\n    \"\"\"Raised when calculating diabetes metrics.\"\"\"\n\nclass TimeAlignmentError(ProcessingError):\n    \"\"\"Raised when aligning data streams.\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#validation-exceptions","title":"Validation Exceptions","text":"<pre><code>class ValidationError(CGMProcessorError):\n    \"\"\"Base class for validation errors.\"\"\"\n\nclass DataValidationError(ValidationError):\n    \"\"\"Raised when there's an error validating data.\"\"\"\n</code></pre>"},{"location":"api/core/exceptions/#example-usage","title":"Example Usage","text":"<pre><code>try:\n    # Attempt to process file\n    processor.process_file(file_path)\n\nexcept FileAccessError as e:\n    logger.error(\"Cannot access file: %s\", e.details.get('path'))\n    raise\n\nexcept FormatDetectionError as e:\n    logger.error(\n        \"Format detection failed. Attempted formats: %s\",\n        e.details.get('attempted_formats')\n    )\n    raise\n\nexcept DataQualityError as e:\n    logger.warning(\n        \"Data quality issues detected: %s\",\n        e.details.get('quality_metrics')\n    )\n    # Handle quality issues...\n\nexcept CGMProcessorError as e:\n    # Catch any other processor errors\n    logger.error(\"Processing error: %s Details: %s\", str(e), e.details)\n    raise\n</code></pre>"},{"location":"api/core/exceptions/#best-practices","title":"Best Practices","text":"<p>Exception Handling Tips</p> <ol> <li> <p>Use Specific Exceptions</p> <ul> <li>Catch the most specific exception possible</li> <li>Provide detailed error messages</li> <li>Include relevant details in the details dictionary</li> </ul> </li> <li> <p>Error Details</p> <ul> <li>Include file paths when relevant</li> <li>Add validation error specifics</li> <li>Include attempted operations</li> <li>Reference related data</li> </ul> </li> <li> <p>Exception Chaining</p> <ul> <li>Use <code>raise ... from e</code> to preserve stack traces</li> <li>Chain exceptions when converting from other types</li> <li>Maintain error context</li> </ul> </li> <li> <p>Error Recovery</p> <ul> <li>Handle expected errors at appropriate levels</li> <li>Log errors with appropriate severity</li> <li>Provide user-friendly error messages</li> </ul> </li> </ol>"},{"location":"api/core/format-registry/","title":"Format Registry API Reference","text":"<p>The Format Registry system provides dynamic loading and management of diabetes device data formats. It consists of two main components that work together to handle format management and detection.</p>"},{"location":"api/core/format-registry/#core-components","title":"Core Components","text":"FormatRegistryFormatDetector <p>The <code>FormatRegistry</code> manages the loading and access of device format definitions.</p> <pre><code>from src.file_parser.format_registry import FormatRegistry\n\nregistry = FormatRegistry()\nformats = registry.formats  # Get all available formats\n</code></pre> <p>The <code>FormatDetector</code> validates files against the registered formats.</p> <pre><code>from src.file_parser.format_detector import FormatDetector\n\ndetector = FormatDetector(registry)\nformat, error, results = detector.detect_format(Path(\"data.sqlite\"))\n</code></pre>"},{"location":"api/core/format-registry/#format-registry-api","title":"Format Registry API","text":""},{"location":"api/core/format-registry/#format-access-methods","title":"Format Access Methods","text":"<p>The registry provides several methods to access and filter formats:</p> <pre><code># Get all registered formats\nformats: List[DeviceFormat] = registry.formats\n\n# Get a specific format by name\nformat: Optional[DeviceFormat] = registry.get_format(\"xdrip_sqlite\")\n\n# Get formats by file type\nsqlite_formats = registry.get_formats_by_type(FileType.SQLITE)\n\n# Get formats containing specific data type\ncgm_formats = registry.get_formats_with_data_type(DataType.CGM)\n\n# Get formats that could handle a specific file\nfile_formats = registry.get_formats_for_file(Path(\"data.sqlite\"))\n\n# Get all available data types\ndata_types = registry.get_available_data_types()\n</code></pre>"},{"location":"api/core/format-registry/#format-loading-process","title":"Format Loading Process","text":"Dynamic Format Loading <p>The registry automatically loads formats from the devices directory:</p> <pre><code>def _load_formats(self) -&gt; None:\n    manufacturers_dir = Path(__file__).parent / \"devices\"\n    if not manufacturers_dir.exists():\n        raise FileAccessError(\n            \"Manufacturers directory not found\",\n            details={\"directory\": str(manufacturers_dir)},\n        )\n\n    # Recursively find all Python files\n    for format_file in manufacturers_dir.rglob(\"*.py\"):\n        if format_file.stem == \"__init__\":\n            continue\n\n        try:\n            self._load_format_file(format_file)\n        except FormatLoadingError as e:\n            logger.error(\n                \"Error loading format file: %s Details: %s\",\n                str(e),\n                e.details\n            )\n</code></pre>"},{"location":"api/core/format-registry/#format-detector-api","title":"Format Detector API","text":""},{"location":"api/core/format-registry/#file-type-validators","title":"File Type Validators","text":"SQLiteCSVJSONXML <pre><code>def _validate_sqlite(self, path: Path, config, val_result: ValidationResult) -&gt; bool:\n    \"\"\"Validate SQLite file structure.\"\"\"\n    try:\n        engine = create_engine(f\"sqlite:///{path}\")\n        inspector = inspect(engine)\n\n        # Get all tables (case sensitive)\n        actual_tables = {name: name for name in inspector.get_table_names()}\n\n        # Check each required table\n        for required_table in config.tables:\n            table_name = required_table.name\n            if table_name not in actual_tables:\n                val_result.missing_tables.append(required_table.name)\n                continue\n\n            # Check columns\n            columns = inspector.get_columns(table_name)\n            column_names = {col[\"name\"] for col in columns}\n\n            # Check required columns exist in file\n            required_columns = {\n                col.source_name\n                for col in required_table.columns\n                if col.requirement != ColumnRequirement.OPTIONAL\n            }\n            missing = required_columns - column_names\n            if missing:\n                val_result.missing_columns[required_table.name] = [\n                    col.source_name\n                    for col in required_table.columns\n                    if col.requirement != ColumnRequirement.OPTIONAL\n                    and col.source_name in missing\n                ]\n\n        return not val_result.has_errors()\n\n    except FormatValidationError as e:\n        logger.debug(\"SQLite validation error: %s\", str(e))\n        return False\n</code></pre> <pre><code>def _validate_csv(self, path: Path, config, val_result: ValidationResult) -&gt; bool:\n    \"\"\"Validate CSV file structure.\"\"\"\n    try:\n        # Read CSV headers only\n        df = pd.read_csv(path, nrows=0)\n        columns = {col.lower() for col in df.columns}\n\n        # CSV should have exactly one table\n        csv_table = config.tables[0]\n\n        # Check required columns\n        required_columns = {\n            col.source_name.lower() for col in csv_table.columns if col.required\n        }\n        missing = required_columns - columns\n        if missing:\n            val_result.missing_columns[\"\"] = [\n                col\n                for col in csv_table.columns\n                if col.required and col.source_name.lower() in missing\n            ]\n\n        return not val_result.has_errors()\n\n    except FormatValidationError as e:\n        logger.debug(\"CSV validation error: %s\", str(e))\n        return False\n</code></pre> <pre><code>def _validate_json(self, path: Path, config, val_result: ValidationResult) -&gt; bool:\n    \"\"\"Validate JSON file structure.\"\"\"\n    try:\n        with open(path, encoding=\"utf-8\") as f:\n            data = json.load(f)\n\n        for json_table in config.tables:\n            if isinstance(data, list):\n                if not data:\n                    val_result.missing_tables.append(json_table.name)\n                    continue\n                record = data[0]\n            else:\n                if json_table.name not in data:\n                    val_result.missing_tables.append(json_table.name)\n                    continue\n                record = (\n                    data[json_table.name][0]\n                    if isinstance(data[json_table.name], list)\n                    else data[json_table.name]\n                )\n\n            # Check required fields\n            fields = {k.lower() for k in record.keys()}\n            required_fields = {\n                col.source_name.lower()\n                for col in json_table.columns\n                if col.required\n            }\n            missing = required_fields - fields\n            if missing:\n                val_result.missing_columns[json_table.name] = [\n                    col\n                    for col in json_table.columns\n                    if col.required and col.source_name.lower() in missing\n                ]\n\n        return not val_result.has_errors()\n\n    except FormatValidationError as e:\n        logger.debug(\"JSON validation error: %s\", str(e))\n        return False\n</code></pre> <pre><code>def _validate_xml(self, path: Path, config, val_result: ValidationResult) -&gt; bool:\n    \"\"\"Validate XML file structure.\"\"\"\n    try:\n        tree = ET.parse(path)\n        root = tree.getroot()\n\n        for xml_table in config.tables:\n            elements = root.findall(f\".//{xml_table.name}\")\n            if not elements:\n                val_result.missing_tables.append(xml_table.name)\n                continue\n\n            # Check first element\n            element = elements[0]\n            fields = set()\n            fields.update(element.attrib.keys())\n            fields.update(child.tag for child in element)\n            fields = {f.lower() for f in fields}\n\n            required_fields = {\n                col.source_name.lower() \n                for col in xml_table.columns \n                if col.required\n            }\n            missing = required_fields - fields\n            if missing:\n                val_result.missing_columns[xml_table.name] = [\n                    col\n                    for col in xml_table.columns\n                    if col.required and col.source_name.lower() in missing\n                ]\n\n        return not val_result.has_errors()\n\n    except FormatValidationError as e:\n        logger.debug(\"XML validation error: %s\", str(e))\n        return False\n</code></pre>"},{"location":"api/core/format-registry/#validation-results","title":"Validation Results","text":"<p>ValidationResult Class</p> <pre><code>class ValidationResult:\n    \"\"\"Container for structure validation results.\"\"\"\n\n    def __init__(self):\n        self.missing_tables: List[str] = []\n        self.missing_columns: Dict[str, List[str]] = {}  # table: [columns]\n\n    def has_errors(self) -&gt; bool:\n        \"\"\"Check if any validation errors exist.\"\"\"\n        return bool(self.missing_tables or self.missing_columns)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Format validation errors as string.\"\"\"\n        errors = []\n        if self.missing_tables:\n            errors.append(f\"Missing tables: {', '.join(self.missing_tables)}\")\n        if self.missing_columns:\n            for table, columns in self.missing_columns.items():\n                errors.append(\n                    f\"Missing required columns in {table}: {', '.join(columns)}\"\n                )\n        return \"\\n\".join(errors)\n</code></pre>"},{"location":"api/core/format-registry/#usage-examples","title":"Usage Examples","text":""},{"location":"api/core/format-registry/#basic-format-discovery","title":"Basic Format Discovery","text":"Format Detection Example <pre><code>from pathlib import Path\nfrom src.file_parser.format_registry import FormatRegistry\nfrom src.file_parser.format_detector import FormatDetector\n\n# Initialize components\nregistry = FormatRegistry()\ndetector = FormatDetector(registry)\n\n# Detect format of a file\nfile_path = Path(\"data.sqlite\")\nformat, error, results = detector.detect_format(file_path)\n\nif format:\n    print(f\"Detected format: {format.name}\")\n    # Access format details\n    for file_config in format.files:\n        for table in file_config.tables:\n            print(f\"Table: {table.name}\")\nelse:\n    print(f\"Detection failed: {error}\")\n    # Print validation results\n    for format_name, result in results.items():\n        if result.has_errors():\n            print(f\"\\nValidation failures for {format_name}:\")\n            print(str(result))\n</code></pre>"},{"location":"api/core/format-registry/#error-handling","title":"Error Handling","text":"<pre><code>graph TD\n    A[CGMProcessorError] --&gt; B[FormatError]\n    A --&gt; C[FileError]\n    B --&gt; D[FormatDetectionError]\n    B --&gt; E[FormatLoadingError]\n    B --&gt; F[FormatValidationError]\n    C --&gt; G[FileAccessError]\n    C --&gt; H[FileExtensionError]</code></pre> Error Handling Example <pre><code>try:\n    format, error, results = detector.detect_format(file_path)\n    if not format:\n        for format_name, result in results.items():\n            if result.has_errors():\n                print(f\"Validation failures for {format_name}:\")\n                print(str(result))\nexcept FormatError as e:\n    print(f\"Format error: {str(e)}\")\n    print(f\"Details: {e.details}\")\n</code></pre>"},{"location":"api/core/format-registry/#best-practices","title":"Best Practices","text":"<p>Format Registry Usage</p> <ol> <li> <p>Format Registration</p> <ul> <li>Place format definitions in the appropriate device directory</li> <li>Use clear, unique format names</li> <li>Include comprehensive table and column definitions</li> </ul> </li> <li> <p>Format Detection</p> <ul> <li>Always handle both the format and error return values</li> <li>Check validation results for debugging format issues</li> <li>Use debug logging for detailed format detection information</li> </ul> </li> <li> <p>Error Handling</p> <ul> <li>Catch specific exceptions for different failure modes</li> <li>Always check validation results when format detection fails</li> <li>Log detailed error information in debug mode</li> </ul> </li> <li> <p>Performance</p> <ul> <li>Format registry loads formats once at initialization</li> <li>Cache format detection results when processing multiple files</li> <li>Use appropriate file type validators for different formats</li> </ul> </li> </ol>"},{"location":"api/file-parsing/","title":"File Parsing Overview","text":"<p>The File Parsing system provides a flexible and extensible framework for reading and processing diabetes device data files. It consists of three main components that work together to handle various file formats and data structures.</p>"},{"location":"api/file-parsing/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph LR\n    subgraph Detection\n        A[Format Registry] --&gt; B[Format Detector]\n    end\n\n    subgraph Readers\n        C[Base Reader]\n        C --&gt; D[SQLite Reader]\n        C --&gt; E[CSV Reader]\n        C --&gt; F[XML Reader]\n    end\n\n    B --&gt; C\n\n    style A fill:#517,stroke:#333\n    style B fill:#517,stroke:#333\n    style C fill:#417,stroke:#333</code></pre>"},{"location":"api/file-parsing/#core-components","title":"Core Components","text":"Format RegistryFormat DetectorReader System <p>Manages device format definitions and provides format discovery: <pre><code>registry = FormatRegistry()\nformats = registry.formats\nsqlite_formats = registry.get_formats_by_type(FileType.SQLITE)\n</code></pre></p> <p>Validates files against registered formats: <pre><code>detector = FormatDetector(registry)\nformat, error, results = detector.detect_format(file_path)\n</code></pre></p> <p>Provides file type-specific data reading capabilities: <pre><code>reader = BaseReader.get_reader_for_format(detected_format, file_path)\nwith reader:\n    table_data = reader.read_all_tables()\n</code></pre></p>"},{"location":"api/file-parsing/#data-flow","title":"Data Flow","text":"<ol> <li> <p>Format Detection</p> <ul> <li>Registry provides format definitions</li> <li>Detector validates file structure</li> <li>Returns matched format or error details</li> </ul> </li> <li> <p>Reader Selection</p> <ul> <li>Automatic reader selection based on file type</li> <li>Resource management via context manager</li> <li>Type-specific data reading</li> </ul> </li> <li> <p>Data Processing</p> <ul> <li>Table structure validation</li> <li>Timestamp processing</li> <li>Data validation and cleanup</li> </ul> </li> </ol>"},{"location":"api/file-parsing/#supported-file-types","title":"Supported File Types","text":"<p>File Type Support</p> <p>The system currently supports:</p> <ul> <li>SQLite: Database files with multiple tables</li> <li>CSV: Single table per file</li> <li>XML: Multiple tables via XPath queries</li> </ul>"},{"location":"api/file-parsing/#common-features","title":"Common Features","text":""},{"location":"api/file-parsing/#1-timestamp-handling","title":"1. Timestamp Handling","text":"<p>All readers provide consistent timestamp processing:</p> <pre><code>def _convert_timestamp_to_utc(\n    self, df: pd.DataFrame, timestamp_column: str\n) -&gt; Tuple[pd.DataFrame, TimestampType]:\n    \"\"\"Convert timestamp column to UTC datetime and set as index.\"\"\"\n</code></pre>"},{"location":"api/file-parsing/#2-data-validation","title":"2. Data Validation","text":"<p>Consistent validation across readers:</p> <pre><code>def _validate_required_data(\n    self, df: pd.DataFrame, columns: List[ColumnMapping]\n) -&gt; List[str]:\n    \"\"\"Check for missing data in required columns.\"\"\"\n</code></pre>"},{"location":"api/file-parsing/#3-resource-management","title":"3. Resource Management","text":"<p>All readers implement proper cleanup:</p> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    return self\n\ndef __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Cleanup resources if needed.\"\"\"\n    self._cleanup()\n</code></pre>"},{"location":"api/file-parsing/#usage-example","title":"Usage Example","text":"<pre><code># Initialize components\nregistry = FormatRegistry()\ndetector = FormatDetector(registry)\n\n# Detect format\nformat, error, results = detector.detect_format(file_path)\nif format:\n    # Get appropriate reader\n    reader = BaseReader.get_reader_for_format(format, file_path)\n\n    # Process data\n    with reader:\n        table_data = reader.read_all_tables()\n\n        # Access processed data\n        for name, data in table_data.items():\n            print(f\"Table: {name}\")\n            print(f\"Shape: {data.dataframe.shape}\")\n</code></pre>"},{"location":"api/file-parsing/#error-handling","title":"Error Handling","text":"Format ErrorsReader Errors <pre><code>try:\n    format, error, results = detector.detect_format(file_path)\n    if not format:\n        print(f\"Detection failed: {error}\")\n        for format_name, result in results.items():\n            if result.has_errors():\n                print(f\"Validation failures for {format_name}:\")\n                print(str(result))\nexcept FormatError as e:\n    print(f\"Format error: {e}\")\n</code></pre> <pre><code>try:\n    with reader:\n        table_data = reader.read_all_tables()\nexcept (DataProcessingError, ReaderError) as e:\n    print(f\"Reading error: {e}\")\n</code></pre>"},{"location":"api/file-parsing/#best-practices","title":"Best Practices","text":"<p>Implementation Guidelines</p> <ol> <li> <p>Format Definition</p> <ul> <li>Define clear table structures</li> <li>Specify required columns</li> <li>Document timestamp formats</li> </ul> </li> <li> <p>Reader Implementation</p> <ul> <li>Handle resource cleanup</li> <li>Validate data integrity</li> <li>Process timestamps consistently</li> </ul> </li> <li> <p>Error Handling</p> <ul> <li>Use specific error types</li> <li>Provide detailed error messages</li> <li>Clean up resources on error</li> </ul> </li> </ol>"},{"location":"api/file-parsing/#next-steps","title":"Next Steps","text":"<p>Explore detailed documentation for each component:</p> <ul> <li>Format Detection - Format validation and detection</li> <li>Readers - File type-specific reader implementations</li> <li>Format Registry - Format management system</li> </ul>"},{"location":"api/file-parsing/format-detection/","title":"Format Detection API Reference","text":"<p>The Format Detection system validates diabetes device data files against registered formats, providing detailed feedback about structure and content validation.</p>"},{"location":"api/file-parsing/format-detection/#core-components","title":"Core Components","text":"FormatDetectorValidationResult <pre><code>class FormatDetector:\n    \"\"\"Detects device formats by examining file structure.\"\"\"\n\n    def __init__(self, format_registry: FormatRegistry):\n        \"\"\"Initialize detector with format registry.\"\"\"\n        self._registry = format_registry\n</code></pre> <pre><code>class ValidationResult:\n    \"\"\"Container for structure validation results.\"\"\"\n\n    def __init__(self):\n        self.missing_tables: List[str] = []\n        self.missing_columns: Dict[str, List[str]] = {}\n</code></pre>"},{"location":"api/file-parsing/format-detection/#detection-process","title":"Detection Process","text":"<pre><code>graph TD\n    A[Input File] --&gt; B[Get Potential Formats]\n    B --&gt; C[Validate Each Format]\n    C --&gt; D[Return Best Match]\n\n    C --&gt; E[Validate Tables]\n    C --&gt; F[Validate Columns]\n    C --&gt; G[Check Requirements]</code></pre>"},{"location":"api/file-parsing/format-detection/#format-detection-method","title":"Format Detection Method","text":"Method SignatureExample Usage <pre><code>def detect_format(\n    self, path: Path\n) -&gt; Tuple[Optional[DeviceFormat], Optional[str], Dict[str, ValidationResult]]:\n    \"\"\"Detect format of provided file.\n\n    Returns:\n        Tuple containing:\n            - Matched format (or None)\n            - Error message (or None)\n            - Dictionary of validation results per format tried\n    \"\"\"\n</code></pre> <pre><code>detector = FormatDetector(registry)\nformat, error, results = detector.detect_format(Path(\"data.sqlite\"))\n\nif format:\n    print(f\"Detected format: {format.name}\")\nelse:\n    print(f\"Detection failed: {error}\")\n    for format_name, result in results.items():\n        if result.has_errors():\n            print(f\"Validation failures for {format_name}:\")\n            print(str(result))\n</code></pre>"},{"location":"api/file-parsing/format-detection/#file-type-validators","title":"File Type Validators","text":"<p>The detector includes specialized validators for each supported file type:</p> SQLiteCSVXML <pre><code>def _validate_sqlite(self, path: Path, config, val_result: ValidationResult) -&gt; bool:\n    \"\"\"Validate SQLite file structure.\"\"\"\n    try:\n        engine = create_engine(f\"sqlite:///{path}\")\n        inspector = inspect(engine)\n        actual_tables = {name: name for name in inspector.get_table_names()}\n\n        # Check each required table\n        for required_table in config.tables:\n            if required_table.name not in actual_tables:\n                val_result.missing_tables.append(required_table.name)\n                continue\n\n            # Check columns\n            columns = inspector.get_columns(required_table.name)\n            column_names = {col[\"name\"] for col in columns}\n            self._validate_columns(required_table, column_names, val_result)\n\n        return not val_result.has_errors()\n    except Exception as e:\n        logger.debug(\"SQLite validation error: %s\", str(e))\n        return False\n</code></pre> <pre><code>def _validate_csv(self, path: Path, config, val_result: ValidationResult) -&gt; bool:\n    \"\"\"Validate CSV file structure.\"\"\"\n    try:\n        df = pd.read_csv(path, nrows=0)\n        columns = {col.lower() for col in df.columns}\n\n        # CSV should have exactly one table\n        csv_table = config.tables[0]\n        required_columns = {\n            col.source_name.lower() \n            for col in csv_table.columns \n            if col.required\n        }\n\n        missing = required_columns - columns\n        if missing:\n            val_result.missing_columns[\"\"] = list(missing)\n\n        return not val_result.has_errors()\n    except Exception as e:\n        logger.debug(\"CSV validation error: %s\", str(e))\n        return False\n</code></pre> <pre><code>def _validate_xml(self, path: Path, config, val_result: ValidationResult) -&gt; bool:\n    \"\"\"Validate XML file structure.\"\"\"\n    try:\n        tree = ET.parse(path)\n        root = tree.getroot()\n\n        for xml_table in config.tables:\n            elements = root.findall(f\".//{xml_table.name}\")\n            if not elements:\n                val_result.missing_tables.append(xml_table.name)\n                continue\n\n            # Check first element structure\n            element = elements[0]\n            fields = set()\n            fields.update(element.attrib.keys())\n            fields.update(child.tag for child in element)\n            fields = {f.lower() for f in fields}\n\n            self._validate_columns(xml_table, fields, val_result)\n\n        return not val_result.has_errors()\n    except Exception as e:\n        logger.debug(\"XML validation error: %s\", str(e))\n        return False\n</code></pre>"},{"location":"api/file-parsing/format-detection/#validation-process","title":"Validation Process","text":""},{"location":"api/file-parsing/format-detection/#structure-validation","title":"Structure Validation","text":"<p>Validation Steps</p> <ol> <li>File existence and accessibility</li> <li>File type match (extension)</li> <li>Required tables presence</li> <li>Required columns presence</li> <li>Column data requirements</li> </ol>"},{"location":"api/file-parsing/format-detection/#validation-results","title":"Validation Results","text":"<p>The <code>ValidationResult</code> class tracks validation issues:</p> <pre><code>class ValidationResult:\n    def has_errors(self) -&gt; bool:\n        \"\"\"Check if any validation errors exist.\"\"\"\n        return bool(self.missing_tables or self.missing_columns)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Format validation errors as string.\"\"\"\n        errors = []\n        if self.missing_tables:\n            errors.append(f\"Missing tables: {', '.join(self.missing_tables)}\")\n        if self.missing_columns:\n            for table, columns in self.missing_columns.items():\n                errors.append(\n                    f\"Missing required columns in {table}: {', '.join(columns)}\"\n                )\n        return \"\\n\".join(errors)\n</code></pre>"},{"location":"api/file-parsing/format-detection/#usage-examples","title":"Usage Examples","text":""},{"location":"api/file-parsing/format-detection/#basic-format-detection","title":"Basic Format Detection","text":"<pre><code>from pathlib import Path\nfrom src.file_parser.format_registry import FormatRegistry\nfrom src.file_parser.format_detector import FormatDetector\n\n# Initialize components\nregistry = FormatRegistry()\ndetector = FormatDetector(registry)\n\n# Detect format\nfile_path = Path(\"data.sqlite\")\nformat, error, results = detector.detect_format(file_path)\n\nif format:\n    print(f\"Detected format: {format.name}\")\n    print(\"Available data types:\")\n    for file_config in format.files:\n        for table in file_config.tables:\n            print(f\"  Table: {table.name}\")\n            for col in table.columns:\n                if col.data_type:\n                    print(f\"    Column: {col.source_name} ({col.data_type.name})\")\n</code></pre>"},{"location":"api/file-parsing/format-detection/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    format, error, results = detector.detect_format(file_path)\n    if not format:\n        print(f\"No matching format: {error}\")\n        # Print validation results\n        for format_name, result in results.items():\n            if result.has_errors():\n                print(f\"\\nValidation failures for {format_name}:\")\n                if result.missing_tables:\n                    print(\"Missing tables:\", \", \".join(result.missing_tables))\n                for table, cols in result.missing_columns.items():\n                    print(f\"Missing columns in {table}:\", \", \".join(cols))\nexcept FormatError as e:\n    print(f\"Format error: {str(e)}\")\n    print(f\"Details: {e.details}\")\n</code></pre>"},{"location":"api/file-parsing/format-detection/#best-practices","title":"Best Practices","text":"<p>Format Detection Tips</p> <ol> <li> <p>Format Definition</p> <ul> <li>Define clear validation requirements</li> <li>Specify required vs optional columns</li> <li>Document expected data structures</li> </ul> </li> <li> <p>Validation Strategy</p> <ul> <li>Check structure before content</li> <li>Provide detailed validation feedback</li> <li>Handle edge cases gracefully</li> </ul> </li> <li> <p>Error Handling</p> <ul> <li>Use specific error types</li> <li>Include context in error messages</li> <li>Log validation failures appropriately</li> </ul> </li> </ol>"},{"location":"api/file-parsing/format-detection/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Missing Tables</p> <ul> <li>Database tables not found</li> <li>XML elements missing</li> <li>Wrong file structure</li> </ul> </li> <li> <p>Column Requirements</p> <ul> <li>Required columns missing</li> <li>Wrong column names</li> <li>Case sensitivity issues</li> </ul> </li> <li> <p>Data Access</p> <ul> <li>File permissions</li> <li>Corrupted files</li> <li>Invalid file formats</li> </ul> </li> </ol>"},{"location":"api/file-parsing/readers/","title":"Readers API Reference","text":"<p>The Readers system provides a flexible framework for reading diabetes device data from various file formats. It uses an abstract base class with concrete implementations for each supported file type.</p>"},{"location":"api/file-parsing/readers/#architecture","title":"Architecture","text":"<pre><code>graph TD\n    A[BaseReader] --&gt; B[SQLiteReader]\n    A --&gt; C[CSVReader]\n    A --&gt; D[XMLReader]\n\n    E[TableData] --- A\n    F[DeviceFormat] --- A\n    G[FileConfig] --- A</code></pre>"},{"location":"api/file-parsing/readers/#core-components","title":"Core Components","text":""},{"location":"api/file-parsing/readers/#reader-base-class","title":"Reader Base Class","text":"Base DefinitionRegistration System <pre><code>class BaseReader(ABC):\n    \"\"\"Abstract base class for all file format readers.\"\"\"\n    _readers: Dict[FileType, Type[\"BaseReader\"]] = {}\n\n    @abstractmethod\n    def read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n        \"\"\"Read and process a single table according to its structure.\"\"\"\n</code></pre> <pre><code>@classmethod\ndef register(cls, file_type: FileType):\n    \"\"\"Register a reader class for a specific file type.\"\"\"\n    def wrapper(reader_cls):\n        cls._readers[file_type] = reader_cls\n        return reader_cls\n    return wrapper\n</code></pre>"},{"location":"api/file-parsing/readers/#table-data-container","title":"Table Data Container","text":"<pre><code>@dataclass\nclass TableData:\n    \"\"\"Holds processed data for a single table.\"\"\"\n    name: str\n    dataframe: pd.DataFrame\n    missing_required_columns: List[str]\n    timestamp_type: Optional[TimestampType] = None\n</code></pre>"},{"location":"api/file-parsing/readers/#reader-implementations","title":"Reader Implementations","text":""},{"location":"api/file-parsing/readers/#sqlite-reader","title":"SQLite Reader","text":"Class DefinitionTable Reading <pre><code>@BaseReader.register(FileType.SQLITE)\nclass SQLiteReader(BaseReader):\n    \"\"\"Reads and processes SQLite files.\"\"\"\n    def __init__(self, path: Path, file_config: FileConfig):\n        super().__init__(path, file_config)\n        self._engine = None\n\n    @property\n    def engine(self):\n        \"\"\"Lazy initialization of database engine.\"\"\"\n        if self._engine is None:\n            self._engine = create_engine(f\"sqlite:///{self.file_path}\")\n        return self._engine\n</code></pre> <pre><code>def read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n    \"\"\"Read a single SQLite table.\"\"\"\n    try:\n        # Validate identifiers\n        if not self._validate_identifier(table_structure.name):\n            raise DataValidationError(f\"Invalid table name: {table_structure.name}\")\n\n        # Read needed columns\n        columns_to_read = [\n            col.source_name\n            for col in table_structure.columns\n            if col.requirement != ColumnRequirement.CONFIRMATION_ONLY\n        ]\n        columns_to_read.append(table_structure.timestamp_column)\n\n        # Create and execute query\n        quoted_columns = [f'\"{col}\"' for col in columns_to_read]\n        query = text(f\"\"\"\n            SELECT {', '.join(quoted_columns)}\n            FROM \"{table_structure.name}\"\n            ORDER BY \"{table_structure.timestamp_column}\"\n        \"\"\")\n\n        with self.engine.connect() as conn:\n            df = pd.read_sql_query(query, conn)\n\n        # Process timestamps and validate\n        df, fmt = self._convert_timestamp_to_utc(df, table_structure.timestamp_column)\n        missing_required = self._validate_required_data(df, table_structure.columns)\n\n        return TableData(\n            name=table_structure.name,\n            dataframe=df,\n            missing_required_columns=missing_required,\n            timestamp_type=fmt,\n        )\n    except Exception as e:\n        logger.error(\"Error reading SQLite table: %s\", str(e))\n        return None\n</code></pre>"},{"location":"api/file-parsing/readers/#csv-reader","title":"CSV Reader","text":"Class DefinitionTable Reading <pre><code>@BaseReader.register(FileType.CSV)\nclass CSVReader(BaseReader):\n    \"\"\"Reads and processes CSV files.\"\"\"\n    def __init__(self, path: Path, file_config: FileConfig):\n        super().__init__(path, file_config)\n        self._data = None\n</code></pre> <pre><code>def read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n    \"\"\"Read CSV file as a single table.\"\"\"\n    try:\n        # Read data if not cached\n        if self._data is None:\n            self._data = pd.read_csv(\n                self.file_path,\n                encoding=\"utf-8\",\n                low_memory=False,\n            )\n\n        # Get needed columns\n        columns_to_read = [\n            col.source_name\n            for col in table_structure.columns\n            if col.requirement != ColumnRequirement.CONFIRMATION_ONLY\n        ]\n        columns_to_read.append(table_structure.timestamp_column)\n\n        # Validate and process\n        df = self._data[columns_to_read].copy()\n        df, fmt = self._convert_timestamp_to_utc(\n            df, table_structure.timestamp_column\n        )\n        missing_required = self._validate_required_data(\n            df, table_structure.columns\n        )\n\n        return TableData(\n            name=table_structure.name,\n            dataframe=df,\n            missing_required_columns=missing_required,\n            timestamp_type=fmt,\n        )\n    except Exception as e:\n        logger.error(\"Error reading CSV: %s\", str(e))\n        return None\n</code></pre>"},{"location":"api/file-parsing/readers/#xml-reader","title":"XML Reader","text":"Class DefinitionTable Reading <pre><code>@BaseReader.register(FileType.XML)\nclass XMLReader(BaseReader):\n    \"\"\"Reads and processes XML files.\"\"\"\n    def __init__(self, path: Path, file_config: FileConfig):\n        super().__init__(path, file_config)\n        self._tree = None\n        self._root = None\n</code></pre> <pre><code>def read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n    \"\"\"Read XML elements as a table.\"\"\"\n    try:\n        self._init_xml()\n\n        # Find table elements\n        table_elements = self._root.findall(f\".//{table_structure.name}\")\n\n        # Extract data\n        data = {col: [] for col in columns_to_read}\n        for element in table_elements:\n            for column in columns_to_read:\n                value = self._extract_value(element, column)\n                data[column].append(value)\n\n        # Convert and process\n        df = pd.DataFrame(data)\n        df, fmt = self._convert_timestamp_to_utc(\n            df, table_structure.timestamp_column\n        )\n        missing_required = self._validate_required_data(\n            df, table_structure.columns\n        )\n\n        return TableData(\n            name=table_structure.name,\n            dataframe=df,\n            missing_required_columns=missing_required,\n            timestamp_type=fmt,\n        )\n    except Exception as e:\n        logger.error(\"Error reading XML: %s\", str(e))\n        return None\n</code></pre>"},{"location":"api/file-parsing/readers/#common-functionality","title":"Common Functionality","text":""},{"location":"api/file-parsing/readers/#timestamp-processing","title":"Timestamp Processing","text":"<p>Timestamp Handling</p> <pre><code>def _convert_timestamp_to_utc(\n    self, df: pd.DataFrame, timestamp_column: str\n) -&gt; Tuple[pd.DataFrame, TimestampType]:\n    \"\"\"Convert timestamp column to UTC datetime and set as index.\"\"\"\n    fmt = self.detect_timestamp_format(df[timestamp_column])\n\n    if fmt == TimestampType.UNIX_SECONDS:\n        df[timestamp_column] = pd.to_datetime(\n            df[timestamp_column], unit=\"s\", utc=True\n        )\n    elif fmt == TimestampType.UNIX_MILLISECONDS:\n        df[timestamp_column] = pd.to_datetime(\n            df[timestamp_column], unit=\"ms\", utc=True\n        )\n    # ... handle other formats\n\n    return df.set_index(timestamp_column).sort_index(), fmt\n</code></pre>"},{"location":"api/file-parsing/readers/#data-validation","title":"Data Validation","text":"<pre><code>def _validate_required_data(\n    self, df: pd.DataFrame, columns: List[ColumnMapping]\n) -&gt; List[str]:\n    \"\"\"Check for missing data in required columns.\"\"\"\n    missing_required = []\n    for col in columns:\n        if (\n            col.requirement == ColumnRequirement.REQUIRED_WITH_DATA\n            and col.source_name in df.columns\n            and df[col.source_name].isna().all()\n        ):\n            missing_required.append(col.source_name)\n    return missing_required\n</code></pre>"},{"location":"api/file-parsing/readers/#usage-examples","title":"Usage Examples","text":""},{"location":"api/file-parsing/readers/#basic-reading","title":"Basic Reading","text":"<pre><code># Get appropriate reader\nreader = BaseReader.get_reader_for_format(detected_format, file_path)\n\n# Read all tables\nwith reader:\n    table_data = reader.read_all_tables()\n\n    # Process each table\n    for name, data in table_data.items():\n        print(f\"Table: {name}\")\n        print(f\"Shape: {data.dataframe.shape}\")\n        if data.missing_required_columns:\n            print(\"Missing columns:\", data.missing_required_columns)\n</code></pre>"},{"location":"api/file-parsing/readers/#custom-processing","title":"Custom Processing","text":"<pre><code># Read specific table\nwith reader:\n    bgreadings = reader.read_table(TableStructure(\n        name=\"BgReadings\",\n        timestamp_column=\"timestamp\",\n        columns=[\n            ColumnMapping(\n                source_name=\"value\",\n                data_type=DataType.CGM,\n                unit=Unit.MGDL\n            )\n        ]\n    ))\n\n    if bgreadings:\n        df = bgreadings.dataframe\n        print(f\"Time range: {df.index.min()} to {df.index.max()}\")\n</code></pre>"},{"location":"api/file-parsing/readers/#best-practices","title":"Best Practices","text":"<p>Implementation Guidelines</p> <ol> <li> <p>Resource Management</p> <ul> <li>Use context managers properly</li> <li>Clean up resources in <code>_cleanup</code></li> <li>Handle file access carefully</li> </ul> </li> <li> <p>Data Processing</p> <ul> <li>Validate data early</li> <li>Handle missing values consistently</li> <li>Process timestamps correctly</li> </ul> </li> <li> <p>Error Handling</p> <ul> <li>Use specific error types</li> <li>Log errors appropriately</li> <li>Clean up on errors</li> </ul> </li> <li> <p>Performance</p> <ul> <li>Cache data when appropriate</li> <li>Use efficient pandas operations</li> <li>Handle large files carefully</li> </ul> </li> </ol>"},{"location":"api/processing/","title":"Data Processing Overview","text":"<p>The Data Processing system uses a modular, type-based architecture to process different kinds of diabetes data. It provides automatic processor selection, data validation, and standardized output formats.</p>"},{"location":"api/processing/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TD\n    I[Table Data]\n    J[Column Mappings]</code></pre> <p><pre><code>graph TD\n    A[DataProcessor] --&gt; B[BaseTypeProcessor]\n    B --&gt; C[CGMProcessor]\n    B --&gt; D[InsulinProcessor]\n    B --&gt; E[CarbsProcessor]\n    B --&gt; F[NotesProcessor]\n\n    G[ColumnData] --- B\n    H[ProcessedTypeData] --- B</code></pre> <pre><code>graph TD\n    K[Processed Data]\n    L[Processing Notes]\n    M[Source Units]</code></pre></p>"},{"location":"api/processing/#core-components","title":"Core Components","text":"Data StructuresProcessor BaseMain Processor <pre><code>@dataclass\nclass ColumnData:\n    \"\"\"Holds data and metadata for a single column.\"\"\"\n    dataframe: pd.DataFrame  # Single column DataFrame\n    unit: Unit\n    config: ColumnMapping\n    is_primary: bool\n\n@dataclass\nclass ProcessedTypeData:\n    \"\"\"Holds processed data for a single data type.\"\"\"\n    dataframe: pd.DataFrame\n    source_units: Dict[str, Unit]  # Maps column name to its original unit\n    processing_notes: List[str]\n</code></pre> <pre><code>class BaseTypeProcessor(ABC):\n    \"\"\"Abstract base class for type-specific processors.\"\"\"\n    @abstractmethod\n    def process_type(self, columns: List[ColumnData]) -&gt; ProcessedTypeData:\n        \"\"\"Process all data of a specific type.\"\"\"\n</code></pre> <pre><code>class DataProcessor:\n    \"\"\"Main processor class that handles all data types.\"\"\"\n    _type_processors: Dict[DataType, Type[BaseTypeProcessor]] = {}\n\n    @classmethod\n    def register_processor(cls, data_type: DataType):\n        \"\"\"Register a processor for a specific data type.\"\"\"\n</code></pre>"},{"location":"api/processing/#type-specific-processors","title":"Type-Specific Processors","text":"<p>Available Processors</p> <p>The system includes processors for:</p> <ol> <li>CGM Data</li> <li>Continuous glucose readings</li> <li>Gap interpolation</li> <li> <p>Unit conversion</p> </li> <li> <p>Insulin Data</p> </li> <li>Dose classification (basal/bolus)</li> <li>Metadata processing</li> <li> <p>Dose validation</p> </li> <li> <p>Carbohydrate Data</p> </li> <li>Intake records</li> <li>Value validation</li> <li> <p>Duplicate handling</p> </li> <li> <p>Notes Data</p> </li> <li>Text processing</li> <li>String cleaning</li> <li>Empty value handling</li> </ol>"},{"location":"api/processing/#processing-pipeline","title":"Processing Pipeline","text":""},{"location":"api/processing/#1-data-organization","title":"1. Data Organization","text":"<pre><code># Group columns by data type\nfor table_name, data in table_data.items():\n    config = table_configs[table_name]\n\n    for column in config.columns:\n        if column.data_type:\n            target_type = (\n                DataType.INSULIN\n                if column.data_type == DataType.INSULIN_META\n                else column.data_type\n            )\n\n            df_subset = data.dataframe[[column.source_name]].copy()\n            column_data = ColumnData(\n                dataframe=df_subset,\n                unit=column.unit,\n                config=column,\n                is_primary=column.is_primary,\n            )\n</code></pre>"},{"location":"api/processing/#2-processor-selection","title":"2. Processor Selection","text":"<pre><code># Get appropriate processor for each type\nprocessor = DataProcessor()\nfor data_type, columns in type_data.items():\n    type_processor = processor.get_processor_for_type(data_type)\n    result = type_processor.process_type(columns)\n</code></pre>"},{"location":"api/processing/#3-data-processing","title":"3. Data Processing","text":"<p>Each processor handles its specific data type:</p> CGM ProcessingInsulin ProcessingCarbs ProcessingNotes Processing <ul> <li>Time alignment (5-min intervals)</li> <li>Gap interpolation</li> <li>Unit conversion (mg/dL \u2194 mmol/L)</li> </ul> <ul> <li>Dose classification</li> <li>Meta information extraction</li> <li>Value validation</li> </ul> <ul> <li>Value validation (\u22651g)</li> <li>Duplicate removal</li> <li>Empty value handling</li> </ul> <ul> <li>Text cleaning</li> <li>Empty string handling</li> <li>NA value management</li> </ul>"},{"location":"api/processing/#usage-examples","title":"Usage Examples","text":""},{"location":"api/processing/#basic-processing","title":"Basic Processing","text":"<pre><code>from src.processors import DataProcessor\nfrom src.core.data_types import DataType\n\n# Initialize processor\nprocessor = DataProcessor()\n\n# Process all tables\nresults = processor.process_tables(table_data, table_configs)\n\n# Access processed data by type\ncgm_data = results.get(DataType.CGM)\ninsulin_data = results.get(DataType.INSULIN)\ncarbs_data = results.get(DataType.CARBS)\nnotes_data = results.get(DataType.NOTES)\n\n# Check processing results\nfor data_type, data in results.items():\n    print(f\"\\n{data_type.name} Processing Results:\")\n    print(f\"Records: {len(data.dataframe)}\")\n    print(\"Notes:\", \"\\n  \".join(data.processing_notes))\n</code></pre>"},{"location":"api/processing/#custom-processor-registration","title":"Custom Processor Registration","text":"<pre><code>@DataProcessor.register_processor(DataType.CGM)\nclass CustomCGMProcessor(BaseTypeProcessor):\n    def process_type(self, columns: List[ColumnData]) -&gt; ProcessedTypeData:\n        # Custom processing logic\n        processed_df = self._custom_processing(columns)\n\n        return ProcessedTypeData(\n            dataframe=processed_df,\n            source_units={\"custom_col\": Unit.MGDL},\n            processing_notes=[\"Custom processing completed\"]\n        )\n</code></pre>"},{"location":"api/processing/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    results = processor.process_tables(table_data, table_configs)\nexcept ProcessingError as e:\n    print(f\"Processing error: {str(e)}\")\n    # Handle error...\n</code></pre> <p>Common errors: - Missing primary columns - Invalid unit types - Data validation failures - Processing errors</p>"},{"location":"api/processing/#best-practices","title":"Best Practices","text":"<p>Implementation Guidelines</p> <ol> <li> <p>Type Processing</p> <ul> <li>Validate input data</li> <li>Handle missing values</li> <li>Document processing steps</li> <li>Track data modifications</li> </ul> </li> <li> <p>Unit Handling</p> <ul> <li>Validate units before processing</li> <li>Convert to standard units</li> <li>Track unit information</li> <li>Document conversions</li> </ul> </li> <li> <p>Performance</p> <ul> <li>Use vectorized operations</li> <li>Handle large datasets</li> <li>Clean up resources</li> <li>Monitor memory usage</li> </ul> </li> <li> <p>Error Handling</p> <ul> <li>Use specific exceptions</li> <li>Provide detailed messages</li> <li>Log processing steps</li> <li>Clean up on errors</li> </ul> </li> </ol>"},{"location":"api/processing/#integration-with-other-components","title":"Integration with Other Components","text":"<ol> <li>Format Detection</li> <li>Provides validated table structures</li> <li>Confirms data availability</li> <li> <p>Validates column requirements</p> </li> <li> <p>Data Alignment</p> </li> <li>Uses processed results</li> <li>Aligns multiple data types</li> <li> <p>Handles time synchronization</p> </li> <li> <p>Result Usage</p> </li> <li>Analysis and visualization</li> <li>Data export</li> <li>Quality metrics</li> <li>Report generation</li> </ol>"},{"location":"api/processing/base-processor/","title":"Base Processor API Reference","text":"<p>The Base Processor system provides the foundation for processing different types of diabetes data. It uses a registry pattern with abstract base classes to enable type-specific processing while maintaining consistent interfaces and behavior.</p>"},{"location":"api/processing/base-processor/#core-data-structures","title":"Core Data Structures","text":"ColumnDataProcessedTypeData <pre><code>@dataclass\nclass ColumnData:\n    \"\"\"Holds data and metadata for a single column.\"\"\"\n    dataframe: pd.DataFrame  # Single column DataFrame\n    unit: Unit\n    config: ColumnMapping\n    is_primary: bool\n\n    @property\n    def data_type(self) -&gt; DataType:\n        \"\"\"Get the data type from the column config.\"\"\"\n        return self.config.data_type\n</code></pre> <pre><code>@dataclass\nclass ProcessedTypeData:\n    \"\"\"Holds processed data for a single data type.\"\"\"\n    dataframe: pd.DataFrame\n    source_units: Dict[str, Unit]  # Maps column name to its original unit\n    processing_notes: List[str]\n</code></pre>"},{"location":"api/processing/base-processor/#base-type-processor","title":"Base Type Processor","text":"<p>Abstract Base Class</p> <p><code>BaseTypeProcessor</code> defines the interface and common functionality for all type-specific processors.</p> <pre><code>class BaseTypeProcessor(ABC):\n    \"\"\"Abstract base class for individual data type processors.\"\"\"\n\n    @abstractmethod\n    def process_type(self, columns: List[ColumnData]) -&gt; ProcessedTypeData:\n        \"\"\"Process all data of a specific type.\"\"\"\n</code></pre>"},{"location":"api/processing/base-processor/#protected-methods","title":"Protected Methods","text":"Column Name GenerationUnit ValidationColumn Combination <pre><code>def _generate_column_name(\n    self, data_type: DataType, is_primary: bool, index: int\n) -&gt; str:\n    \"\"\"Generate standardized column names.\"\"\"\n    base_name = data_type.name.lower()\n    if is_primary:\n        return f\"{base_name}_primary\"\n    return f\"{base_name}_{index + 1}\"\n</code></pre> <pre><code>def _validate_units(self, data_type: DataType, source_unit: Unit) -&gt; None:\n    \"\"\"Validate that units are compatible with data type.\"\"\"\n    valid_units = {\n        DataType.CGM: [Unit.MGDL, Unit.MMOL],\n        DataType.BGM: [Unit.MGDL, Unit.MMOL],\n        DataType.INSULIN: [Unit.UNITS],\n        DataType.CARBS: [Unit.GRAMS],\n    }\n\n    if data_type not in valid_units:\n        raise ProcessingError(f\"No unit validation defined for {data_type.value}\")\n\n    if source_unit not in valid_units[data_type]:\n        raise ProcessingError(\n            f\"Invalid source unit {source_unit.value} for {data_type.value}\"\n        )\n</code></pre> <pre><code>def _combine_and_rename_columns(\n    self, columns: List[ColumnData], data_type: DataType\n) -&gt; Tuple[pd.DataFrame, Dict[str, Unit]]:\n    \"\"\"Combine multiple columns into a single DataFrame with standardized names.\"\"\"\n    try:\n        # Sort columns to ensure primary comes first\n        sorted_columns = sorted(columns, key=lambda x: (not x.is_primary))\n\n        combined_df = pd.DataFrame(index=pd.DatetimeIndex([]))\n        column_units = {}\n\n        # Process each column\n        for idx, col_data in enumerate(sorted_columns):\n            new_name = self._generate_column_name(\n                data_type, col_data.is_primary, idx\n            )\n\n            # Merge with existing data\n            temp_df = col_data.dataframe.copy()\n            temp_df.columns = [new_name]\n\n            if combined_df.empty:\n                combined_df = temp_df\n            else:\n                combined_df = combined_df.join(temp_df, how=\"outer\")\n\n            column_units[new_name] = col_data.unit\n\n        return combined_df, column_units\n\n    except Exception as e:\n        raise ProcessingError(f\"Failed to combine columns: {str(e)}\") from e\n</code></pre>"},{"location":"api/processing/base-processor/#main-data-processor","title":"Main Data Processor","text":"<p>Central Processing Class</p> <p><code>DataProcessor</code> manages type-specific processors and handles data routing.</p> <pre><code>class DataProcessor:\n    \"\"\"Main processor class that handles processing of all data types.\"\"\"\n    _type_processors: Dict[DataType, Type[BaseTypeProcessor]] = {}\n</code></pre>"},{"location":"api/processing/base-processor/#key-methods","title":"Key Methods","text":"Process TablesProcessor RegistrationProcessor Selection <pre><code>def process_tables(\n    self,\n    table_data: Dict[str, TableData],\n    table_configs: Dict[str, TableStructure]\n) -&gt; Dict[DataType, ProcessedTypeData]:\n    \"\"\"Process all tables according to their configuration.\"\"\"\n    # Organize data by type\n    type_data: Dict[DataType, List[ColumnData]] = {}\n\n    for table_name, data in table_data.items():\n        config = table_configs[table_name]\n\n        # Group columns by data type\n        for column in config.columns:\n            if column.data_type:\n                # Include insulin meta data with insulin data\n                target_type = (\n                    DataType.INSULIN\n                    if column.data_type == DataType.INSULIN_META\n                    else column.data_type\n                )\n\n                df_subset = data.dataframe[[column.source_name]].copy()\n                df_subset.columns = [\"value\"]\n\n                column_data = ColumnData(\n                    dataframe=df_subset,\n                    unit=column.unit,\n                    config=column,\n                    is_primary=column.is_primary,\n                )\n\n                if target_type not in type_data:\n                    type_data[target_type] = []\n\n                type_data[target_type].append(column_data)\n</code></pre> <pre><code>@classmethod\ndef register_processor(cls, data_type: DataType):\n    \"\"\"Register a processor class for a specific data type.\"\"\"\n    def wrapper(processor_cls: Type[BaseTypeProcessor]):\n        cls._type_processors[data_type] = processor_cls\n        return processor_cls\n    return wrapper\n</code></pre> <pre><code>def get_processor_for_type(self, data_type: DataType) -&gt; BaseTypeProcessor:\n    \"\"\"Get appropriate processor instance for the data type.\"\"\"\n    processor_cls = self._type_processors.get(data_type)\n    if processor_cls is None:\n        raise ProcessingError(\n            f\"No processor registered for data type: {data_type.value}\"\n        )\n    return processor_cls()\n</code></pre>"},{"location":"api/processing/base-processor/#usage-examples","title":"Usage Examples","text":""},{"location":"api/processing/base-processor/#creating-a-type-processor","title":"Creating a Type Processor","text":"<pre><code>@DataProcessor.register_processor(DataType.CGM)\nclass CGMProcessor(BaseTypeProcessor):\n    def process_type(self, columns: List[ColumnData]) -&gt; ProcessedTypeData:\n        processing_notes = []\n        try:\n            # Validate input\n            if not any(col.is_primary for col in columns):\n                raise ProcessingError(\"No primary CGM column found\")\n\n            # Process data...\n            combined_df, column_units = self._combine_and_rename_columns(\n                columns, DataType.CGM\n            )\n\n            return ProcessedTypeData(\n                dataframe=combined_df,\n                source_units=column_units,\n                processing_notes=processing_notes\n            )\n        except Exception as e:\n            raise ProcessingError(f\"Error processing CGM data: {str(e)}\") from e\n</code></pre>"},{"location":"api/processing/base-processor/#processing-data","title":"Processing Data","text":"<pre><code># Initialize processor\nprocessor = DataProcessor()\n\n# Process tables\nresults = processor.process_tables(table_data, table_configs)\n\n# Access processed data by type\ncgm_data = results.get(DataType.CGM)\ninsulin_data = results.get(DataType.INSULIN)\n</code></pre>"},{"location":"api/processing/base-processor/#best-practices","title":"Best Practices","text":"<p>Implementation Guidelines</p> <ol> <li> <p>Type Processor Implementation</p> <ul> <li>Always validate input data</li> <li>Handle primary/secondary column relationships</li> <li>Maintain data type consistency</li> <li>Document processing steps in notes</li> </ul> </li> <li> <p>Unit Handling</p> <ul> <li>Validate units before processing</li> <li>Convert to standard units if needed</li> <li>Track unit information in results</li> </ul> </li> <li> <p>Error Handling</p> <ul> <li>Use specific error types</li> <li>Include context in error messages</li> <li>Handle cleanup in error cases</li> </ul> </li> <li> <p>Performance</p> <ul> <li>Process columns efficiently</li> <li>Handle large datasets appropriately</li> <li>Use vectorized operations where possible</li> </ul> </li> </ol>"},{"location":"api/processing/carbs/","title":"Carbs Processor API Reference","text":"<p>The Carbs Processor handles carbohydrate intake data, providing validation and cleaning of carbohydrate entries from various sources.</p>"},{"location":"api/processing/carbs/#processor-overview","title":"Processor Overview","text":"<pre><code>@DataProcessor.register_processor(DataType.CARBS)\nclass CarbsProcessor(BaseTypeProcessor):\n    \"\"\"Processes carbohydrate intake data with validation and cleaning.\"\"\"\n</code></pre>"},{"location":"api/processing/carbs/#processing-pipeline","title":"Processing Pipeline","text":"<pre><code>graph TD\n    A[Input Validation] --&gt; B[Column Combination]\n    B --&gt; C[Filter Small Values]\n    C --&gt; D[Handle Duplicates]\n    D --&gt; E[Drop Empty Rows]</code></pre>"},{"location":"api/processing/carbs/#main-processing-method","title":"Main Processing Method","text":"Method SignatureExample Usage <pre><code>def process_type(\n    self,\n    columns: List[ColumnData],\n) -&gt; ProcessedTypeData:\n    \"\"\"Process carbohydrate data from various sources.\n\n    Args:\n        columns: List of ColumnData containing carb data columns\n\n    Returns:\n        ProcessedTypeData containing combined and cleaned carb data\n    \"\"\"\n</code></pre> <pre><code>processor = CarbsProcessor()\nresult = processor.process_type(carb_columns)\n\n# Access processed data\ncarbs_df = result.dataframe\nunits = result.source_units\nnotes = result.processing_notes\n</code></pre>"},{"location":"api/processing/carbs/#processing-steps","title":"Processing Steps","text":""},{"location":"api/processing/carbs/#1-input-validation","title":"1. Input Validation","text":"<p>Initial Checks</p> <pre><code># Validate primary column exists\nif not any(col.is_primary for col in columns):\n    raise ProcessingError(\"No primary carbohydrate column found\")\n\n# Sort columns (primary first)\nsorted_columns = sorted(columns, key=lambda x: (not x.is_primary))\n</code></pre>"},{"location":"api/processing/carbs/#2-column-combination","title":"2. Column Combination","text":"<pre><code># Combine all columns with standardized names\ncombined_df, column_units = self._combine_and_rename_columns(\n    sorted_columns, DataType.CARBS\n)\n\nif combined_df.empty:\n    raise ProcessingError(\"No carbohydrate data to process\")\n\n# Log column information\nprocessing_notes.append(\n    f\"Processing {len(combined_df.columns)} carb columns: \"\n    f\"{', '.join(combined_df.columns)}\"\n)\n</code></pre>"},{"location":"api/processing/carbs/#3-data-validation","title":"3. Data Validation","text":"<p>Value Filtering</p> <pre><code># Track original row count\noriginal_count = len(combined_df)\n\n# Process each carb column\nfor col in combined_df.columns:\n    # Keep only rows where carbs is &gt;= 1.0 grams\n    mask = combined_df[col] &gt;= 1.0\n    combined_df.loc[~mask, col] = None\n\n    filtered_count = mask.sum()\n    processing_notes.append(\n        f\"Column {col}: Kept {filtered_count} entries \u22651g \"\n        f\"({filtered_count / original_count * 100:.1f}%)\"\n    )\n</code></pre>"},{"location":"api/processing/carbs/#4-cleanup","title":"4. Cleanup","text":"Drop Empty RowsHandle Duplicates <pre><code># Drop rows where all values are null\ncombined_df = combined_df.dropna(how=\"all\")\n</code></pre> <pre><code># Remove duplicate timestamps\nduplicates = combined_df.index.duplicated()\nif duplicates.any():\n    dup_count = duplicates.sum()\n    processing_notes.append(f\"Removed {dup_count} duplicate timestamps\")\n    combined_df = combined_df[~duplicates]\n</code></pre>"},{"location":"api/processing/carbs/#output-format","title":"Output Format","text":"<p>The processor produces a <code>ProcessedTypeData</code> with:</p> DataFrame StructureUnits DictionaryProcessing Notes <ul> <li>Index: <ul> <li>DatetimeIndex</li> </ul> </li> <li>Columns:<ul> <li><code>carbs_primary</code>: Primary carb entries</li> <li><code>carbs_2</code>, <code>carbs_3</code>, etc.: Additional columns if present</li> </ul> </li> <li>All values are in grams</li> </ul> <pre><code>{\n    'carbs_primary': Unit.GRAMS,\n    'carbs_2': Unit.GRAMS,\n    # ... additional columns\n}\n</code></pre> <pre><code>[\n    \"Processing 2 carb columns: carbs_primary, carbs_2\",\n    \"Column carbs_primary: Kept 45 entries \u22651g (90.0%)\",\n    \"Column carbs_2: Kept 12 entries \u22651g (85.7%)\",\n    \"Removed 2 duplicate timestamps\",\n    \"Final dataset contains 55 entries from 60 original records\"\n]\n</code></pre>"},{"location":"api/processing/carbs/#validation-rules","title":"Validation Rules","text":"<p>Data Quality Rules</p> <ol> <li> <p>Minimum Value Rule</p> <ul> <li>Entries must be \u2265 1.0 grams</li> <li>Smaller values are set to None</li> <li>Prevents noise from tiny values</li> </ul> </li> <li> <p>Duplicate Handling</p> <ul> <li>Duplicate timestamps are removed</li> <li>First occurrence is kept</li> <li>Duplicates are logged</li> </ul> </li> <li> <p>Empty Data</p> <ul> <li>Rows with all null values are dropped</li> <li>Empty DataFrames raise ProcessingError</li> </ul> </li> </ol>"},{"location":"api/processing/carbs/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    processor = CarbsProcessor()\n    result = processor.process_type(columns)\nexcept ProcessingError as e:\n    logger.error(\"Carbs processing failed: %s\", str(e))\n    # Handle error...\n</code></pre> <p>Common errors: - No primary column - Empty data after filtering - Unit validation failures</p>"},{"location":"api/processing/carbs/#best-practices","title":"Best Practices","text":"<p>Usage Guidelines</p> <ol> <li> <p>Data Preparation</p> <ul> <li>Ensure values are in grams</li> <li>Validate timestamp index</li> <li>Check primary column designation</li> </ul> </li> <li> <p>Value Filtering</p> <ul> <li>Consider clinical significance</li> <li>Monitor filtering ratios</li> <li>Document unusual patterns</li> </ul> </li> <li> <p>Performance</p> <ul> <li>Use vectorized operations</li> <li>Handle large datasets efficiently</li> <li>Monitor memory usage</li> </ul> </li> </ol>"},{"location":"api/processing/carbs/#integration-example","title":"Integration Example","text":"<pre><code># Process carbohydrate data\nprocessor = CarbsProcessor()\nresult = processor.process_type(carb_columns)\n\n# Access processed data\ncarbs_df = result.dataframe\n\n# Calculate statistics\ntotal_carbs = carbs_df['carbs_primary'].sum()\nmean_carbs = carbs_df['carbs_primary'].mean()\nentries_per_day = carbs_df.groupby(carbs_df.index.date).size()\n\nprint(f\"Total Carbohydrates: {total_carbs}g\")\nprint(f\"Average per Entry: {mean_carbs:.1f}g\")\nprint(f\"Entries per Day: {entries_per_day.mean():.1f}\")\n\n# Check data quality\nvalid_entries = carbs_df['carbs_primary'].notna()\nquality_ratio = valid_entries.sum() / len(carbs_df)\nprint(f\"Data Quality: {quality_ratio * 100:.1f}% valid entries\")\n</code></pre>"},{"location":"api/processing/carbs/#comparison-with-other-processors","title":"Comparison with Other Processors","text":"<p>Processor Characteristics</p> <ul> <li>Simpler than CGM/Insulin processors</li> <li>No unit conversion needed (always grams)</li> <li>Binary validation (\u22651g or not)</li> <li>Minimal metadata requirements</li> </ul>"},{"location":"api/processing/cgm/","title":"CGM Processor API Reference","text":"<p>The CGM Processor handles Continuous Glucose Monitoring data, providing cleaning, interpolation, and unit conversion functionality.</p>"},{"location":"api/processing/cgm/#processor-overview","title":"Processor Overview","text":"<pre><code>@DataProcessor.register_processor(DataType.CGM)\nclass CGMProcessor(BaseTypeProcessor):\n    \"\"\"Processes CGM data with validation and cleaning.\"\"\"\n</code></pre>"},{"location":"api/processing/cgm/#processing-pipeline","title":"Processing Pipeline","text":"<p>The processor performs these operations in sequence:</p> <pre><code>graph TD\n    A[Input Validation] --&gt; B[Unit Conversion]\n    B --&gt; C[Data Combination]\n    C --&gt; D[Timestamp Alignment]\n    D --&gt; E[Gap Handling]\n    E --&gt; F[Value Validation]\n    F --&gt; G[Unit Generation]\n    G --&gt; H[Missing Data Flags]</code></pre>"},{"location":"api/processing/cgm/#main-processing-method","title":"Main Processing Method","text":"Method SignatureExample Usage <pre><code>def process_type(\n    self,\n    columns: List[ColumnData],\n    interpolation_limit: int = 4,\n) -&gt; ProcessedTypeData:\n    \"\"\"Process CGM data from various sources.\n\n    Args:\n        columns: List of ColumnData containing CGM data columns\n        interpolation_limit: Maximum missing values to interpolate\n                           (default: 4 = 20 minutes at 5-min intervals)\n\n    Returns:\n        ProcessedTypeData with combined and cleaned CGM data\n    \"\"\"\n</code></pre> <pre><code>processor = CGMProcessor()\nresult = processor.process_type(cgm_columns, interpolation_limit=6)\n\n# Access processed data\ncgm_df = result.dataframe\nunits = result.source_units\nnotes = result.processing_notes\n</code></pre>"},{"location":"api/processing/cgm/#processing-steps","title":"Processing Steps","text":""},{"location":"api/processing/cgm/#1-data-validation","title":"1. Data Validation","text":"<p>Initial Checks</p> <pre><code># Validate primary column exists\nif not any(col.is_primary for col in columns):\n    raise ProcessingError(\"No primary CGM column found\")\n\n# Sort columns (primary first)\nsorted_columns = sorted(columns, key=lambda x: (not x.is_primary))\n</code></pre>"},{"location":"api/processing/cgm/#2-unit-conversion","title":"2. Unit Conversion","text":"<p>Standardization to mg/dL</p> <pre><code>if col_data.unit == Unit.MMOL:\n    df[\"value\"] = df[\"value\"] * 18.0182\n    processing_notes.append(\n        f\"Converted CGM column {idx + 1} from {Unit.MMOL.value} to {Unit.MGDL.value}\"\n    )\n</code></pre>"},{"location":"api/processing/cgm/#3-time-alignment","title":"3. Time Alignment","text":"<pre><code># Round timestamps to 5-minute intervals\ncombined_df.index = combined_df.index.round(\"5min\")\n\n# Handle duplicate times by averaging\ncombined_df = combined_df.groupby(level=0).mean()\n\n# Create complete timeline\nfull_index = pd.date_range(\n    start=combined_df.index.min(),\n    end=combined_df.index.max(),\n    freq=\"5min\"\n)\n</code></pre>"},{"location":"api/processing/cgm/#4-gap-handling","title":"4. Gap Handling","text":"<p>Gap Detection and Interpolation</p> <pre><code># Create groups of consecutive missing values\ngap_groups = (~combined_df[col].isna()).cumsum()\n\n# Count size of each gap\ngap_size = combined_df[combined_df[col].isna()].groupby(gap_groups).size()\n\n# Find large gaps (&gt; interpolation_limit)\nlarge_gaps = gap_size[gap_size &gt; interpolation_limit].index\n\n# Interpolate within limit\ncombined_df[col] = combined_df[col].interpolate(\n    method=\"linear\",\n    limit=interpolation_limit,\n    limit_direction=\"forward\",\n)\n\n# Reset large gaps back to NaN\nfor gap_group in large_gaps:\n    mask = (gap_groups == gap_group) &amp; combined_df[col].isna()\n    combined_df.loc[mask, col] = np.nan\n</code></pre>"},{"location":"api/processing/cgm/#5-value-validation","title":"5. Value Validation","text":"<pre><code># Clip values to valid range (mg/dL)\ncombined_df[col] = combined_df[col].clip(lower=39.64, upper=360.36)\n</code></pre>"},{"location":"api/processing/cgm/#6-unit-generation","title":"6. Unit Generation","text":"<pre><code># Create mmol/L versions of each column\nfor col in combined_df.columns.copy():\n    mmol_col = f\"{col}_mmol\"\n    combined_df[mmol_col] = combined_df[col] * 0.0555\n    column_units[mmol_col] = Unit.MMOL\n    column_units[col] = Unit.MGDL\n</code></pre>"},{"location":"api/processing/cgm/#output-format","title":"Output Format","text":"<p>The processor produces a <code>ProcessedTypeData</code> with:</p> DataFrame StructureUnits DictionaryProcessing Notes <ul> <li>Index: DatetimeIndex (5-minute intervals)</li> <li>Columns:<ul> <li><code>cgm_primary</code>: Primary readings (mg/dL)</li> <li><code>cgm_primary_mmol</code>: Primary readings (mmol/L)</li> <li><code>cgm_2</code>, <code>cgm_3</code>, etc.: Secondary readings if available</li> <li><code>missing</code>: Boolean flag for missing/interpolated values</li> </ul> </li> </ul> <pre><code>{\n    'cgm_primary': Unit.MGDL,\n    'cgm_primary_mmol': Unit.MMOL,\n    'cgm_2': Unit.MGDL,\n    'cgm_2_mmol': Unit.MMOL,\n    # ... additional columns\n}\n</code></pre> <pre><code>[\n    \"Converted CGM column 1 from mmol/L to mg/dL\",\n    \"Processed 1440 total CGM readings\",\n    \"Found 12 missing or interpolated values in primary data\"\n]\n</code></pre>"},{"location":"api/processing/cgm/#data-quality-features","title":"Data Quality Features","text":"<ol> <li> <p>Interpolation Control</p> <ul> <li>Maximum gap interpolation: 4 readings (20 minutes)</li> <li>Larger gaps preserved as missing data</li> <li>Missing data tracking via flags</li> </ul> </li> <li> <p>Value Validation</p> <ul> <li>Valid range: 39.64 - 360.36 mg/dL</li> <li>Out-of-range values clipped</li> <li>Unit conversion precision maintained</li> </ul> </li> <li> <p>Time Alignment</p> <ul> <li>5-minute interval standardization</li> <li>Duplicate handling via averaging</li> <li>Complete timeline generation</li> </ul> </li> </ol>"},{"location":"api/processing/cgm/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    processor = CGMProcessor()\n    result = processor.process_type(columns)\nexcept ProcessingError as e:\n    logger.error(\"CGM processing failed: %s\", str(e))\n    # Handle error...\n</code></pre> <p>Common errors:</p> <ul> <li>No primary column</li> <li>Invalid unit types</li> <li>Data validation failures</li> <li>Interpolation issues</li> </ul>"},{"location":"api/processing/cgm/#best-practices","title":"Best Practices","text":"<p>Usage Guidelines</p> <ol> <li> <p>Data Preparation</p> <ul> <li>Ensure timestamp index</li> <li>Validate unit specifications</li> <li>Check primary/secondary relationships</li> </ul> </li> <li> <p>Interpolation Settings</p> <ul> <li>Default (4) works for most CGM data</li> <li>Adjust based on device characteristics</li> <li>Consider clinical requirements</li> </ul> </li> <li> <p>Performance</p> <ul> <li>Process in manageable time chunks</li> <li>Monitor memory usage with large datasets</li> <li>Use efficient pandas operations</li> </ul> </li> </ol>"},{"location":"api/processing/insulin/","title":"Insulin Processor API Reference","text":"<p>The Insulin Processor handles insulin dose data, providing dose classification (basal/bolus) through both dose-based rules and metadata analysis.</p>"},{"location":"api/processing/insulin/#processor-overview","title":"Processor Overview","text":"<pre><code>@DataProcessor.register_processor(DataType.INSULIN)\nclass InsulinProcessor(BaseTypeProcessor):\n    \"\"\"Processes insulin dose data with classification from meta data if available.\"\"\"\n</code></pre>"},{"location":"api/processing/insulin/#processing-pipeline","title":"Processing Pipeline","text":"<pre><code>graph TD\n    A[Input Columns] --&gt; B[Split Doses/Meta]\n    B --&gt; C[Validate Primary]\n    C --&gt; D[Process Doses]\n    D --&gt; E[Apply Dose Rules]\n    E --&gt; F[Process Metadata]\n    F --&gt; G[Update Classifications]</code></pre>"},{"location":"api/processing/insulin/#main-processing-method","title":"Main Processing Method","text":"Method SignatureExample Usage <pre><code>def process_type(\n    self,\n    columns: List[ColumnData],\n    bolus_limit: float = 8.0,\n    max_limit: float = 15.0,\n) -&gt; ProcessedTypeData:\n    \"\"\"Process insulin data and classify doses.\n\n    Args:\n        columns: List of ColumnData containing insulin data and metadata\n        bolus_limit: Maximum insulin units to classify as bolus\n        max_limit: Maximum valid insulin dose\n\n    Returns:\n        ProcessedTypeData with processed insulin data\n    \"\"\"\n</code></pre> <pre><code>processor = InsulinProcessor()\nresult = processor.process_type(\n    insulin_columns,\n    bolus_limit=10.0,  # Custom bolus limit\n    max_limit=20.0     # Custom maximum dose\n)\n</code></pre>"},{"location":"api/processing/insulin/#processing-steps","title":"Processing Steps","text":""},{"location":"api/processing/insulin/#1-column-separation","title":"1. Column Separation","text":"<p>Separating Doses from Metadata</p> <pre><code># Find insulin dose and meta columns\ndose_cols = [col for col in columns if col.data_type == DataType.INSULIN]\nmeta_cols = [\n    col for col in columns if col.data_type == DataType.INSULIN_META\n]\n\nif not any(col.is_primary for col in dose_cols):\n    raise ProcessingError(\"No primary insulin dose column found\")\n</code></pre>"},{"location":"api/processing/insulin/#2-dose-processing","title":"2. Dose Processing","text":"Initial ValidationDose Classification <pre><code># Keep only positive doses\nvalid_mask = df[\"value\"] &gt; 0.0\ndf = df[valid_mask]\n\nif len(df) &gt; 0:\n    processing_notes.append(f\"Found {len(df)} positive doses\")\n</code></pre> <pre><code># Initial classification based on dose\nresult_df[\"is_bolus\"] = df[\"value\"] &lt;= bolus_limit\nresult_df[\"is_basal\"] = (df[\"value\"] &gt; bolus_limit) &amp; (\n    df[\"value\"] &lt;= max_limit\n)\nresult_df[\"type\"] = \"\"  # Will be filled by metadata\n</code></pre>"},{"location":"api/processing/insulin/#3-metadata-processing","title":"3. Metadata Processing","text":"<p>Metadata Extraction</p> <pre><code>def _extract_meta_info(self, meta_value: str) -&gt; Tuple[bool, bool, Optional[str]]:\n    \"\"\"Extract insulin type information from meta JSON.\n\n    Returns:\n        Tuple of (is_bolus, is_basal, insulin_type)\n    \"\"\"\n    try:\n        meta_data = json.loads(meta_value)\n        if meta_data and isinstance(meta_data, list):\n            insulin = meta_data[0].get(\"insulin\", \"\").lower()\n            if \"novorapid\" in insulin:\n                return True, False, \"novorapid\"\n            if \"levemir\" in insulin:\n                return False, True, \"levemir\"\n    except (json.JSONDecodeError, IndexError, KeyError, AttributeError):\n        pass\n\n    return False, False, None\n</code></pre>"},{"location":"api/processing/insulin/#4-classification-update","title":"4. Classification Update","text":"<pre><code># Update classification with metadata if available\nif meta_cols and not result_df.empty:\n    meta_updates = 0\n    for col in meta_cols:\n        for idx, meta_value in col.dataframe[\"value\"].items():\n            if idx in result_df.index:\n                is_bolus, is_basal, insulin_type = self._extract_meta_info(\n                    meta_value\n                )\n                if insulin_type:\n                    result_df.loc[idx, \"is_bolus\"] = is_bolus\n                    result_df.loc[idx, \"is_basal\"] = is_basal\n                    result_df.loc[idx, \"type\"] = insulin_type\n                    meta_updates += 1\n</code></pre>"},{"location":"api/processing/insulin/#output-format","title":"Output Format","text":"<p>The processor produces a <code>ProcessedTypeData</code> with:</p> DataFrame StructureUnits DictionaryProcessing Notes <ul> <li>Index: <ul> <li>DatetimeIndex</li> </ul> </li> <li>Columns:<ul> <li><code>dose</code>: Insulin dose in units</li> <li><code>is_bolus</code>: Boolean flag for bolus doses</li> <li><code>is_basal</code>: Boolean flag for basal doses</li> <li><code>type</code>: String identifier for insulin type</li> </ul> </li> </ul> <pre><code>{\n    'dose': Unit.UNITS\n}\n</code></pre> <pre><code>[\n    \"Found 24 positive doses\",\n    \"Initial dose-based classification:\",\n    \"- 18 doses classified as bolus (\u22648.0U)\",\n    \"- 6 doses classified as basal (&gt;8.0U)\",\n    \"- Dropped 0 doses exceeding 15.0U\",\n    \"Updated 12 classifications using metadata\",\n    \"Final dataset contains 24 insulin records\"\n]\n</code></pre>"},{"location":"api/processing/insulin/#classification-rules","title":"Classification Rules","text":"<p>Dose Classification Logic</p> <ol> <li> <p>Initial Rule-Based Classification</p> <ul> <li>Bolus: dose \u2264 bolus_limit</li> <li>Basal: bolus_limit &lt; dose \u2264 max_limit</li> <li>Invalid: dose &gt; max_limit</li> </ul> </li> <li> <p>Metadata-Based Classification</p> <ul> <li>\"novorapid\" \u2192 Bolus</li> <li>\"levemir\" \u2192 Basal</li> <li>Metadata overrides rule-based classification</li> </ul> </li> <li> <p>Default Parameters</p> <ul> <li>bolus_limit = 8.0 units</li> <li>max_limit = 15.0 units</li> </ul> </li> </ol>"},{"location":"api/processing/insulin/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    processor = InsulinProcessor()\n    result = processor.process_type(columns)\nexcept ProcessingError as e:\n    logger.error(\"Insulin processing failed: %s\", str(e))\n    # Handle error...\n</code></pre> <p>Common errors:</p> <ul> <li>No primary dose column</li> <li>Invalid metadata format</li> <li>Dose validation failures</li> </ul>"},{"location":"api/processing/insulin/#best-practices","title":"Best Practices","text":"<p>Usage Guidelines</p> <ol> <li> <p>Data Preparation</p> <ul> <li>Ensure doses are in correct units</li> <li>Validate metadata JSON format</li> <li>Check timestamp alignment</li> </ul> </li> <li> <p>Classification Limits</p> <ul> <li>Adjust based on treatment protocol</li> <li>Consider patient-specific limits</li> <li>Document limit changes</li> </ul> </li> <li> <p>Metadata Handling</p> <ul> <li>Validate JSON structure</li> <li>Handle missing metadata gracefully</li> <li>Log metadata processing results</li> </ul> </li> <li> <p>Performance</p> <ul> <li>Parse metadata efficiently</li> <li>Use vectorized operations</li> <li>Minimize row-by-row operations</li> </ul> </li> </ol>"},{"location":"api/processing/insulin/#integration-example","title":"Integration Example","text":"<pre><code># Process insulin data with custom limits\nprocessor = InsulinProcessor()\nresult = processor.process_type(\n    columns=insulin_columns,\n    bolus_limit=10.0,  # Higher bolus threshold\n    max_limit=20.0     # Higher maximum dose\n)\n\n# Access processed data\ndoses_df = result.dataframe\nbolus_mask = doses_df[\"is_bolus\"]\nbasal_mask = doses_df[\"is_basal\"]\n\n# Calculate statistics\ntotal_bolus = doses_df.loc[bolus_mask, \"dose\"].sum()\ntotal_basal = doses_df.loc[basal_mask, \"dose\"].sum()\n\nprint(f\"Total Bolus: {total_bolus}U\")\nprint(f\"Total Basal: {total_basal}U\")\n</code></pre>"},{"location":"api/processing/notes/","title":"Notes Processor API Reference","text":"<p>The Notes Processor handles diabetes-related text notes and comments, providing text validation and cleaning while preserving temporal relationships.</p>"},{"location":"api/processing/notes/#processor-overview","title":"Processor Overview","text":"<pre><code>@DataProcessor.register_processor(DataType.NOTES)\nclass NotesProcessor(BaseTypeProcessor):\n    \"\"\"Processes text notes/comments data.\"\"\"\n</code></pre>"},{"location":"api/processing/notes/#processing-pipeline","title":"Processing Pipeline","text":"<pre><code>graph TD\n    A[Input Validation] --&gt; B[Text Cleaning]\n    B --&gt; C[Handle Missing Values]\n    C --&gt; D[Remove Empty Strings]\n    D --&gt; E[Drop Empty Rows]</code></pre>"},{"location":"api/processing/notes/#main-processing-method","title":"Main Processing Method","text":"Method SignatureExample Usage <pre><code>def process_type(\n    self,\n    columns: List[ColumnData],\n) -&gt; ProcessedTypeData:\n    \"\"\"Process notes data ensuring safe string storage.\n\n    Args:\n        columns: List of ColumnData containing notes columns\n\n    Returns:\n        ProcessedTypeData containing processed notes\n    \"\"\"\n</code></pre> <pre><code>processor = NotesProcessor()\nresult = processor.process_type(notes_columns)\n\n# Access processed data\nnotes_df = result.dataframe\nnotes = result.processing_notes\n</code></pre>"},{"location":"api/processing/notes/#processing-steps","title":"Processing Steps","text":""},{"location":"api/processing/notes/#1-input-validation","title":"1. Input Validation","text":"<p>Initial Checks</p> <pre><code># Validate primary column exists\nif not any(col.is_primary for col in columns):\n    raise ProcessingError(\"No primary notes column found\")\n\n# Sort columns (primary first)\nsorted_columns = sorted(columns, key=lambda x: (not x.is_primary))\n</code></pre>"},{"location":"api/processing/notes/#2-text-processing","title":"2. Text Processing","text":"<pre><code># Initialize result dataframe\nresult_df = pd.DataFrame(index=pd.DatetimeIndex([]))\n\n# Process each column\nfor idx, col_data in enumerate(sorted_columns):\n    # Generate column name\n    col_name = self._generate_column_name(\n        DataType.NOTES, col_data.is_primary, idx\n    )\n\n    # Get the notes series\n    notes_series = col_data.dataframe[\"value\"]\n\n    # Replace None with pd.NA\n    notes_series = notes_series.replace([None], pd.NA)\n\n    # Clean non-NA values\n    notes_series = notes_series.apply(\n        lambda x: x.strip() if pd.notna(x) else pd.NA\n    )\n\n    # Remove empty strings\n    notes_series = notes_series.replace({\"\": pd.NA})\n</code></pre>"},{"location":"api/processing/notes/#3-data-validation","title":"3. Data Validation","text":"<p>Value Handling</p> <pre><code># Add to result DataFrame only if we have any valid notes\nif not notes_series.isna().all():\n    result_df[col_name] = notes_series\n\n    # Track stats\n    valid_notes = notes_series.notna()\n    processing_notes.append(\n        f\"Column {col_name}: found {valid_notes.sum()} valid notes\"\n    )\n</code></pre>"},{"location":"api/processing/notes/#4-cleanup","title":"4. Cleanup","text":"<pre><code># Drop rows where all values are NA\nif not result_df.empty:\n    result_df = result_df.dropna(how=\"all\")\n\nprocessing_notes.append(\n    f\"Final dataset contains {len(result_df)} notes entries\"\n)\n</code></pre>"},{"location":"api/processing/notes/#output-format","title":"Output Format","text":"<p>The processor produces a <code>ProcessedTypeData</code> with:</p> DataFrame StructureUnits DictionaryProcessing Notes <ul> <li>Index:<ul> <li>DatetimeIndex</li> </ul> </li> <li>Columns:<ul> <li><code>notes_primary</code>: Primary notes entries</li> <li><code>notes_2</code>, <code>notes_3</code>, etc.: Additional columns if present</li> </ul> </li> <li>All values are strings or pd.NA</li> </ul> <pre><code>{\n    # Empty dictionary - no units for text data\n}\n</code></pre> <pre><code>[\n    \"Column notes_primary: found 45 valid notes\",\n    \"Column notes_2: found 12 valid notes\",\n    \"Final dataset contains 55 notes entries\"\n]\n</code></pre>"},{"location":"api/processing/notes/#text-processing-rules","title":"Text Processing Rules","text":"<p>Text Handling Rules</p> <ol> <li> <p>NULL Handling</p> <ul> <li>None values converted to pd.NA</li> <li>Empty strings converted to pd.NA</li> <li>Consistent NA representation</li> </ul> </li> <li> <p>String Cleaning</p> <ul> <li>Leading/trailing whitespace removed</li> <li>Empty strings after stripping become pd.NA</li> <li>Original text content preserved</li> </ul> </li> <li> <p>Row Filtering</p> <ul> <li>Rows with all NA values are dropped</li> <li>At least one valid note required</li> <li>Empty DataFrames are valid</li> </ul> </li> </ol>"},{"location":"api/processing/notes/#error-handling","title":"Error Handling","text":"<pre><code>try:\n    processor = NotesProcessor()\n    result = processor.process_type(columns)\nexcept ProcessingError as e:\n    logger.error(\"Notes processing failed: %s\", str(e))\n    # Handle error...\n</code></pre> <p>Common errors:</p> <ul> <li>No primary column</li> <li>Text encoding issues</li> <li>Memory issues with large text</li> </ul>"},{"location":"api/processing/notes/#best-practices","title":"Best Practices","text":"<p>Usage Guidelines</p> <ol> <li> <p>Data Preparation</p> <ul> <li>Ensure text encoding consistency</li> <li>Validate timestamp index</li> <li>Check primary column designation</li> </ul> </li> <li> <p>Text Cleaning</p> <ul> <li>Consider additional cleaning rules</li> <li>Monitor string lengths</li> <li>Handle special characters</li> </ul> </li> <li> <p>Memory Management</p> <ul> <li>Monitor text data size</li> <li>Consider chunking large datasets</li> <li>Clean up temporary objects</li> </ul> </li> </ol>"},{"location":"api/processing/notes/#integration-example","title":"Integration Example","text":"<pre><code># Process notes data\nprocessor = NotesProcessor()\nresult = processor.process_type(notes_columns)\n\n# Access processed data\nnotes_df = result.dataframe\n\n# Analyze notes patterns\nnotes_per_day = notes_df.groupby(notes_df.index.date).count()['notes_primary']\nhas_notes = notes_df['notes_primary'].notna()\n\n# Print statistics\nprint(f\"Total Notes: {has_notes.sum()}\")\nprint(f\"Average Notes per Day: {notes_per_day.mean():.1f}\")\n\n# Search notes content\ndef search_notes(df: pd.DataFrame, term: str) -&gt; pd.DataFrame:\n    \"\"\"Search notes for specific terms.\"\"\"\n    mask = df['notes_primary'].str.contains(\n        term, \n        case=False, \n        na=False\n    )\n    return df[mask]\n\n# Example: Find exercise-related notes\nexercise_notes = search_notes(notes_df, 'exercise')\nprint(f\"Found {len(exercise_notes)} exercise-related notes\")\n</code></pre>"},{"location":"api/processing/notes/#comparison-with-other-processors","title":"Comparison with Other Processors","text":"<p>Processor Characteristics</p> <ul> <li>Text-based rather than numeric</li> <li>No unit handling required</li> <li>Flexible validation rules</li> <li>String manipulation focus</li> <li>Memory intensive with large text</li> </ul>"},{"location":"api/processing/notes/#use-cases","title":"Use Cases","text":"<ol> <li> <p>Clinical Notes</p> <ul> <li>Patient observations</li> <li>Treatment adjustments</li> <li>Lifestyle factors</li> </ul> </li> <li> <p>Event Tracking</p> <ul> <li>Exercise sessions</li> <li>Diet changes</li> <li>Medication changes</li> </ul> </li> <li> <p>Data Analysis</p> <ul> <li>Pattern identification</li> <li>Event correlation</li> <li>Temporal analysis</li> </ul> </li> </ol>"},{"location":"user-guide/","title":"User Guide","text":"<p>Welcome to the CGM Data Processor user guide! This guide will help you understand how to effectively use the package to process and analyze your Continuous Glucose Monitoring data.</p>"},{"location":"user-guide/#what-you-can-do","title":"What You Can Do","text":"<p>With CGM Data Processor, you can:</p> <ul> <li>Load data from XDrip+ SQLite backups</li> <li>Clean and standardize glucose readings</li> <li>Classify insulin treatments (basal/bolus)</li> <li>Align data to consistent 5-minute intervals</li> <li>Analyze data quality and gaps</li> <li>Visualize data patterns and issues</li> </ul>"},{"location":"user-guide/#getting-started","title":"Getting Started","text":"<p>If you're new to CGM Data Processor, we recommend following this learning path:</p> <ol> <li> <p>Basic Data Processing</p> <ul> <li>Loading and Exporting Data: Learn how to load your XDrip+ data and export processed results</li> </ul> </li> <li> <p>Understanding the Data</p> <ul> <li>Data Processing Pipeline: Understand how your data is processed</li> <li>Gap Analysis: Learn about data quality assessment</li> </ul> </li> </ol>"},{"location":"user-guide/#core-concepts","title":"Core Concepts","text":""},{"location":"user-guide/#data-loading","title":"Data Loading","text":"<p>CGM Data Processor currently supports XDrip+ SQLite backup files, loading:</p> <ul> <li>Glucose readings from the BgReadings table</li> <li>Insulin and carbohydrate data from the Treatments table</li> </ul>"},{"location":"user-guide/#data-processing","title":"Data Processing","text":"<p>Your data goes through several processing steps:</p> <ol> <li>Cleaning and validation</li> <li>Unit standardization (mg/dL and mmol/L)</li> <li>Insulin classification</li> <li>Timeline alignment</li> <li>Gap analysis</li> </ol>"},{"location":"user-guide/#output-formats","title":"Output Formats","text":"<p>You can export your processed data in multiple formats:</p> <ol> <li>Complete aligned dataset</li> <li>Separate component datasets:</li> <li>Processed glucose readings</li> <li>Classified insulin treatments</li> <li>Validated carbohydrate entries</li> </ol>"},{"location":"user-guide/#common-use-cases","title":"Common Use Cases","text":""},{"location":"user-guide/#basic-data-export","title":"Basic Data Export","text":"<pre><code>from preprocessing.loading import XDrip\nfrom preprocessing.cleaning import clean_glucose\nfrom aligners.cgm import align_diabetes_data\n\n# Load and process data\ndata = XDrip('xdrip_backup.sqlite')\nglucose_df = clean_glucose(data.load_glucose_df())\n\n# Export processed data\nglucose_df.to_csv('processed_glucose.csv')\n</code></pre>"},{"location":"user-guide/#quality-assessment","title":"Quality Assessment","text":"<pre><code>from analysis.gap_analysis import analyse_glucose_gaps\nfrom visualization.gap_dashboard import create_gap_dashboard\n\n# Analyze data quality\ngaps_data = analyse_glucose_gaps(aligned_df)\ndashboard = create_gap_dashboard(gaps_data)\n</code></pre>"},{"location":"user-guide/#best-practices","title":"Best Practices","text":"<ol> <li> <p>Data Quality</p> <ul> <li>Always check your XDrip+ backup is recent</li> <li>Review gap analysis before detailed analysis</li> <li>Document any known sensor changes or issues</li> </ul> </li> <li> <p>Processing</p> <ul> <li>Start with default parameters</li> <li>Adjust interpolation limits based on your needs</li> <li>Verify insulin classification results</li> </ul> </li> <li> <p>Analysis</p> <ul> <li>Consider gap impact on your analysis</li> <li>Use appropriate visualizations</li> <li>Document processing decisions</li> </ul> </li> </ol>"},{"location":"user-guide/#need-help","title":"Need Help?","text":"<ul> <li>Check the API Reference for detailed function documentation</li> <li>Review Example Notebooks for practical guides</li> <li>Submit issues on GitHub</li> </ul>"},{"location":"user-guide/#next-steps","title":"Next Steps","text":"<p>Ready to start processing your data?</p> <ol> <li>Follow the Loading and Exporting Data tutorial</li> <li>Review the Data Processing Pipeline</li> <li>Learn about Gap Analysis</li> </ol>"},{"location":"user-guide/#contributing","title":"Contributing","text":"<p>Interested in contributing to CGM Data Processor?</p> <ul> <li>Read our Contributing Guide</li> <li>Check the Development Guide for setup instructions</li> <li>Review our roadmap for planned features</li> </ul>"}]}