{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"CGM Data Processor <p>A robust Python framework for processing and analysing diabetes device data</p> <p> </p>"},{"location":"#process-your-diabetes-data","title":"\ud83d\udcc8 Process Your Diabetes Data","text":"<p>Analyse data from multiple diabetes management systems including XDrip+, Dexcom, and Freestyle Libre. Handle CGM readings, insulin doses, carbs, and treatment notes with confidence.</p>"},{"location":"#cgm-analysis","title":"\ud83e\ude78 CGM Analysis","text":"<ul> <li>Gap detection</li> <li>Configurable Interpolation</li> <li>Quality metrics</li> </ul>"},{"location":"#treatment-data","title":"\ud83d\udc89 Treatment Data","text":"<ul> <li>Insulin doses</li> <li>Carb intake</li> <li>Event notes</li> </ul>"},{"location":"#advanced-features","title":"\ud83d\ude80 Advanced Features","text":"<ul> <li>Automated format detection</li> <li>Data alignment</li> <li>Flexible export options</li> <li>Complete metadata carried through to output format</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>Install CGM Data Processor - Installation Guide</p> <p>The simplest way to use the CGM Data Processor is to run <code>python -m src.cli path/to/data/export.file</code> from the root directory. The following arguments can be supllied:</p> <pre><code>python -m src.cli data.sqlite \\\n    --interpolation-limit 6   # Max CGM gaps to fill (6 = 30 mins)\n    --bolus-limit 10.0       # Max bolus insulin units\n    --max-dose 20.0          # Max valid insulin dose\n    --output ./my_analysis   # Output location\n</code></pre> <p>For individual use cases check out our API Reference section.</p> <p>Example of simple use case: <pre><code>from src.core.format_registry import FormatRegistry\nfrom src.file_parser.format_detector import FormatDetector\nfrom src.processors import DataProcessor\n\n# Initialise format detection\nregistry = FormatRegistry()\ndetector = FormatDetector(registry)\n\n# Process file\nformat, _, _ = detector.detect_format(\"my_data.sqlite\")\nprocessed_data = DataProcessor.process_file(\"my_data.sqlite\")\n</code></pre></p>"},{"location":"#key-features","title":"\ud83d\udca1 Key Features","text":"<ul> <li>Automated format detection for multiple data sources</li> <li>Robust data validation and cleaning</li> <li>Gap detection and interpolation for CGM data</li> <li>Treatment classification and verification</li> <li>Flexible data export options</li> </ul>"},{"location":"#example-output-structure","title":"\ud83d\udcca Example Output Structure","text":"<pre><code>data/exports\n\u251c\u2500\u2500 2023-06-03_to_2024-09-28_complete\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 aligned_data.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 carbs.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cgm.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 insulin.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 notes.csv\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 processing_notes.json\n\u2514\u2500\u2500 monthly\n    \u251c\u2500\u2500 2023-06\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 aligned_data.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 carbs.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 cgm.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 insulin.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 notes.csv\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 processing_notes.json\n    \u251c\u2500\u2500 2023-07\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 aligned_data.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 carbs.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 cgm.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 insulin.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 notes.csv\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 processing_notes.json\n</code></pre>"},{"location":"#responsible-use","title":"\ud83d\udee1\ufe0f Responsible Use","text":"This tool is designed for data analysis only. Not intended for real-time monitoring or medical decision making. Always consult healthcare providers for medical advice."},{"location":"api/core/","title":"Core API Reference","text":"<p>Module Location</p> <p><code>src/core/data_types.py</code></p>"},{"location":"api/core/#key-components","title":"Key Components","text":""},{"location":"api/core/#filetype","title":"FileType","text":"<pre><code>class FileType(Enum):\n    \"\"\"Supported file types for diabetes data.\"\"\"\n    SQLITE = \"sqlite\"\n    CSV = \"csv\"\n    JSON = \"json\"\n    XML = \"xml\"\n</code></pre> <p>Usage</p> <p>Used to specify and validate input file types during format detection.</p>"},{"location":"api/core/#datatype","title":"DataType","text":"<pre><code>class DataType(Enum):\n    \"\"\"Core diabetes data types.\"\"\"\n    CGM = auto()        # Continuous glucose monitoring data\n    BGM = auto()        # Blood glucose meter readings\n    INSULIN = auto()    # Insulin doses\n    INSULIN_META = auto() # Insulin metadata\n    CARBS = auto()      # Carbohydrate intake\n    NOTES = auto()      # Text notes/comments\n</code></pre> <p>Example</p> <pre><code>from src.core.data_types import DataType\n\n# Check if data is CGM reading\nif column.data_type == DataType.CGM:\n    process_cgm_data(column)\n</code></pre>"},{"location":"api/core/#timestamptype","title":"TimestampType","text":"<pre><code>class TimestampType(Enum):\n    \"\"\"Common types of timestamp format.\"\"\"\n    UNIX_SECONDS = \"unix_seconds\"\n    UNIX_MILLISECONDS = \"unix_milliseconds\"\n    UNIX_MICROSECONDS = \"unix_microseconds\"\n    ISO_8601 = \"iso_8601\"\n    UNKNOWN = \"unknown\"\n</code></pre> <p>Important</p> <p>All timestamps are converted to UTC during processing.</p>"},{"location":"api/core/#columnrequirement","title":"ColumnRequirement","text":"<pre><code>class ColumnRequirement(Enum):\n    \"\"\"Defines column validation requirements.\"\"\"\n    CONFIRMATION_ONLY = auto()    # Just needs to exist\n    REQUIRED_WITH_DATA = auto()   # Must exist with data\n    REQUIRED_NULLABLE = auto()    # Can have missing values\n    OPTIONAL = auto()             # May not exist\n</code></pre>"},{"location":"api/core/#unit","title":"Unit","text":"<pre><code>class Unit(Enum):\n    \"\"\"Supported units of measurement.\"\"\"\n    MGDL = \"mg/dL\"    # Blood glucose\n    MMOL = \"mmol/L\"   # Blood glucose\n    UNITS = \"U\"       # Insulin\n    GRAMS = \"g\"       # Carbohydrates\n</code></pre>"},{"location":"api/core/#columnmapping","title":"ColumnMapping","text":"<pre><code>@dataclass\nclass ColumnMapping:\n    \"\"\"Maps source columns to standardized data types.\n\n    Args:\n        source_name: Original column name\n        data_type: Column data type\n        unit: Unit of measurement\n        requirement: Validation requirement\n        is_primary: Primary column flag\n    \"\"\"\n    source_name: str\n    data_type: Optional[DataType] = None\n    unit: Optional[Unit] = None\n    requirement: ColumnRequirement = ColumnRequirement.REQUIRED_WITH_DATA\n    is_primary: bool = True\n</code></pre> <p>ColumnMapping Example</p> <pre><code>    glucose = ColumnMapping(\n        source_name=\"calculated_value\",\n        data_type=DataType.CGM,\n        unit=Unit.MGDL\n    )\n</code></pre>"},{"location":"api/core/#tablestructure","title":"TableStructure","text":"<pre><code>@dataclass\nclass TableStructure:\n    \"\"\"Defines data table structure.\n\n    Args:\n        name: Table name\n        timestamp_column: Timestamp column name\n        columns: Column mappings\n\n    Methods:\n        validate_columns(): Ensures table has columns\n        validate_unique_source_names(): Checks for duplicates\n        validate_primary_columns(): Validates primary columns\n    \"\"\"\n    name: str\n    timestamp_column: str\n    columns: List[ColumnMapping]\n</code></pre> <p>Validation Methods</p> <p>All validation methods raise <code>FormatValidationError</code> on failure</p>"},{"location":"api/core/#fileconfig","title":"FileConfig","text":"<pre><code>@dataclass\nclass FileConfig:\n    \"\"\"Configuration for device format file.\n\n    Args:\n        name_pattern: Filename pattern\n        file_type: File type enum\n        tables: Table structures\n\n    Validates:\n        - At least one table\n        - CSV files have one unnamed table\n    \"\"\"\n    name_pattern: str\n    file_type: FileType\n    tables: List[TableStructure]\n</code></pre>"},{"location":"api/core/#deviceformat","title":"DeviceFormat","text":"<pre><code>@dataclass\nclass DeviceFormat:\n    \"\"\"Complete device format specification.\n\n    Args:\n        name: Format name\n        files: File configurations\n\n    Methods:\n        __str__: Returns format name and data types\n    \"\"\"\n    name: str\n    files: List[FileConfig]\n</code></pre> <p>Complete Format Example</p> <pre><code>    xdrip_format = DeviceFormat(\n        name=\"xdrip_sqlite\",\n        files=[\n            FileConfig(\n                name_pattern=\"*.sqlite\",\n                file_type=FileType.SQLITE,\n                tables=[\n                    TableStructure(\n                        name=\"BgReadings\",\n                        timestamp_column=\"timestamp\",\n                        columns=[\n                            ColumnMapping(\n                                source_name=\"calculated_value\",\n                                data_type=DataType.CGM,\n                                unit=Unit.MGDL\n                            )\n                        ]\n                    )\n                ]\n            )\n        ]\n    )\n</code></pre>"},{"location":"api/core/#complete-reference","title":"Complete Reference","text":""},{"location":"api/core/#src.core.data_types","title":"<code>src.core.data_types</code>","text":"<p>Core data type definitions for diabetes data processing.</p> <p>This module defines the core data types and structures used for processing diabetes device data exports. It supports multiple file formats, different units of measurement, and various data types commonly found in diabetes management tools.</p> The structure allows for <ul> <li>Multiple files in a single format</li> <li>Multiple data types per table</li> <li>Different file types (SQLite, CSV, etc.)</li> <li>Flexible column mapping</li> <li>Primary/secondary data distinction</li> </ul>"},{"location":"api/core/#src.core.data_types.ColumnMapping","title":"<code>ColumnMapping</code>  <code>dataclass</code>","text":"<p>Maps source columns to standardized data types.</p> <p>Parameters:</p> Name Type Description Default <code>source_name</code> <code>str</code> <p>Original column name in the data source</p> required <code>data_type</code> <code>Optional[DataType]</code> <p>Type of data this column contains (if applicable - Any column can be used for confirming device.)</p> <code>None</code> <code>unit</code> <code>Optional[Unit]</code> <p>Unit of measurement (if applicable)</p> <code>None</code> <code>requirement</code> <code>ColumnRequirement</code> <p>Type of requirement - default = REQUIRED_WITH_DATA</p> <code>REQUIRED_WITH_DATA</code> <code>is_primary</code> <code>bool</code> <p>Whether this is the primary column - default = True</p> <code>True</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; glucose_column = ColumnMapping(\n...     source_name=\"calculated_value\",\n...     data_type=DataType.CGM,\n...     unit=Unit.MGDL,\n... )\n&gt;&gt;&gt; raw_glucose = ColumnMapping(\n...     source_name=\"raw_data\",\n...     data_type=DataType.CGM,\n...     requirement=ColumnRequirement.REQUIRED_NULLABLE,\n...     is_primary=False\n... )\n</code></pre> Source code in <code>src/core/data_types.py</code> <pre><code>@dataclass\nclass ColumnMapping:\n    \"\"\"Maps source columns to standardized data types.\n\n    Args:\n        source_name: Original column name in the data source\n        data_type: Type of data this column contains (if applicable - Any column can be used for confirming device.)\n        unit: Unit of measurement (if applicable)\n        requirement: Type of requirement - default = REQUIRED_WITH_DATA\n        is_primary: Whether this is the primary column - default = True\n\n    Examples:\n        &gt;&gt;&gt; glucose_column = ColumnMapping(\n        ...     source_name=\"calculated_value\",\n        ...     data_type=DataType.CGM,\n        ...     unit=Unit.MGDL,\n        ... )\n        &gt;&gt;&gt; raw_glucose = ColumnMapping(\n        ...     source_name=\"raw_data\",\n        ...     data_type=DataType.CGM,\n        ...     requirement=ColumnRequirement.REQUIRED_NULLABLE,\n        ...     is_primary=False\n        ... )\n    \"\"\"\n\n    source_name: str\n    data_type: Optional[DataType] = None\n    unit: Optional[Unit] = None\n    requirement: ColumnRequirement = ColumnRequirement.REQUIRED_WITH_DATA\n    is_primary: bool = True\n</code></pre>"},{"location":"api/core/#src.core.data_types.ColumnRequirement","title":"<code>ColumnRequirement</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Defines how column should be validated and if data reading is required</p> Source code in <code>src/core/data_types.py</code> <pre><code>class ColumnRequirement(Enum):\n    \"\"\"Defines how column should be validated and if data reading is required\"\"\"\n\n    CONFIRMATION_ONLY = auto()  # Just needs to exist - no data read\n    REQUIRED_WITH_DATA = auto()  # Must exist - data read &amp; fail if not\n    REQUIRED_NULLABLE = auto()  # Must exist, can have all missing values - data read\n    OPTIONAL = auto()  # May or may not exist - data read\n</code></pre>"},{"location":"api/core/#src.core.data_types.DataType","title":"<code>DataType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Core diabetes data types.</p> Source code in <code>src/core/data_types.py</code> <pre><code>class DataType(Enum):\n    \"\"\"Core diabetes data types.\"\"\"\n\n    # CGM Data\n    CGM = auto()  # Continuous glucose monitoring data\n\n    # BGM Data\n    BGM = auto()  # Blood glucose meter readings\n\n    # Treatment Data\n    INSULIN = auto()  # Insulin doses\n    INSULIN_META = auto()  # Insulin metadata eg brand\n    CARBS = auto()  # Carbohydrate intake\n    NOTES = auto()  # Text notes/comments\n</code></pre>"},{"location":"api/core/#src.core.data_types.DeviceFormat","title":"<code>DeviceFormat</code>  <code>dataclass</code>","text":"<p>Complete format specification for a diabetes device data export.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the device/format</p> required <code>files</code> <code>List[FileConfig]</code> <p>List of file configurations</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; xdrip_format = DeviceFormat(\n...     name=\"xdrip_sqlite\",\n...     files=[sqlite_file]  # FileConfig from previous example\n... )\n</code></pre> Source code in <code>src/core/data_types.py</code> <pre><code>@dataclass\nclass DeviceFormat:\n    \"\"\"Complete format specification for a diabetes device data export.\n\n    Args:\n        name: Name of the device/format\n        files: List of file configurations\n\n    Examples:\n        &gt;&gt;&gt; xdrip_format = DeviceFormat(\n        ...     name=\"xdrip_sqlite\",\n        ...     files=[sqlite_file]  # FileConfig from previous example\n        ... )\n    \"\"\"\n\n    name: str\n    files: List[FileConfig]\n\n    def __post_init__(self):\n        \"\"\"Validate device format after initialization.\n\n        Raises:\n            FormatValidationError: If device format is invalid\n        \"\"\"\n        if not self.files:\n            raise FormatValidationError(\n                f\"Device format {self.name} must have at least one file defined\",\n                details={\"format_name\": self.name},\n            )\n\n    def __str__(self) -&gt; str:\n        \"\"\"String representation including available data types.\"\"\"\n        types = set()\n        for file_config in self.files:\n            for table in file_config.tables:\n                for column in table.columns:\n                    if column.is_primary:\n                        types.add(column.data_type.name)\n        return f\"{self.name} - Available data: {', '.join(sorted(types))}\"\n</code></pre>"},{"location":"api/core/#src.core.data_types.DeviceFormat.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate device format after initialization.</p> <p>Raises:</p> Type Description <code>FormatValidationError</code> <p>If device format is invalid</p> Source code in <code>src/core/data_types.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate device format after initialization.\n\n    Raises:\n        FormatValidationError: If device format is invalid\n    \"\"\"\n    if not self.files:\n        raise FormatValidationError(\n            f\"Device format {self.name} must have at least one file defined\",\n            details={\"format_name\": self.name},\n        )\n</code></pre>"},{"location":"api/core/#src.core.data_types.DeviceFormat.__str__","title":"<code>__str__()</code>","text":"<p>String representation including available data types.</p> Source code in <code>src/core/data_types.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"String representation including available data types.\"\"\"\n    types = set()\n    for file_config in self.files:\n        for table in file_config.tables:\n            for column in table.columns:\n                if column.is_primary:\n                    types.add(column.data_type.name)\n    return f\"{self.name} - Available data: {', '.join(sorted(types))}\"\n</code></pre>"},{"location":"api/core/#src.core.data_types.FileConfig","title":"<code>FileConfig</code>  <code>dataclass</code>","text":"<p>Configuration for a specific file in a device format.</p> <p>Parameters:</p> Name Type Description Default <code>name_pattern</code> <code>str</code> <p>Pattern to match filename (e.g., \"*.sqlite\", \"glucose.csv\")</p> required <code>file_type</code> <code>FileType</code> <p>Type of the data file</p> required <code>tables</code> <code>List[TableStructure]</code> <p>List of table structures in the file</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; sqlite_file = FileConfig(\n...     name_pattern=\"*.sqlite\",\n...     file_type=FileType.SQLITE,\n...     tables=[bgreadings]  # TableStructure from previous example\n... )\n</code></pre> Source code in <code>src/core/data_types.py</code> <pre><code>@dataclass\nclass FileConfig:\n    \"\"\"Configuration for a specific file in a device format.\n\n    Args:\n        name_pattern: Pattern to match filename (e.g., \"*.sqlite\", \"glucose.csv\")\n        file_type: Type of the data file\n        tables: List of table structures in the file\n\n    Examples:\n        &gt;&gt;&gt; sqlite_file = FileConfig(\n        ...     name_pattern=\"*.sqlite\",\n        ...     file_type=FileType.SQLITE,\n        ...     tables=[bgreadings]  # TableStructure from previous example\n        ... )\n    \"\"\"\n\n    name_pattern: str\n    file_type: FileType\n    tables: List[TableStructure]\n\n    def __post_init__(self):\n        \"\"\"Validate file configuration after initialization.\n\n        Raises:\n            FormatValidationError: If file configuration is invalid\n        \"\"\"\n        if not self.tables:\n            raise FormatValidationError(\n                f\"File {self.name_pattern} must have at least one table defined\",\n                details={\"file_pattern\": self.name_pattern},\n            )\n\n        # For CSV files, ensure only one table with empty name\n        if self.file_type == FileType.CSV:\n            if len(self.tables) &gt; 1:\n                raise FormatValidationError(\n                    \"CSV files can only have one table structure\",\n                    details={\n                        \"file_pattern\": self.name_pattern,\n                        \"tables_count\": len(self.tables),\n                    },\n                )\n            if self.tables[0].name != \"\":\n                raise FormatValidationError(\n                    f\"CSV file table name should be empty string for file {self.name_pattern}\",\n                    details={\n                        \"file_pattern\": self.name_pattern,\n                        \"table_name\": self.tables[0].name,\n                    },\n                )\n</code></pre>"},{"location":"api/core/#src.core.data_types.FileConfig.__post_init__","title":"<code>__post_init__()</code>","text":"<p>Validate file configuration after initialization.</p> <p>Raises:</p> Type Description <code>FormatValidationError</code> <p>If file configuration is invalid</p> Source code in <code>src/core/data_types.py</code> <pre><code>def __post_init__(self):\n    \"\"\"Validate file configuration after initialization.\n\n    Raises:\n        FormatValidationError: If file configuration is invalid\n    \"\"\"\n    if not self.tables:\n        raise FormatValidationError(\n            f\"File {self.name_pattern} must have at least one table defined\",\n            details={\"file_pattern\": self.name_pattern},\n        )\n\n    # For CSV files, ensure only one table with empty name\n    if self.file_type == FileType.CSV:\n        if len(self.tables) &gt; 1:\n            raise FormatValidationError(\n                \"CSV files can only have one table structure\",\n                details={\n                    \"file_pattern\": self.name_pattern,\n                    \"tables_count\": len(self.tables),\n                },\n            )\n        if self.tables[0].name != \"\":\n            raise FormatValidationError(\n                f\"CSV file table name should be empty string for file {self.name_pattern}\",\n                details={\n                    \"file_pattern\": self.name_pattern,\n                    \"table_name\": self.tables[0].name,\n                },\n            )\n</code></pre>"},{"location":"api/core/#src.core.data_types.FileType","title":"<code>FileType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Supported file types for diabetes data.</p> Source code in <code>src/core/data_types.py</code> <pre><code>class FileType(Enum):\n    \"\"\"Supported file types for diabetes data.\"\"\"\n\n    SQLITE = \"sqlite\"\n    CSV = \"csv\"\n    JSON = \"json\"\n    XML = \"xml\"\n</code></pre>"},{"location":"api/core/#src.core.data_types.TableStructure","title":"<code>TableStructure</code>  <code>dataclass</code>","text":"<p>Defines the structure of a data table.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Table name in the data source (empty string for CSV files)</p> required <code>timestamp_column</code> <code>str</code> <p>Name of the timestamp column</p> required <code>columns</code> <code>List[ColumnMapping]</code> <p>List of column mappings</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; bgreadings = TableStructure(\n...     name=\"bgreadings\",\n...     timestamp_column=\"timestamp\",\n...     columns=[\n...         ColumnMapping(\n...             source_name=\"calculated_value\",\n...             data_type=DataType.CGM,\n...             unit=Unit.MGDL\n...         ),\n...         ColumnMapping(\n...             source_name=\"raw_data\",\n...             data_type=DataType.CGM,\n...             requirement=ColumnRequirement.REQUIRED_NULLABLE,\n...             is_primary=False\n...         )\n...     ]\n... )\n</code></pre> Source code in <code>src/core/data_types.py</code> <pre><code>@dataclass\nclass TableStructure:\n    \"\"\"Defines the structure of a data table.\n\n    Args:\n        name: Table name in the data source (empty string for CSV files)\n        timestamp_column: Name of the timestamp column\n        columns: List of column mappings\n\n    Examples:\n        &gt;&gt;&gt; bgreadings = TableStructure(\n        ...     name=\"bgreadings\",\n        ...     timestamp_column=\"timestamp\",\n        ...     columns=[\n        ...         ColumnMapping(\n        ...             source_name=\"calculated_value\",\n        ...             data_type=DataType.CGM,\n        ...             unit=Unit.MGDL\n        ...         ),\n        ...         ColumnMapping(\n        ...             source_name=\"raw_data\",\n        ...             data_type=DataType.CGM,\n        ...             requirement=ColumnRequirement.REQUIRED_NULLABLE,\n        ...             is_primary=False\n        ...         )\n        ...     ]\n        ... )\n    \"\"\"\n\n    name: str\n    timestamp_column: str\n    columns: List[ColumnMapping]\n\n    def validate_columns(self):\n        \"\"\"Validate that table has at least one column defined.\n\n        Raises:\n            FormatValidationError: If table has no columns defined\n        \"\"\"\n        if not self.columns:\n            raise FormatValidationError(\n                f\"Table {self.name} must have at least one column defined\",\n                details={\"table_name\": self.name, \"columns_count\": 0},\n            )\n\n    def validate_unique_source_names(self):\n        \"\"\"Validate that all column names are unique.\n\n        Raises:\n            FormatValidationError: If duplicate column names are found\n        \"\"\"\n        column_names = [col.source_name for col in self.columns]\n        unique_names = set(column_names)\n        if len(column_names) != len(unique_names):\n            duplicates = [name for name in unique_names if column_names.count(name) &gt; 1]\n            raise FormatValidationError(\n                f\"Duplicate column names in table {self.name}\",\n                details={\"table_name\": self.name, \"duplicate_columns\": duplicates},\n            )\n\n    def validate_primary_columns(self):\n        \"\"\"Validate that each data type has at most one primary column.\n\n        Raises:\n            FormatValidationError: If multiple primary columns exist for any data type\n        \"\"\"\n        for data_type in DataType:\n            primary_columns = [\n                col.source_name\n                for col in self.columns\n                if col.data_type == data_type and col.is_primary\n            ]\n            if len(primary_columns) &gt; 1:\n                raise FormatValidationError(\n                    f\"Multiple primary columns for {data_type.value} in table {self.name}\",\n                    details={\n                        \"table_name\": self.name,\n                        \"data_type\": data_type.value,\n                        \"primary_columns\": primary_columns,\n                    },\n                )\n\n    def __post_init__(self):\n        self.validate_columns()\n        self.validate_unique_source_names()\n        self.validate_primary_columns()\n</code></pre>"},{"location":"api/core/#src.core.data_types.TableStructure.validate_columns","title":"<code>validate_columns()</code>","text":"<p>Validate that table has at least one column defined.</p> <p>Raises:</p> Type Description <code>FormatValidationError</code> <p>If table has no columns defined</p> Source code in <code>src/core/data_types.py</code> <pre><code>def validate_columns(self):\n    \"\"\"Validate that table has at least one column defined.\n\n    Raises:\n        FormatValidationError: If table has no columns defined\n    \"\"\"\n    if not self.columns:\n        raise FormatValidationError(\n            f\"Table {self.name} must have at least one column defined\",\n            details={\"table_name\": self.name, \"columns_count\": 0},\n        )\n</code></pre>"},{"location":"api/core/#src.core.data_types.TableStructure.validate_primary_columns","title":"<code>validate_primary_columns()</code>","text":"<p>Validate that each data type has at most one primary column.</p> <p>Raises:</p> Type Description <code>FormatValidationError</code> <p>If multiple primary columns exist for any data type</p> Source code in <code>src/core/data_types.py</code> <pre><code>def validate_primary_columns(self):\n    \"\"\"Validate that each data type has at most one primary column.\n\n    Raises:\n        FormatValidationError: If multiple primary columns exist for any data type\n    \"\"\"\n    for data_type in DataType:\n        primary_columns = [\n            col.source_name\n            for col in self.columns\n            if col.data_type == data_type and col.is_primary\n        ]\n        if len(primary_columns) &gt; 1:\n            raise FormatValidationError(\n                f\"Multiple primary columns for {data_type.value} in table {self.name}\",\n                details={\n                    \"table_name\": self.name,\n                    \"data_type\": data_type.value,\n                    \"primary_columns\": primary_columns,\n                },\n            )\n</code></pre>"},{"location":"api/core/#src.core.data_types.TableStructure.validate_unique_source_names","title":"<code>validate_unique_source_names()</code>","text":"<p>Validate that all column names are unique.</p> <p>Raises:</p> Type Description <code>FormatValidationError</code> <p>If duplicate column names are found</p> Source code in <code>src/core/data_types.py</code> <pre><code>def validate_unique_source_names(self):\n    \"\"\"Validate that all column names are unique.\n\n    Raises:\n        FormatValidationError: If duplicate column names are found\n    \"\"\"\n    column_names = [col.source_name for col in self.columns]\n    unique_names = set(column_names)\n    if len(column_names) != len(unique_names):\n        duplicates = [name for name in unique_names if column_names.count(name) &gt; 1]\n        raise FormatValidationError(\n            f\"Duplicate column names in table {self.name}\",\n            details={\"table_name\": self.name, \"duplicate_columns\": duplicates},\n        )\n</code></pre>"},{"location":"api/core/#src.core.data_types.TimestampType","title":"<code>TimestampType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Common types of timestamp format, ensuring correct conversion</p> Source code in <code>src/core/data_types.py</code> <pre><code>class TimestampType(Enum):\n    \"\"\"Common types of timestamp format, ensuring correct conversion\"\"\"\n\n    UNIX_SECONDS = \"unix_seconds\"\n    UNIX_MILLISECONDS = \"unix_milliseconds\"\n    UNIX_MICROSECONDS = \"unix_microseconds\"\n    ISO_8601 = \"iso_8601\"\n    UNKNOWN = \"unknown\"\n</code></pre>"},{"location":"api/core/#src.core.data_types.Unit","title":"<code>Unit</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Supported units of measurement.</p> Source code in <code>src/core/data_types.py</code> <pre><code>class Unit(Enum):\n    \"\"\"Supported units of measurement.\"\"\"\n\n    MGDL = \"mg/dL\"  # Blood glucose in mg/dL\n    MMOL = \"mmol/L\"  # Blood glucose in mmol/L\n    UNITS = \"U\"  # Insulin units\n    GRAMS = \"g\"  # Carbohydrates in grams\n</code></pre>"},{"location":"api/exporters/","title":"Exporters API","text":""},{"location":"api/exporters/#complete-reference","title":"Complete Reference","text":""},{"location":"api/exporters/#src.exporters","title":"<code>src.exporters</code>","text":"<p>Initialize processors package and register all processors.</p>"},{"location":"api/exporters/#src.exporters.BaseExporter","title":"<code>BaseExporter</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for data exporters.</p> Source code in <code>src/exporters/base.py</code> <pre><code>class BaseExporter(ABC):\n    \"\"\"Abstract base class for data exporters.\"\"\"\n\n    def __init__(self, config: ExportConfig):\n        self.config = config\n        self._ensure_directories()\n        self.monthly_notes_cache = {}\n\n    def _ensure_directories(self) -&gt; None:\n        \"\"\"Create necessary directory structure.\"\"\"\n        self.config.output_dir.mkdir(parents=True, exist_ok=True)\n        if self.config.split_by_month:\n            (self.config.output_dir / \"monthly\").mkdir(exist_ok=True)\n\n    @abstractmethod\n    def export_complete_dataset(\n        self, data: ProcessedTypeData, data_type: DataType, output_dir: Path\n    ) -&gt; None:\n        \"\"\"Export complete dataset for a specific data type.\"\"\"\n\n    @abstractmethod\n    def export_monthly_split(\n        self, data: ProcessedTypeData, data_type: DataType, month_dir: Path\n    ) -&gt; None:\n        \"\"\"Export monthly split for a specific data type.\"\"\"\n\n    @abstractmethod\n    def export_processing_notes(\n        self, data: Union[ProcessedTypeData, AlignmentResult], output_path: Path\n    ) -&gt; None:\n        \"\"\"Export processing notes.\"\"\"\n\n    @abstractmethod\n    def export_aligned_complete_dataset(\n        self, data: AlignmentResult, output_dir: Path\n    ) -&gt; None:\n        \"\"\"Export complete aligned dataset.\"\"\"\n\n    @abstractmethod\n    def export_aligned_monthly_split(\n        self, data: AlignmentResult, month_dir: Path\n    ) -&gt; None:\n        \"\"\"Export monthly split for aligned data.\"\"\"\n\n    def _generate_type_stats(\n        self, data: pd.DataFrame, data_type: Optional[DataType] = None\n    ) -&gt; List[str]:\n        \"\"\"Generate statistics for a specific data type.\"\"\"\n        stats = []\n\n        if data_type == DataType.CGM or \"cgm_primary\" in data.columns:\n            missing_count = data.get(\"missing_cgm\", data.get(\"missing\", 0)).sum()\n            total_readings = len(data)\n            total_na = data[\"cgm_primary\"].isna().sum()\n            initial_completeness = (\n                (total_readings - missing_count) / total_readings\n            ) * 100\n            remaining_completeness = (\n                (total_readings - total_na) / total_readings\n            ) * 100\n\n            stats.extend(\n                [\n                    \"CGM Processing Notes:\",\n                    f\"  Processed {total_readings} total CGM readings\",\n                    f\"  Found {missing_count} missing or interpolated values\",\n                    f\"  Initial CGM completeness: {initial_completeness:.2f}%\",\n                    f\"  CGM completeness after interpolation: {remaining_completeness:.2f}%\",\n                ]\n            )\n\n        if data_type == DataType.INSULIN or \"basal_dose\" in data.columns:\n            if data_type == DataType.INSULIN:\n                basal_count = data[\"is_basal\"].sum()\n                bolus_count = data[\"is_bolus\"].sum()\n            else:\n                basal_count = (data[\"basal_dose\"] &gt; 0).sum()\n                bolus_count = (data[\"bolus_dose\"] &gt; 0).sum()\n            stats.extend(\n                [\n                    \"INSULIN Processing Notes:\",\n                    f\"  Found {basal_count + bolus_count} total doses\",\n                    f\"  {basal_count} basal doses\",\n                    f\"  {bolus_count} bolus doses\",\n                ]\n            )\n\n        if data_type == DataType.CARBS or \"carbs_primary\" in data.columns:\n            carb_entries = (\n                (data[\"carbs_primary\"] &gt; 0).sum()\n                if \"carbs_primary\" in data.columns\n                else (data &gt; 0).sum()\n            )\n            stats.extend(\n                [\"CARBS Processing Notes:\", f\"  Found {carb_entries} carb entries\"]\n            )\n\n        if data_type == DataType.NOTES or \"notes_primary\" in data.columns:\n            note_count = (\n                data[\"notes_primary\"].notna().sum()\n                if \"notes_primary\" in data.columns\n                else data.notna().sum()\n            )\n            stats.extend(\n                [\"NOTES Processing Notes:\", f\"  Found {note_count} notes entries\"]\n            )\n\n        return stats\n\n    def _accumulate_monthly_stats(\n        self, month_str: str, group: pd.DataFrame, data_type: DataType\n    ) -&gt; None:\n        \"\"\"Accumulate statistics for a month across all data types.\"\"\"\n        if month_str not in self.monthly_notes_cache:\n            self.monthly_notes_cache[month_str] = {\n                \"period\": month_str,\n                \"record_count\": len(group),\n                \"stats\": [],\n            }\n\n        type_stats = self._generate_type_stats(group, data_type)\n        if type_stats:\n            self.monthly_notes_cache[month_str][\"stats\"].extend(type_stats)\n\n    def _handle_monthly_exports(\n        self, data: ProcessedTypeData, data_type: DataType\n    ) -&gt; None:\n        \"\"\"Handle monthly data splits and exports.\"\"\"\n\n        monthly_base = self.config.output_dir / \"monthly\"\n\n        for timestamp, group in data.dataframe.groupby(pd.Grouper(freq=\"ME\")):\n            if not group.empty:\n                month_str = pd.Timestamp(timestamp).strftime(\"%Y-%m\")\n                month_dir = monthly_base / month_str\n                month_dir.mkdir(parents=True, exist_ok=True)\n\n                self._accumulate_monthly_stats(month_str, group, data_type)\n\n                monthly_notes = [\n                    f\"Period: {month_str}\",\n                    f\"Records: {len(group)}\",\n                    f\"Columns present: {', '.join(group.columns)}\",\n                    *self.monthly_notes_cache[month_str][\"stats\"],\n                ]\n\n                monthly_data = ProcessedTypeData(\n                    dataframe=group,\n                    source_units=data.source_units,\n                    processing_notes=monthly_notes,\n                )\n\n                if self.config.include_processing_notes:\n                    self.export_processing_notes(monthly_data, month_dir)\n                self.export_monthly_split(monthly_data, data_type, month_dir)\n\n    def _handle_monthly_aligned_exports(self, data: AlignmentResult) -&gt; None:\n        \"\"\"Handle monthly splits for aligned data.\"\"\"\n        monthly_base = self.config.output_dir / \"monthly\"\n\n        for timestamp, group in data.dataframe.groupby(pd.Grouper(freq=\"ME\")):\n            if not group.empty:\n                month_str = pd.Timestamp(timestamp).strftime(\"%Y-%m\")\n                month_dir = monthly_base / month_str\n                month_dir.mkdir(parents=True, exist_ok=True)\n\n                monthly_notes = [\n                    f\"Period: {month_str}\",\n                    f\"Records: {len(group)}\",\n                    f\"Columns present: {', '.join(group.columns)}\",\n                    *self._generate_type_stats(group),\n                ]\n\n                monthly_aligned = AlignmentResult(\n                    dataframe=group,\n                    start_time=group.index.min(),\n                    end_time=group.index.max(),\n                    frequency=data.frequency,\n                    processing_notes=monthly_notes,\n                    source_units=data.source_units,\n                )\n\n                self.export_aligned_monthly_split(monthly_aligned, month_dir)\n\n    def export_data(\n        self,\n        processed_data: Dict[DataType, ProcessedTypeData],\n        aligned_data: Optional[AlignmentResult] = None,\n    ) -&gt; None:\n        \"\"\"Export all processed data and aligned data if available.\"\"\"\n        if not processed_data and not aligned_data:\n            return\n\n        # Reset monthly notes cache\n        self.monthly_notes_cache = {}\n\n        # Get date range from either source\n        date_range = self.get_date_range(\n            next(iter(processed_data.values())) if processed_data else aligned_data\n        )\n        complete_dir = self.config.output_dir / f\"{date_range}_complete\"\n        complete_dir.mkdir(parents=True, exist_ok=True)\n\n        # Export individual datasets\n        for data_type, type_data in processed_data.items():\n            self.export_complete_dataset(type_data, data_type, complete_dir)\n            if self.config.split_by_month:\n                self._handle_monthly_exports(type_data, data_type)\n\n        # Export aligned data if available\n        if aligned_data:\n            self._export_aligned_data(aligned_data, complete_dir)\n            if self.config.split_by_month:\n                self._handle_monthly_aligned_exports(aligned_data)\n\n    def _export_aligned_data(\n        self, aligned_data: AlignmentResult, output_dir: Path\n    ) -&gt; None:\n        \"\"\"Export aligned dataset.\"\"\"\n        self.export_aligned_complete_dataset(aligned_data, output_dir)\n        if self.config.include_processing_notes:\n            self.export_processing_notes(aligned_data, output_dir)\n\n    @staticmethod\n    def get_date_range(data: Union[ProcessedTypeData, AlignmentResult]) -&gt; str:\n        \"\"\"Get date range string from data.\"\"\"\n        df = data.dataframe\n        if not isinstance(df.index, pd.DatetimeIndex):\n            raise ValueError(\"DataFrame must have DatetimeIndex\")\n        start = df.index.min().strftime(\"%Y-%m-%d\")\n        end = df.index.max().strftime(\"%Y-%m-%d\")\n        return f\"{start}_to_{end}\"\n</code></pre>"},{"location":"api/exporters/#src.exporters.BaseExporter.export_aligned_complete_dataset","title":"<code>export_aligned_complete_dataset(data, output_dir)</code>  <code>abstractmethod</code>","text":"<p>Export complete aligned dataset.</p> Source code in <code>src/exporters/base.py</code> <pre><code>@abstractmethod\ndef export_aligned_complete_dataset(\n    self, data: AlignmentResult, output_dir: Path\n) -&gt; None:\n    \"\"\"Export complete aligned dataset.\"\"\"\n</code></pre>"},{"location":"api/exporters/#src.exporters.BaseExporter.export_aligned_monthly_split","title":"<code>export_aligned_monthly_split(data, month_dir)</code>  <code>abstractmethod</code>","text":"<p>Export monthly split for aligned data.</p> Source code in <code>src/exporters/base.py</code> <pre><code>@abstractmethod\ndef export_aligned_monthly_split(\n    self, data: AlignmentResult, month_dir: Path\n) -&gt; None:\n    \"\"\"Export monthly split for aligned data.\"\"\"\n</code></pre>"},{"location":"api/exporters/#src.exporters.BaseExporter.export_complete_dataset","title":"<code>export_complete_dataset(data, data_type, output_dir)</code>  <code>abstractmethod</code>","text":"<p>Export complete dataset for a specific data type.</p> Source code in <code>src/exporters/base.py</code> <pre><code>@abstractmethod\ndef export_complete_dataset(\n    self, data: ProcessedTypeData, data_type: DataType, output_dir: Path\n) -&gt; None:\n    \"\"\"Export complete dataset for a specific data type.\"\"\"\n</code></pre>"},{"location":"api/exporters/#src.exporters.BaseExporter.export_data","title":"<code>export_data(processed_data, aligned_data=None)</code>","text":"<p>Export all processed data and aligned data if available.</p> Source code in <code>src/exporters/base.py</code> <pre><code>def export_data(\n    self,\n    processed_data: Dict[DataType, ProcessedTypeData],\n    aligned_data: Optional[AlignmentResult] = None,\n) -&gt; None:\n    \"\"\"Export all processed data and aligned data if available.\"\"\"\n    if not processed_data and not aligned_data:\n        return\n\n    # Reset monthly notes cache\n    self.monthly_notes_cache = {}\n\n    # Get date range from either source\n    date_range = self.get_date_range(\n        next(iter(processed_data.values())) if processed_data else aligned_data\n    )\n    complete_dir = self.config.output_dir / f\"{date_range}_complete\"\n    complete_dir.mkdir(parents=True, exist_ok=True)\n\n    # Export individual datasets\n    for data_type, type_data in processed_data.items():\n        self.export_complete_dataset(type_data, data_type, complete_dir)\n        if self.config.split_by_month:\n            self._handle_monthly_exports(type_data, data_type)\n\n    # Export aligned data if available\n    if aligned_data:\n        self._export_aligned_data(aligned_data, complete_dir)\n        if self.config.split_by_month:\n            self._handle_monthly_aligned_exports(aligned_data)\n</code></pre>"},{"location":"api/exporters/#src.exporters.BaseExporter.export_monthly_split","title":"<code>export_monthly_split(data, data_type, month_dir)</code>  <code>abstractmethod</code>","text":"<p>Export monthly split for a specific data type.</p> Source code in <code>src/exporters/base.py</code> <pre><code>@abstractmethod\ndef export_monthly_split(\n    self, data: ProcessedTypeData, data_type: DataType, month_dir: Path\n) -&gt; None:\n    \"\"\"Export monthly split for a specific data type.\"\"\"\n</code></pre>"},{"location":"api/exporters/#src.exporters.BaseExporter.export_processing_notes","title":"<code>export_processing_notes(data, output_path)</code>  <code>abstractmethod</code>","text":"<p>Export processing notes.</p> Source code in <code>src/exporters/base.py</code> <pre><code>@abstractmethod\ndef export_processing_notes(\n    self, data: Union[ProcessedTypeData, AlignmentResult], output_path: Path\n) -&gt; None:\n    \"\"\"Export processing notes.\"\"\"\n</code></pre>"},{"location":"api/exporters/#src.exporters.BaseExporter.get_date_range","title":"<code>get_date_range(data)</code>  <code>staticmethod</code>","text":"<p>Get date range string from data.</p> Source code in <code>src/exporters/base.py</code> <pre><code>@staticmethod\ndef get_date_range(data: Union[ProcessedTypeData, AlignmentResult]) -&gt; str:\n    \"\"\"Get date range string from data.\"\"\"\n    df = data.dataframe\n    if not isinstance(df.index, pd.DatetimeIndex):\n        raise ValueError(\"DataFrame must have DatetimeIndex\")\n    start = df.index.min().strftime(\"%Y-%m-%d\")\n    end = df.index.max().strftime(\"%Y-%m-%d\")\n    return f\"{start}_to_{end}\"\n</code></pre>"},{"location":"api/exporters/#src.exporters.CSVExporter","title":"<code>CSVExporter</code>","text":"<p>               Bases: <code>BaseExporter</code></p> <p>CSV implementation of data exporter.</p> Source code in <code>src/exporters/csv.py</code> <pre><code>class CSVExporter(BaseExporter):\n    \"\"\"CSV implementation of data exporter.\"\"\"\n\n    def export_complete_dataset(\n        self, data: ProcessedTypeData, data_type: DataType, output_dir: Path\n    ) -&gt; None:\n        \"\"\"Export complete dataset as CSV.\"\"\"\n        filename = f\"{data_type.name.lower()}.csv\"\n        data.dataframe.to_csv(output_dir / filename)\n\n        if self.config.include_processing_notes:\n            self.export_processing_notes(data, output_dir)\n\n    def export_monthly_split(\n        self, data: ProcessedTypeData, data_type: DataType, month_dir: Path\n    ) -&gt; None:\n        \"\"\"Export monthly split as CSV.\"\"\"\n        filename = f\"{data_type.name.lower()}.csv\"\n        data.dataframe.to_csv(month_dir / filename)\n\n    def export_aligned_complete_dataset(\n        self, data: AlignmentResult, output_dir: Path\n    ) -&gt; None:\n        \"\"\"Export complete aligned dataset as CSV.\"\"\"\n        data.dataframe.to_csv(output_dir / \"aligned_data.csv\")\n\n    def export_aligned_monthly_split(\n        self, data: AlignmentResult, month_dir: Path\n    ) -&gt; None:\n        \"\"\"Export monthly split of aligned data as CSV.\"\"\"\n        data.dataframe.to_csv(month_dir / \"aligned_data.csv\")\n        if self.config.include_processing_notes:\n            self.export_processing_notes(data, month_dir)\n\n    def export_processing_notes(\n        self, data: Union[ProcessedTypeData, AlignmentResult], output_path: Path\n    ) -&gt; None:\n        \"\"\"Export processing notes as JSON.\"\"\"\n        common_data = {\n            \"export_date\": pd.Timestamp.now().isoformat(),\n            \"date_range\": f\"{data.dataframe.index.min().strftime('%Y-%m-%d')} to {data.dataframe.index.max().strftime('%Y-%m-%d')}\",\n            \"record_count\": len(data.dataframe),\n            \"notes\": data.processing_notes,\n        }\n\n        if isinstance(data, ProcessedTypeData):\n            common_data[\"source_units\"] = {\n                k: v.value for k, v in data.source_units.items()\n            }\n        else:  # AlignmentResult\n            common_data[\"frequency\"] = data.frequency\n            common_data[\"completeness\"] = {\n                col: f\"{(data.dataframe[col].notna().mean() * 100):.2f}%\"\n                for col in data.dataframe.columns\n            }\n\n        with open(output_path / \"processing_notes.json\", \"w\", encoding=\"utf-8\") as f:\n            json.dump(common_data, f, indent=2)\n</code></pre>"},{"location":"api/exporters/#src.exporters.CSVExporter.export_aligned_complete_dataset","title":"<code>export_aligned_complete_dataset(data, output_dir)</code>","text":"<p>Export complete aligned dataset as CSV.</p> Source code in <code>src/exporters/csv.py</code> <pre><code>def export_aligned_complete_dataset(\n    self, data: AlignmentResult, output_dir: Path\n) -&gt; None:\n    \"\"\"Export complete aligned dataset as CSV.\"\"\"\n    data.dataframe.to_csv(output_dir / \"aligned_data.csv\")\n</code></pre>"},{"location":"api/exporters/#src.exporters.CSVExporter.export_aligned_monthly_split","title":"<code>export_aligned_monthly_split(data, month_dir)</code>","text":"<p>Export monthly split of aligned data as CSV.</p> Source code in <code>src/exporters/csv.py</code> <pre><code>def export_aligned_monthly_split(\n    self, data: AlignmentResult, month_dir: Path\n) -&gt; None:\n    \"\"\"Export monthly split of aligned data as CSV.\"\"\"\n    data.dataframe.to_csv(month_dir / \"aligned_data.csv\")\n    if self.config.include_processing_notes:\n        self.export_processing_notes(data, month_dir)\n</code></pre>"},{"location":"api/exporters/#src.exporters.CSVExporter.export_complete_dataset","title":"<code>export_complete_dataset(data, data_type, output_dir)</code>","text":"<p>Export complete dataset as CSV.</p> Source code in <code>src/exporters/csv.py</code> <pre><code>def export_complete_dataset(\n    self, data: ProcessedTypeData, data_type: DataType, output_dir: Path\n) -&gt; None:\n    \"\"\"Export complete dataset as CSV.\"\"\"\n    filename = f\"{data_type.name.lower()}.csv\"\n    data.dataframe.to_csv(output_dir / filename)\n\n    if self.config.include_processing_notes:\n        self.export_processing_notes(data, output_dir)\n</code></pre>"},{"location":"api/exporters/#src.exporters.CSVExporter.export_monthly_split","title":"<code>export_monthly_split(data, data_type, month_dir)</code>","text":"<p>Export monthly split as CSV.</p> Source code in <code>src/exporters/csv.py</code> <pre><code>def export_monthly_split(\n    self, data: ProcessedTypeData, data_type: DataType, month_dir: Path\n) -&gt; None:\n    \"\"\"Export monthly split as CSV.\"\"\"\n    filename = f\"{data_type.name.lower()}.csv\"\n    data.dataframe.to_csv(month_dir / filename)\n</code></pre>"},{"location":"api/exporters/#src.exporters.CSVExporter.export_processing_notes","title":"<code>export_processing_notes(data, output_path)</code>","text":"<p>Export processing notes as JSON.</p> Source code in <code>src/exporters/csv.py</code> <pre><code>def export_processing_notes(\n    self, data: Union[ProcessedTypeData, AlignmentResult], output_path: Path\n) -&gt; None:\n    \"\"\"Export processing notes as JSON.\"\"\"\n    common_data = {\n        \"export_date\": pd.Timestamp.now().isoformat(),\n        \"date_range\": f\"{data.dataframe.index.min().strftime('%Y-%m-%d')} to {data.dataframe.index.max().strftime('%Y-%m-%d')}\",\n        \"record_count\": len(data.dataframe),\n        \"notes\": data.processing_notes,\n    }\n\n    if isinstance(data, ProcessedTypeData):\n        common_data[\"source_units\"] = {\n            k: v.value for k, v in data.source_units.items()\n        }\n    else:  # AlignmentResult\n        common_data[\"frequency\"] = data.frequency\n        common_data[\"completeness\"] = {\n            col: f\"{(data.dataframe[col].notna().mean() * 100):.2f}%\"\n            for col in data.dataframe.columns\n        }\n\n    with open(output_path / \"processing_notes.json\", \"w\", encoding=\"utf-8\") as f:\n        json.dump(common_data, f, indent=2)\n</code></pre>"},{"location":"api/exporters/#src.exporters.ExportConfig","title":"<code>ExportConfig</code>  <code>dataclass</code>","text":"<p>Configuration for data export.</p> Source code in <code>src/exporters/base.py</code> <pre><code>@dataclass\nclass ExportConfig:\n    \"\"\"Configuration for data export.\"\"\"\n\n    output_dir: Path = Path(\"data/exports\")  # Default exports directory\n    split_by_month: bool = True\n    include_processing_notes: bool = True\n    date_in_filename: bool = True\n</code></pre>"},{"location":"api/exporters/#src.exporters.create_csv_exporter","title":"<code>create_csv_exporter(output_dir=ExportConfig.output_dir)</code>","text":"<p>Factory function for CSV exporter.</p> Source code in <code>src/exporters/csv.py</code> <pre><code>def create_csv_exporter(\n    output_dir: str | Path = ExportConfig.output_dir,\n) -&gt; CSVExporter:\n    \"\"\"Factory function for CSV exporter.\"\"\"\n    config = ExportConfig(output_dir=Path(output_dir))\n    return CSVExporter(config)\n</code></pre>"},{"location":"api/file_parser/","title":"Format Detection API","text":""},{"location":"api/file_parser/#complete-reference","title":"Complete Reference","text":""},{"location":"api/file_parser/#src.file_parser.format_detector","title":"<code>src.file_parser.format_detector</code>","text":"<p>Format detection for diabetes device data files.</p> <p>This module provides functionality to detect device formats by examining file structure. It validates only the presence of required tables and columns, without checking data content.</p>"},{"location":"api/file_parser/#src.file_parser.format_detector.FormatDetector","title":"<code>FormatDetector</code>","text":"<p>Detects device formats by examining file structure.</p> Source code in <code>src/file_parser/format_detector.py</code> <pre><code>class FormatDetector:\n    \"\"\"Detects device formats by examining file structure.\"\"\"\n\n    def __init__(self, format_registry: FormatRegistry):\n        \"\"\"Initialize detector with format registry.\"\"\"\n        self._registry = format_registry\n\n    def detect_format(\n        self, path: Path\n    ) -&gt; Tuple[Optional[DeviceFormat], Optional[str], Dict[str, ValidationResult]]:\n        \"\"\"Detect format of provided file.\n\n        Args:\n            file_path: Path to the file to check\n\n        Returns:\n            Tuple containing:\n                - Matched format (or None)\n                - Error message (or None)\n                - Dictionary of validation results per format tried\n\n        Example:\n            &gt;&gt;&gt; detector = FormatDetector(registry)\n            &gt;&gt;&gt; fmt, error, results = detector.detect_format(Path(\"data.sqlite\"))\n            &gt;&gt;&gt; if fmt:\n            ...     print(f\"Matched format: {fmt.name}\")\n            ... else:\n            ...     print(f\"No match: {error}\")\n        \"\"\"\n        logger.debug(\"Starting format detection for: %s\", path)\n        val_results = {}\n\n        try:\n            # Validate file exists and is readable\n            if not self._validate_file_exists(path):\n                return None, f\"File not found or not accessible: {path}\", {}\n\n            # Get potential formats based on file extension\n            potential_formats = self._registry.get_formats_for_file(path)\n            if not potential_formats:\n                return None, f\"No formats available for {path.suffix} files\", {}\n\n            # Try each format\n            for fmt in potential_formats:\n                try:\n                    val_test_result = ValidationResult()\n                    if self._validate_format(path, fmt, val_test_result):\n                        logger.debug(\"Successfully matched format: %s\", fmt.name)\n                        return fmt, None, val_results\n                    val_results[fmt.name] = val_test_result\n                except FormatValidationError as e:\n                    logger.debug(\"Error validating format %s: %s\", fmt.name, str(e))\n                    continue\n\n            return None, \"No matching format found\", val_results\n\n        except FileAccessError as e:\n            logger.error(\"Unexpected error during format detection: %s\", str(e))\n            return None, f\"Detection error: {str(e)}\", {}\n\n    def _validate_file_exists(self, path: Path) -&gt; bool:\n        \"\"\"Validate file exists and is accessible.\"\"\"\n        try:\n            return path.exists() and path.is_file()\n        except Exception as e:\n            raise FileAccessError(f\"Error occurred: {str(e)}\") from e\n\n    def _validate_format(\n        self, path: Path, fmt: DeviceFormat, validation_result: ValidationResult\n    ) -&gt; bool:\n        \"\"\"Validate if file matches format definition.\"\"\"\n        for config in fmt.files:\n            validator = self._get_validator(config.file_type)\n            if validator is None:\n                logger.warning(\"No validator available for %s\", config.file_type.value)\n                return False\n            try:\n                if not validator(path, config, validation_result):\n                    return False\n            except FormatValidationError as e:\n                logger.debug(\"Validation failed: %s\", {str(e)})\n                return False\n        return True\n\n    def _get_validator(self, file_type: FileType):\n        \"\"\"Get appropriate validation function for file type.\"\"\"\n        validators = {\n            FileType.SQLITE: self._validate_sqlite,\n            FileType.CSV: self._validate_csv,\n            FileType.JSON: self._validate_json,\n            FileType.XML: self._validate_xml,\n        }\n        return validators.get(file_type)\n\n    def _validate_sqlite(\n        self, path: Path, config, val_result: ValidationResult\n    ) -&gt; bool:\n        \"\"\"Validate SQLite file structure.\"\"\"\n        try:\n            engine = create_engine(f\"sqlite:///{path}\")\n            inspector = inspect(engine)\n\n            # Get all tables (case sensitive)\n            actual_tables = {name: name for name in inspector.get_table_names()}\n\n            # Check each required table\n            for required_table in config.tables:\n                table_name = required_table.name\n                if table_name not in actual_tables:\n                    val_result.missing_tables.append(required_table.name)\n                    continue\n\n                # Check columns\n                columns = inspector.get_columns(table_name)\n                column_names = {col[\"name\"] for col in columns}\n\n                # Check required columns exist in file\n                required_columns = {\n                    col.source_name\n                    for col in required_table.columns\n                    if col.requirement != ColumnRequirement.OPTIONAL\n                }\n                missing = required_columns - column_names\n                if missing:\n                    val_result.missing_columns[required_table.name] = [\n                        col.source_name\n                        for col in required_table.columns\n                        if col.requirement != ColumnRequirement.OPTIONAL\n                        and col.source_name in missing\n                    ]\n\n            return not val_result.has_errors()\n\n        except FormatValidationError as e:\n            logger.debug(\"SQLite validation error: %s\", str(e))\n            return False\n\n    def _validate_csv(self, path: Path, config, val_result: ValidationResult) -&gt; bool:\n        \"\"\"Validate CSV file structure.\"\"\"\n        try:\n            # Read CSV headers only\n            df = pd.read_csv(path, nrows=0)\n            columns = {col.lower() for col in df.columns}\n\n            # CSV should have exactly one table\n            csv_table = config.tables[0]\n\n            # Check required columns\n            required_columns = {\n                col.source_name.lower() for col in csv_table.columns if col.required\n            }\n            missing = required_columns - columns\n            if missing:\n                val_result.missing_columns[\"\"] = [\n                    col\n                    for col in csv_table.columns\n                    if col.required and col.source_name.lower() in missing\n                ]\n\n            return not val_result.has_errors()\n\n        except FormatValidationError as e:\n            logger.debug(\"CSV validation error: %s\", str(e))\n            return False\n\n    def _validate_json(self, path: Path, config, val_result: ValidationResult) -&gt; bool:\n        \"\"\"Validate JSON file structure.\"\"\"\n        try:\n            with open(path, encoding=\"utf-8\") as f:\n                data = json.load(f)\n\n            for json_table in config.tables:\n                if isinstance(data, list):\n                    if not data:\n                        val_result.missing_tables.append(json_table.name)\n                        continue\n                    record = data[0]\n                else:\n                    if json_table.name not in data:\n                        val_result.missing_tables.append(json_table.name)\n                        continue\n                    record = (\n                        data[json_table.name][0]\n                        if isinstance(data[json_table.name], list)\n                        else data[json_table.name]\n                    )\n\n                # Check required fields\n                fields = {k.lower() for k in record.keys()}\n                required_fields = {\n                    col.source_name.lower()\n                    for col in json_table.columns\n                    if col.required\n                }\n                missing = required_fields - fields\n                if missing:\n                    val_result.missing_columns[json_table.name] = [\n                        col\n                        for col in json_table.columns\n                        if col.required and col.source_name.lower() in missing\n                    ]\n\n            return not val_result.has_errors()\n\n        except FormatValidationError as e:\n            logger.debug(\"JSON validation error: %s\", str(e))\n            return False\n\n    def _validate_xml(self, path: Path, config, val_result: ValidationResult) -&gt; bool:\n        \"\"\"Validate XML file structure.\"\"\"\n        try:\n            tree = ET.parse(path)\n            root = tree.getroot()\n\n            for xml_table in config.tables:\n                elements = root.findall(f\".//{xml_table.name}\")\n                if not elements:\n                    val_result.missing_tables.append(xml_table.name)\n                    continue\n\n                # Check first element\n                element = elements[0]\n                fields = set()\n                fields.update(element.attrib.keys())\n                fields.update(child.tag for child in element)\n                fields = {f.lower() for f in fields}\n\n                required_fields = {\n                    col.source_name.lower() for col in xml_table.columns if col.required\n                }\n                missing = required_fields - fields\n                if missing:\n                    val_result.missing_columns[xml_table.name] = [\n                        col\n                        for col in xml_table.columns\n                        if col.required and col.source_name.lower() in missing\n                    ]\n\n            return not val_result.has_errors()\n\n        except FormatValidationError as e:\n            logger.debug(\"XML validation error: %s\", str(e))\n            return False\n</code></pre>"},{"location":"api/file_parser/#src.file_parser.format_detector.FormatDetector.__init__","title":"<code>__init__(format_registry)</code>","text":"<p>Initialize detector with format registry.</p> Source code in <code>src/file_parser/format_detector.py</code> <pre><code>def __init__(self, format_registry: FormatRegistry):\n    \"\"\"Initialize detector with format registry.\"\"\"\n    self._registry = format_registry\n</code></pre>"},{"location":"api/file_parser/#src.file_parser.format_detector.FormatDetector.detect_format","title":"<code>detect_format(path)</code>","text":"<p>Detect format of provided file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <p>Path to the file to check</p> required <p>Returns:</p> Type Description <code>Tuple[Optional[DeviceFormat], Optional[str], Dict[str, ValidationResult]]</code> <p>Tuple containing: - Matched format (or None) - Error message (or None) - Dictionary of validation results per format tried</p> Example <p>detector = FormatDetector(registry) fmt, error, results = detector.detect_format(Path(\"data.sqlite\")) if fmt: ...     print(f\"Matched format: {fmt.name}\") ... else: ...     print(f\"No match: {error}\")</p> Source code in <code>src/file_parser/format_detector.py</code> <pre><code>def detect_format(\n    self, path: Path\n) -&gt; Tuple[Optional[DeviceFormat], Optional[str], Dict[str, ValidationResult]]:\n    \"\"\"Detect format of provided file.\n\n    Args:\n        file_path: Path to the file to check\n\n    Returns:\n        Tuple containing:\n            - Matched format (or None)\n            - Error message (or None)\n            - Dictionary of validation results per format tried\n\n    Example:\n        &gt;&gt;&gt; detector = FormatDetector(registry)\n        &gt;&gt;&gt; fmt, error, results = detector.detect_format(Path(\"data.sqlite\"))\n        &gt;&gt;&gt; if fmt:\n        ...     print(f\"Matched format: {fmt.name}\")\n        ... else:\n        ...     print(f\"No match: {error}\")\n    \"\"\"\n    logger.debug(\"Starting format detection for: %s\", path)\n    val_results = {}\n\n    try:\n        # Validate file exists and is readable\n        if not self._validate_file_exists(path):\n            return None, f\"File not found or not accessible: {path}\", {}\n\n        # Get potential formats based on file extension\n        potential_formats = self._registry.get_formats_for_file(path)\n        if not potential_formats:\n            return None, f\"No formats available for {path.suffix} files\", {}\n\n        # Try each format\n        for fmt in potential_formats:\n            try:\n                val_test_result = ValidationResult()\n                if self._validate_format(path, fmt, val_test_result):\n                    logger.debug(\"Successfully matched format: %s\", fmt.name)\n                    return fmt, None, val_results\n                val_results[fmt.name] = val_test_result\n            except FormatValidationError as e:\n                logger.debug(\"Error validating format %s: %s\", fmt.name, str(e))\n                continue\n\n        return None, \"No matching format found\", val_results\n\n    except FileAccessError as e:\n        logger.error(\"Unexpected error during format detection: %s\", str(e))\n        return None, f\"Detection error: {str(e)}\", {}\n</code></pre>"},{"location":"api/file_parser/#src.file_parser.format_detector.ValidationResult","title":"<code>ValidationResult</code>","text":"<p>Container for structure validation results.</p> Source code in <code>src/file_parser/format_detector.py</code> <pre><code>class ValidationResult:\n    \"\"\"Container for structure validation results.\"\"\"\n\n    def __init__(self):\n        self.missing_tables: List[str] = []\n        self.missing_columns: Dict[str, List[str]] = {}  # table: [columns]\n\n    def has_errors(self) -&gt; bool:\n        \"\"\"Check if any validation errors exist.\"\"\"\n        return bool(self.missing_tables or self.missing_columns)\n\n    def __str__(self) -&gt; str:\n        \"\"\"Format validation errors as string.\"\"\"\n        errors = []\n        if self.missing_tables:\n            errors.append(f\"Missing tables: {', '.join(self.missing_tables)}\")\n        if self.missing_columns:\n            for current_table, columns in self.missing_columns.items():\n                errors.append(\n                    f\"Missing required columns in {current_table}: {', '.join(columns)}\"\n                )\n        return \"\\n\".join(errors)\n</code></pre>"},{"location":"api/file_parser/#src.file_parser.format_detector.ValidationResult.__str__","title":"<code>__str__()</code>","text":"<p>Format validation errors as string.</p> Source code in <code>src/file_parser/format_detector.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"Format validation errors as string.\"\"\"\n    errors = []\n    if self.missing_tables:\n        errors.append(f\"Missing tables: {', '.join(self.missing_tables)}\")\n    if self.missing_columns:\n        for current_table, columns in self.missing_columns.items():\n            errors.append(\n                f\"Missing required columns in {current_table}: {', '.join(columns)}\"\n            )\n    return \"\\n\".join(errors)\n</code></pre>"},{"location":"api/file_parser/#src.file_parser.format_detector.ValidationResult.has_errors","title":"<code>has_errors()</code>","text":"<p>Check if any validation errors exist.</p> Source code in <code>src/file_parser/format_detector.py</code> <pre><code>def has_errors(self) -&gt; bool:\n    \"\"\"Check if any validation errors exist.\"\"\"\n    return bool(self.missing_tables or self.missing_columns)\n</code></pre>"},{"location":"api/processors/","title":"Processors API","text":""},{"location":"api/processors/#complete-reference","title":"Complete Reference","text":""},{"location":"api/processors/#src.processors","title":"<code>src.processors</code>","text":"<p>Initialize processors package and register all processors.</p>"},{"location":"api/processors/#src.processors.CGMProcessor","title":"<code>CGMProcessor</code>","text":"<p>               Bases: <code>BaseTypeProcessor</code></p> <p>Processes CGM data with validation and cleaning.</p> Source code in <code>src/processors/cgm.py</code> <pre><code>@DataProcessor.register_processor(DataType.CGM)\nclass CGMProcessor(BaseTypeProcessor):\n    \"\"\"Processes CGM data with validation and cleaning.\"\"\"\n\n    def process_type(\n        self,\n        columns: List[ColumnData],\n        interpolation_limit: int = 4,\n    ) -&gt; ProcessedTypeData:\n        \"\"\"Process all CGM data from various sources.\n\n        Args:\n            columns: List of ColumnData containing all CGM data columns\n            interpolation_limit: Maximum number of consecutive missing values to interpolate\n                               (default: 4, equivalent to 20 minutes at 5-min intervals)\n\n        Returns:\n            ProcessedTypeData containing combined and cleaned CGM data\n        \"\"\"\n        processing_notes = []\n\n        try:\n            # Validate we have at least one primary column\n            if not any(col.is_primary for col in columns):\n                raise ProcessingError(\"No primary CGM column found\")\n\n            # Sort columns to ensure primary is first\n            sorted_columns = sorted(columns, key=lambda x: (not x.is_primary))\n\n            # Create initial dataframe\n            combined_df = pd.DataFrame(index=pd.DatetimeIndex([]))\n            column_units = {}\n\n            # First pass - combine all data and convert units\n            for idx, col_data in enumerate(sorted_columns):\n                # Convert to mg/dL if needed\n                df = col_data.dataframe.copy()\n                if col_data.unit == Unit.MMOL:\n                    df[\"value\"] = df[\"value\"] * 18.0182\n                    processing_notes.append(\n                        f\"Converted CGM column {idx + 1} from {Unit.MMOL.value} to {Unit.MGDL.value}\"\n                    )\n\n                # Generate column name\n                new_name = self._generate_column_name(\n                    DataType.CGM, col_data.is_primary, idx\n                )\n                df.columns = [new_name]\n\n                # Merge with existing data\n                if combined_df.empty:\n                    combined_df = df\n                else:\n                    combined_df = combined_df.join(df, how=\"outer\")\n\n                column_units[new_name] = col_data.unit\n\n            # Round all timestamps to nearest 5 minute interval\n            combined_df.index = combined_df.index.round(\"5min\")\n\n            # Handle duplicate times by taking mean\n            combined_df = combined_df.groupby(level=0).mean()\n\n            # Create complete 5-minute interval index\n            full_index = pd.date_range(\n                start=combined_df.index.min(), end=combined_df.index.max(), freq=\"5min\"\n            )\n\n            # Reindex to include all intervals\n            combined_df = combined_df.reindex(full_index)\n\n            # Get primary column name\n            primary_col = next(col for col in combined_df.columns if \"primary\" in col)\n\n            # Create missing flags for each column\n            missing_flags = pd.DataFrame(index=combined_df.index)\n            missing_flags[\"missing\"] = combined_df[primary_col].isna()\n\n            # Handle interpolation for each column\n            for col in combined_df.columns:\n                # Create groups of consecutive missing values\n                gap_groups = (~combined_df[col].isna()).cumsum()\n\n                # Within each False group (where missing=True), count the group size\n                gap_size = (\n                    combined_df[combined_df[col].isna()].groupby(gap_groups).size()\n                )\n\n                # Identify gap groups that are larger than interpolation_limit\n                large_gaps = gap_size[gap_size &gt; interpolation_limit].index\n\n                # Interpolate all gaps initially\n                combined_df[col] = combined_df[col].interpolate(\n                    method=\"linear\",\n                    limit=interpolation_limit,\n                    limit_direction=\"forward\",\n                )\n\n                # Reset interpolated values back to NaN for large gaps\n                for gap_group in large_gaps:\n                    mask = (gap_groups == gap_group) &amp; combined_df[col].isna()\n                    combined_df.loc[mask, col] = np.nan\n\n                # Clip values to valid range\n                combined_df[col] = combined_df[col].clip(lower=39.64, upper=360.36)\n\n            # Create mmol/L columns for each mg/dL column\n            for col in combined_df.columns.copy():\n                mmol_col = f\"{col}_mmol\"\n                combined_df[mmol_col] = combined_df[col] * 0.0555\n                column_units[mmol_col] = Unit.MMOL\n                column_units[col] = Unit.MGDL\n\n            # Add the missing flags\n            combined_df[\"missing\"] = missing_flags[\"missing\"]\n\n            # Track stats about the processed data\n            total_readings = len(combined_df)\n            missing_primary = combined_df[\"missing\"].sum()\n            total_na = combined_df[\"cgm_primary\"].isna().sum()\n            initial_completeness_percent = (\n                (total_readings - missing_primary) / total_readings\n            ) * 100\n            remaining_completeness_percent = (\n                (total_readings - total_na) / total_readings\n            ) * 100\n            processing_notes.extend(\n                [\n                    f\"Processed {total_readings} total CGM readings\",\n                    f\"Found {missing_primary} missing or interpolated values in primary data\",\n                    f\"Found {total_na} missing values after interpolation\",\n                    f\"Initial CGM dataset completeness: {initial_completeness_percent:.2f}%\",\n                    f\"CGM completeness after interpolation: {remaining_completeness_percent:.2f}%\",\n                ]\n            )\n\n            return ProcessedTypeData(\n                dataframe=combined_df,\n                source_units=column_units,\n                processing_notes=processing_notes,\n            )\n\n        except Exception as e:\n            raise ProcessingError(f\"Error processing CGM data: {str(e)}\") from e\n</code></pre>"},{"location":"api/processors/#src.processors.CGMProcessor.process_type","title":"<code>process_type(columns, interpolation_limit=4)</code>","text":"<p>Process all CGM data from various sources.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>List[ColumnData]</code> <p>List of ColumnData containing all CGM data columns</p> required <code>interpolation_limit</code> <code>int</code> <p>Maximum number of consecutive missing values to interpolate                (default: 4, equivalent to 20 minutes at 5-min intervals)</p> <code>4</code> <p>Returns:</p> Type Description <code>ProcessedTypeData</code> <p>ProcessedTypeData containing combined and cleaned CGM data</p> Source code in <code>src/processors/cgm.py</code> <pre><code>def process_type(\n    self,\n    columns: List[ColumnData],\n    interpolation_limit: int = 4,\n) -&gt; ProcessedTypeData:\n    \"\"\"Process all CGM data from various sources.\n\n    Args:\n        columns: List of ColumnData containing all CGM data columns\n        interpolation_limit: Maximum number of consecutive missing values to interpolate\n                           (default: 4, equivalent to 20 minutes at 5-min intervals)\n\n    Returns:\n        ProcessedTypeData containing combined and cleaned CGM data\n    \"\"\"\n    processing_notes = []\n\n    try:\n        # Validate we have at least one primary column\n        if not any(col.is_primary for col in columns):\n            raise ProcessingError(\"No primary CGM column found\")\n\n        # Sort columns to ensure primary is first\n        sorted_columns = sorted(columns, key=lambda x: (not x.is_primary))\n\n        # Create initial dataframe\n        combined_df = pd.DataFrame(index=pd.DatetimeIndex([]))\n        column_units = {}\n\n        # First pass - combine all data and convert units\n        for idx, col_data in enumerate(sorted_columns):\n            # Convert to mg/dL if needed\n            df = col_data.dataframe.copy()\n            if col_data.unit == Unit.MMOL:\n                df[\"value\"] = df[\"value\"] * 18.0182\n                processing_notes.append(\n                    f\"Converted CGM column {idx + 1} from {Unit.MMOL.value} to {Unit.MGDL.value}\"\n                )\n\n            # Generate column name\n            new_name = self._generate_column_name(\n                DataType.CGM, col_data.is_primary, idx\n            )\n            df.columns = [new_name]\n\n            # Merge with existing data\n            if combined_df.empty:\n                combined_df = df\n            else:\n                combined_df = combined_df.join(df, how=\"outer\")\n\n            column_units[new_name] = col_data.unit\n\n        # Round all timestamps to nearest 5 minute interval\n        combined_df.index = combined_df.index.round(\"5min\")\n\n        # Handle duplicate times by taking mean\n        combined_df = combined_df.groupby(level=0).mean()\n\n        # Create complete 5-minute interval index\n        full_index = pd.date_range(\n            start=combined_df.index.min(), end=combined_df.index.max(), freq=\"5min\"\n        )\n\n        # Reindex to include all intervals\n        combined_df = combined_df.reindex(full_index)\n\n        # Get primary column name\n        primary_col = next(col for col in combined_df.columns if \"primary\" in col)\n\n        # Create missing flags for each column\n        missing_flags = pd.DataFrame(index=combined_df.index)\n        missing_flags[\"missing\"] = combined_df[primary_col].isna()\n\n        # Handle interpolation for each column\n        for col in combined_df.columns:\n            # Create groups of consecutive missing values\n            gap_groups = (~combined_df[col].isna()).cumsum()\n\n            # Within each False group (where missing=True), count the group size\n            gap_size = (\n                combined_df[combined_df[col].isna()].groupby(gap_groups).size()\n            )\n\n            # Identify gap groups that are larger than interpolation_limit\n            large_gaps = gap_size[gap_size &gt; interpolation_limit].index\n\n            # Interpolate all gaps initially\n            combined_df[col] = combined_df[col].interpolate(\n                method=\"linear\",\n                limit=interpolation_limit,\n                limit_direction=\"forward\",\n            )\n\n            # Reset interpolated values back to NaN for large gaps\n            for gap_group in large_gaps:\n                mask = (gap_groups == gap_group) &amp; combined_df[col].isna()\n                combined_df.loc[mask, col] = np.nan\n\n            # Clip values to valid range\n            combined_df[col] = combined_df[col].clip(lower=39.64, upper=360.36)\n\n        # Create mmol/L columns for each mg/dL column\n        for col in combined_df.columns.copy():\n            mmol_col = f\"{col}_mmol\"\n            combined_df[mmol_col] = combined_df[col] * 0.0555\n            column_units[mmol_col] = Unit.MMOL\n            column_units[col] = Unit.MGDL\n\n        # Add the missing flags\n        combined_df[\"missing\"] = missing_flags[\"missing\"]\n\n        # Track stats about the processed data\n        total_readings = len(combined_df)\n        missing_primary = combined_df[\"missing\"].sum()\n        total_na = combined_df[\"cgm_primary\"].isna().sum()\n        initial_completeness_percent = (\n            (total_readings - missing_primary) / total_readings\n        ) * 100\n        remaining_completeness_percent = (\n            (total_readings - total_na) / total_readings\n        ) * 100\n        processing_notes.extend(\n            [\n                f\"Processed {total_readings} total CGM readings\",\n                f\"Found {missing_primary} missing or interpolated values in primary data\",\n                f\"Found {total_na} missing values after interpolation\",\n                f\"Initial CGM dataset completeness: {initial_completeness_percent:.2f}%\",\n                f\"CGM completeness after interpolation: {remaining_completeness_percent:.2f}%\",\n            ]\n        )\n\n        return ProcessedTypeData(\n            dataframe=combined_df,\n            source_units=column_units,\n            processing_notes=processing_notes,\n        )\n\n    except Exception as e:\n        raise ProcessingError(f\"Error processing CGM data: {str(e)}\") from e\n</code></pre>"},{"location":"api/processors/#src.processors.CarbsProcessor","title":"<code>CarbsProcessor</code>","text":"<p>               Bases: <code>BaseTypeProcessor</code></p> <p>Processes carbohydrate intake data with validation and cleaning.</p> Source code in <code>src/processors/carbs.py</code> <pre><code>@DataProcessor.register_processor(DataType.CARBS)\nclass CarbsProcessor(BaseTypeProcessor):\n    \"\"\"Processes carbohydrate intake data with validation and cleaning.\"\"\"\n\n    def process_type(\n        self,\n        columns: List[ColumnData],\n    ) -&gt; ProcessedTypeData:\n        \"\"\"Process all carbohydrate data from various sources.\n\n        Args:\n            columns: List of ColumnData containing all carb data columns\n\n        Returns:\n            ProcessedTypeData containing combined and cleaned carb data\n        \"\"\"\n        processing_notes = []\n\n        try:\n            # Validate we have at least one primary column\n            if not any(col.is_primary for col in columns):\n                raise ProcessingError(\"No primary carbohydrate column found\")\n\n            # Sort columns to ensure primary is first\n            sorted_columns = sorted(columns, key=lambda x: (not x.is_primary))\n\n            # Combine all columns with standardized names\n            combined_df, column_units = self._combine_and_rename_columns(\n                sorted_columns, DataType.CARBS\n            )\n\n            if combined_df.empty:\n                raise ProcessingError(\"No carbohydrate data to process\")\n\n            # Log what we're processing\n            processing_notes.append(\n                f\"Processing {len(combined_df.columns)} carb columns: {', '.join(combined_df.columns)}\"\n            )\n\n            # Track original row count\n            original_count = len(combined_df)\n\n            # Process each carb column\n            for col in combined_df.columns:\n                # Keep only rows where carbs is &gt;= 1.0 grams\n                mask = combined_df[col] &gt;= 1.0\n                combined_df.loc[~mask, col] = None\n\n                filtered_count = mask.sum()\n                processing_notes.append(\n                    f\"Column {col}: Kept {filtered_count} entries \u22651g \"\n                    f\"({filtered_count / original_count * 100:.1f}%)\"\n                )\n\n            # Drop rows where all values are null (no significant carbs in any column)\n            combined_df = combined_df.dropna(how=\"all\")\n\n            # Handle duplicate timestamps by keeping the first occurrence\n            duplicates = combined_df.index.duplicated()\n            if duplicates.any():\n                dup_count = duplicates.sum()\n                processing_notes.append(f\"Removed {dup_count} duplicate timestamps\")\n                combined_df = combined_df[~duplicates]\n\n            # Final stats\n            processing_notes.append(\n                f\"Final dataset contains {len(combined_df)} entries \"\n                f\"from {original_count} original records\"\n            )\n\n            return ProcessedTypeData(\n                dataframe=combined_df,\n                source_units=column_units,\n                processing_notes=processing_notes,\n            )\n\n        except Exception as e:\n            raise ProcessingError(\n                f\"Error processing carbohydrate data: {str(e)}\"\n            ) from e\n</code></pre>"},{"location":"api/processors/#src.processors.CarbsProcessor.process_type","title":"<code>process_type(columns)</code>","text":"<p>Process all carbohydrate data from various sources.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>List[ColumnData]</code> <p>List of ColumnData containing all carb data columns</p> required <p>Returns:</p> Type Description <code>ProcessedTypeData</code> <p>ProcessedTypeData containing combined and cleaned carb data</p> Source code in <code>src/processors/carbs.py</code> <pre><code>def process_type(\n    self,\n    columns: List[ColumnData],\n) -&gt; ProcessedTypeData:\n    \"\"\"Process all carbohydrate data from various sources.\n\n    Args:\n        columns: List of ColumnData containing all carb data columns\n\n    Returns:\n        ProcessedTypeData containing combined and cleaned carb data\n    \"\"\"\n    processing_notes = []\n\n    try:\n        # Validate we have at least one primary column\n        if not any(col.is_primary for col in columns):\n            raise ProcessingError(\"No primary carbohydrate column found\")\n\n        # Sort columns to ensure primary is first\n        sorted_columns = sorted(columns, key=lambda x: (not x.is_primary))\n\n        # Combine all columns with standardized names\n        combined_df, column_units = self._combine_and_rename_columns(\n            sorted_columns, DataType.CARBS\n        )\n\n        if combined_df.empty:\n            raise ProcessingError(\"No carbohydrate data to process\")\n\n        # Log what we're processing\n        processing_notes.append(\n            f\"Processing {len(combined_df.columns)} carb columns: {', '.join(combined_df.columns)}\"\n        )\n\n        # Track original row count\n        original_count = len(combined_df)\n\n        # Process each carb column\n        for col in combined_df.columns:\n            # Keep only rows where carbs is &gt;= 1.0 grams\n            mask = combined_df[col] &gt;= 1.0\n            combined_df.loc[~mask, col] = None\n\n            filtered_count = mask.sum()\n            processing_notes.append(\n                f\"Column {col}: Kept {filtered_count} entries \u22651g \"\n                f\"({filtered_count / original_count * 100:.1f}%)\"\n            )\n\n        # Drop rows where all values are null (no significant carbs in any column)\n        combined_df = combined_df.dropna(how=\"all\")\n\n        # Handle duplicate timestamps by keeping the first occurrence\n        duplicates = combined_df.index.duplicated()\n        if duplicates.any():\n            dup_count = duplicates.sum()\n            processing_notes.append(f\"Removed {dup_count} duplicate timestamps\")\n            combined_df = combined_df[~duplicates]\n\n        # Final stats\n        processing_notes.append(\n            f\"Final dataset contains {len(combined_df)} entries \"\n            f\"from {original_count} original records\"\n        )\n\n        return ProcessedTypeData(\n            dataframe=combined_df,\n            source_units=column_units,\n            processing_notes=processing_notes,\n        )\n\n    except Exception as e:\n        raise ProcessingError(\n            f\"Error processing carbohydrate data: {str(e)}\"\n        ) from e\n</code></pre>"},{"location":"api/processors/#src.processors.DataProcessor","title":"<code>DataProcessor</code>","text":"<p>Main processor class that handles processing of all data types.</p> Source code in <code>src/processors/base.py</code> <pre><code>class DataProcessor:\n    \"\"\"Main processor class that handles processing of all data types.\"\"\"\n\n    _type_processors: Dict[DataType, Type[BaseTypeProcessor]] = {}\n\n    @staticmethod\n    def create_table_configs(detected_format) -&gt; Dict[str, TableStructure]:\n        \"\"\"\n        Creates a table configuration dictionary from detected format.\n\n        Args:\n            detected_format: Format object containing files and their table configurations\n\n        Returns:\n            Dict[str, TableStructure]: Dictionary mapping table names to their structures\n        \"\"\"\n        try:\n            return {\n                table.name: table\n                for file_config in detected_format.files\n                for table in file_config.tables\n            }\n        except Exception as e:\n            logger.error(\"Failed to create table configurations: %s\", str(e))\n            raise ProcessingError(\"Failed to create table configurations\") from e\n\n    def process_tables(\n        self,\n        table_data: Dict[str, TableData],\n        detected_format: DeviceFormat,\n        interpolation_limit: Optional[\n            Any\n        ] = None,  # Optional parameter for CGM processor\n        bolus_limit: Optional[Any] = None,  # Optional parameters for insulin processor\n        max_dose: Optional[Any] = None,\n    ) -&gt; Dict[DataType, ProcessedTypeData]:\n        \"\"\"\n        Process all tables according to their configuration.\n\n        Args:\n            table_data: Dictionary mapping table names to their data\n            detected_format: Format object containing table configurations\n            interpolation_limit: Max length of gaps to interpolate\n            bolus_limit: Maximum insulin dose to be classified as bolus(default = 8)\n            max_limit: Maximum insulin dose - all over will be discarded\n\n        Returns:\n            Dict[DataType, ProcessedTypeData]: Processed data organized by type\n        \"\"\"\n        table_configs = self.create_table_configs(detected_format)\n\n        # Rest of your existing process_tables implementation\n        type_data: Dict[DataType, List[ColumnData]] = {}\n\n        for table_name, data in table_data.items():\n            config = table_configs[table_name]\n\n            # Group columns by data type\n            for column in config.columns:\n                if column.data_type:\n                    # Include insulin meta data with insulin data\n                    target_type = (\n                        DataType.INSULIN\n                        if column.data_type == DataType.INSULIN_META\n                        else column.data_type\n                    )\n\n                    df_subset = data.dataframe[[column.source_name]].copy()\n                    df_subset.columns = [\"value\"]\n\n                    column_data = ColumnData(\n                        dataframe=df_subset,\n                        unit=column.unit,\n                        config=column,\n                        is_primary=column.is_primary,\n                    )\n\n                    if target_type not in type_data:\n                        type_data[target_type] = []\n\n                    type_data[target_type].append(column_data)\n\n        # Process each data type\n        results = {}\n        for data_type, columns in type_data.items():\n            try:\n                processor = self.get_processor_for_type(data_type)\n\n                # Inject optional parameters based on processor type\n                if data_type == DataType.CGM and interpolation_limit is not None:\n                    result = processor.process_type(columns, interpolation_limit)\n                elif data_type == DataType.INSULIN:\n                    result = processor.process_type(columns, bolus_limit, max_dose)\n                else:\n                    result = processor.process_type(columns)\n\n                if not result.dataframe.empty:\n                    results[data_type] = result\n\n                    col_count = len(columns)\n                    primary_count = sum(1 for c in columns if c.is_primary)\n                    logger.info(\n                        \"    \\u2713 Processed %s: %d primary and %d secondary columns\",\n                        data_type.name,\n                        primary_count,\n                        col_count - primary_count,\n                    )\n\n            except ProcessingError as e:\n                logger.error(\"Error processing %s: %s\", data_type, str(e))\n                continue\n\n        return results\n\n    @classmethod\n    def register_processor(cls, data_type: DataType):\n        \"\"\"Register a processor class for a specific data type.\"\"\"\n\n        def wrapper(processor_cls: Type[BaseTypeProcessor]):\n            cls._type_processors[data_type] = processor_cls\n            return processor_cls\n\n        return wrapper\n\n    def get_processor_for_type(self, data_type: DataType) -&gt; BaseTypeProcessor:\n        \"\"\"Get appropriate processor instance for the data type.\"\"\"\n        processor_cls = self._type_processors.get(data_type)\n        if processor_cls is None:\n            raise ProcessingError(\n                f\"No processor registered for data type: {data_type.value}\"\n            )\n        return processor_cls()\n</code></pre>"},{"location":"api/processors/#src.processors.DataProcessor.create_table_configs","title":"<code>create_table_configs(detected_format)</code>  <code>staticmethod</code>","text":"<p>Creates a table configuration dictionary from detected format.</p> <p>Parameters:</p> Name Type Description Default <code>detected_format</code> <p>Format object containing files and their table configurations</p> required <p>Returns:</p> Type Description <code>Dict[str, TableStructure]</code> <p>Dict[str, TableStructure]: Dictionary mapping table names to their structures</p> Source code in <code>src/processors/base.py</code> <pre><code>@staticmethod\ndef create_table_configs(detected_format) -&gt; Dict[str, TableStructure]:\n    \"\"\"\n    Creates a table configuration dictionary from detected format.\n\n    Args:\n        detected_format: Format object containing files and their table configurations\n\n    Returns:\n        Dict[str, TableStructure]: Dictionary mapping table names to their structures\n    \"\"\"\n    try:\n        return {\n            table.name: table\n            for file_config in detected_format.files\n            for table in file_config.tables\n        }\n    except Exception as e:\n        logger.error(\"Failed to create table configurations: %s\", str(e))\n        raise ProcessingError(\"Failed to create table configurations\") from e\n</code></pre>"},{"location":"api/processors/#src.processors.DataProcessor.get_processor_for_type","title":"<code>get_processor_for_type(data_type)</code>","text":"<p>Get appropriate processor instance for the data type.</p> Source code in <code>src/processors/base.py</code> <pre><code>def get_processor_for_type(self, data_type: DataType) -&gt; BaseTypeProcessor:\n    \"\"\"Get appropriate processor instance for the data type.\"\"\"\n    processor_cls = self._type_processors.get(data_type)\n    if processor_cls is None:\n        raise ProcessingError(\n            f\"No processor registered for data type: {data_type.value}\"\n        )\n    return processor_cls()\n</code></pre>"},{"location":"api/processors/#src.processors.DataProcessor.process_tables","title":"<code>process_tables(table_data, detected_format, interpolation_limit=None, bolus_limit=None, max_dose=None)</code>","text":"<p>Process all tables according to their configuration.</p> <p>Parameters:</p> Name Type Description Default <code>table_data</code> <code>Dict[str, TableData]</code> <p>Dictionary mapping table names to their data</p> required <code>detected_format</code> <code>DeviceFormat</code> <p>Format object containing table configurations</p> required <code>interpolation_limit</code> <code>Optional[Any]</code> <p>Max length of gaps to interpolate</p> <code>None</code> <code>bolus_limit</code> <code>Optional[Any]</code> <p>Maximum insulin dose to be classified as bolus(default = 8)</p> <code>None</code> <code>max_limit</code> <p>Maximum insulin dose - all over will be discarded</p> required <p>Returns:</p> Type Description <code>Dict[DataType, ProcessedTypeData]</code> <p>Dict[DataType, ProcessedTypeData]: Processed data organized by type</p> Source code in <code>src/processors/base.py</code> <pre><code>def process_tables(\n    self,\n    table_data: Dict[str, TableData],\n    detected_format: DeviceFormat,\n    interpolation_limit: Optional[\n        Any\n    ] = None,  # Optional parameter for CGM processor\n    bolus_limit: Optional[Any] = None,  # Optional parameters for insulin processor\n    max_dose: Optional[Any] = None,\n) -&gt; Dict[DataType, ProcessedTypeData]:\n    \"\"\"\n    Process all tables according to their configuration.\n\n    Args:\n        table_data: Dictionary mapping table names to their data\n        detected_format: Format object containing table configurations\n        interpolation_limit: Max length of gaps to interpolate\n        bolus_limit: Maximum insulin dose to be classified as bolus(default = 8)\n        max_limit: Maximum insulin dose - all over will be discarded\n\n    Returns:\n        Dict[DataType, ProcessedTypeData]: Processed data organized by type\n    \"\"\"\n    table_configs = self.create_table_configs(detected_format)\n\n    # Rest of your existing process_tables implementation\n    type_data: Dict[DataType, List[ColumnData]] = {}\n\n    for table_name, data in table_data.items():\n        config = table_configs[table_name]\n\n        # Group columns by data type\n        for column in config.columns:\n            if column.data_type:\n                # Include insulin meta data with insulin data\n                target_type = (\n                    DataType.INSULIN\n                    if column.data_type == DataType.INSULIN_META\n                    else column.data_type\n                )\n\n                df_subset = data.dataframe[[column.source_name]].copy()\n                df_subset.columns = [\"value\"]\n\n                column_data = ColumnData(\n                    dataframe=df_subset,\n                    unit=column.unit,\n                    config=column,\n                    is_primary=column.is_primary,\n                )\n\n                if target_type not in type_data:\n                    type_data[target_type] = []\n\n                type_data[target_type].append(column_data)\n\n    # Process each data type\n    results = {}\n    for data_type, columns in type_data.items():\n        try:\n            processor = self.get_processor_for_type(data_type)\n\n            # Inject optional parameters based on processor type\n            if data_type == DataType.CGM and interpolation_limit is not None:\n                result = processor.process_type(columns, interpolation_limit)\n            elif data_type == DataType.INSULIN:\n                result = processor.process_type(columns, bolus_limit, max_dose)\n            else:\n                result = processor.process_type(columns)\n\n            if not result.dataframe.empty:\n                results[data_type] = result\n\n                col_count = len(columns)\n                primary_count = sum(1 for c in columns if c.is_primary)\n                logger.info(\n                    \"    \\u2713 Processed %s: %d primary and %d secondary columns\",\n                    data_type.name,\n                    primary_count,\n                    col_count - primary_count,\n                )\n\n        except ProcessingError as e:\n            logger.error(\"Error processing %s: %s\", data_type, str(e))\n            continue\n\n    return results\n</code></pre>"},{"location":"api/processors/#src.processors.DataProcessor.register_processor","title":"<code>register_processor(data_type)</code>  <code>classmethod</code>","text":"<p>Register a processor class for a specific data type.</p> Source code in <code>src/processors/base.py</code> <pre><code>@classmethod\ndef register_processor(cls, data_type: DataType):\n    \"\"\"Register a processor class for a specific data type.\"\"\"\n\n    def wrapper(processor_cls: Type[BaseTypeProcessor]):\n        cls._type_processors[data_type] = processor_cls\n        return processor_cls\n\n    return wrapper\n</code></pre>"},{"location":"api/processors/#src.processors.InsulinProcessor","title":"<code>InsulinProcessor</code>","text":"<p>               Bases: <code>BaseTypeProcessor</code></p> <p>Processes insulin dose data with classification from meta data if available.</p> Source code in <code>src/processors/insulin.py</code> <pre><code>@DataProcessor.register_processor(DataType.INSULIN)\nclass InsulinProcessor(BaseTypeProcessor):\n    \"\"\"Processes insulin dose data with classification from meta data if available.\"\"\"\n\n    def _extract_meta_info(self, meta_value: str) -&gt; Tuple[bool, bool, Optional[str]]:\n        \"\"\"Extract insulin type information from meta JSON.\n\n        Returns:\n            Tuple of (is_bolus, is_basal, insulin_type)\n        \"\"\"\n        try:\n            meta_data = json.loads(meta_value)\n            if meta_data and isinstance(meta_data, list):\n                insulin = meta_data[0].get(\"insulin\", \"\").lower()\n                if \"novorapid\" in insulin:\n                    return True, False, \"novorapid\"\n                if \"levemir\" in insulin:\n                    return False, True, \"levemir\"\n        except (json.JSONDecodeError, IndexError, KeyError, AttributeError):\n            pass\n\n        return False, False, None\n\n    def process_type(\n        self,\n        columns: List[ColumnData],\n        bolus_limit: float = 8.0,\n        max_limit: float = 15.0,\n    ) -&gt; ProcessedTypeData:\n        \"\"\"Process insulin data and classify doses.\n\n        Args:\n            columns: List of ColumnData containing insulin data and metadata\n            bolus_limit: Maximum insulin units to classify as bolus\n            max_limit: Maximum valid insulin dose\n        \"\"\"\n        processing_notes = []\n\n        try:\n            # Find insulin dose and meta columns\n            dose_cols = [col for col in columns if col.data_type == DataType.INSULIN]\n            meta_cols = [\n                col for col in columns if col.data_type == DataType.INSULIN_META\n            ]\n\n            if not any(col.is_primary for col in dose_cols):\n                raise ProcessingError(\"No primary insulin dose column found\")\n\n            # Initialize result DataFrame with dose data\n            result_df = pd.DataFrame()\n            result_units = {}\n\n            # Process dose data first\n            for col in dose_cols:\n                df = col.dataframe.copy()\n\n                # Keep only positive doses\n                valid_mask = df[\"value\"] &gt; 0.0\n                df = df[valid_mask]\n\n                if len(df) &gt; 0:\n                    processing_notes.append(f\"Found {len(df)} positive doses\")\n\n                    # Add dose column\n                    result_df[\"dose\"] = df[\"value\"]\n                    result_units[\"dose\"] = col.unit\n\n                    # Initial classification based on dose\n                    result_df[\"is_bolus\"] = df[\"value\"] &lt;= bolus_limit\n                    result_df[\"is_basal\"] = (df[\"value\"] &gt; bolus_limit) &amp; (\n                        df[\"value\"] &lt;= max_limit\n                    )\n                    result_df[\"type\"] = \"\"  # Will be filled by metadata if available\n\n                    # Track classification stats\n                    processing_notes.extend(\n                        [\n                            \"Initial dose-based classification:\",\n                            f\"- {result_df['is_bolus'].sum()} doses classified as bolus (\u2264{bolus_limit}U)\",\n                            f\"- {result_df['is_basal'].sum()} doses classified as basal (&gt;{bolus_limit}U)\",\n                            f\"- Dropped {(df['value'] &gt; max_limit).sum()} doses exceeding {max_limit}U\",\n                        ]\n                    )\n\n            # Update classification with metadata if available\n            if meta_cols and not result_df.empty:\n                meta_updates = 0\n                for col in meta_cols:\n                    for idx, meta_value in col.dataframe[\"value\"].items():\n                        if idx in result_df.index:\n                            is_bolus, is_basal, insulin_type = self._extract_meta_info(\n                                meta_value\n                            )\n                            if insulin_type:\n                                result_df.loc[idx, \"is_bolus\"] = is_bolus\n                                result_df.loc[idx, \"is_basal\"] = is_basal\n                                result_df.loc[idx, \"type\"] = insulin_type\n                                meta_updates += 1\n\n                if meta_updates &gt; 0:\n                    processing_notes.append(\n                        f\"Updated {meta_updates} classifications using metadata\"\n                    )\n\n            # Final stats\n            processing_notes.append(\n                f\"Final dataset contains {len(result_df)} insulin records\"\n            )\n\n            return ProcessedTypeData(\n                dataframe=result_df,\n                source_units=result_units,\n                processing_notes=processing_notes,\n            )\n\n        except Exception as e:\n            raise ProcessingError(f\"Error processing insulin data: {str(e)}\") from e\n</code></pre>"},{"location":"api/processors/#src.processors.InsulinProcessor.process_type","title":"<code>process_type(columns, bolus_limit=8.0, max_limit=15.0)</code>","text":"<p>Process insulin data and classify doses.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>List[ColumnData]</code> <p>List of ColumnData containing insulin data and metadata</p> required <code>bolus_limit</code> <code>float</code> <p>Maximum insulin units to classify as bolus</p> <code>8.0</code> <code>max_limit</code> <code>float</code> <p>Maximum valid insulin dose</p> <code>15.0</code> Source code in <code>src/processors/insulin.py</code> <pre><code>def process_type(\n    self,\n    columns: List[ColumnData],\n    bolus_limit: float = 8.0,\n    max_limit: float = 15.0,\n) -&gt; ProcessedTypeData:\n    \"\"\"Process insulin data and classify doses.\n\n    Args:\n        columns: List of ColumnData containing insulin data and metadata\n        bolus_limit: Maximum insulin units to classify as bolus\n        max_limit: Maximum valid insulin dose\n    \"\"\"\n    processing_notes = []\n\n    try:\n        # Find insulin dose and meta columns\n        dose_cols = [col for col in columns if col.data_type == DataType.INSULIN]\n        meta_cols = [\n            col for col in columns if col.data_type == DataType.INSULIN_META\n        ]\n\n        if not any(col.is_primary for col in dose_cols):\n            raise ProcessingError(\"No primary insulin dose column found\")\n\n        # Initialize result DataFrame with dose data\n        result_df = pd.DataFrame()\n        result_units = {}\n\n        # Process dose data first\n        for col in dose_cols:\n            df = col.dataframe.copy()\n\n            # Keep only positive doses\n            valid_mask = df[\"value\"] &gt; 0.0\n            df = df[valid_mask]\n\n            if len(df) &gt; 0:\n                processing_notes.append(f\"Found {len(df)} positive doses\")\n\n                # Add dose column\n                result_df[\"dose\"] = df[\"value\"]\n                result_units[\"dose\"] = col.unit\n\n                # Initial classification based on dose\n                result_df[\"is_bolus\"] = df[\"value\"] &lt;= bolus_limit\n                result_df[\"is_basal\"] = (df[\"value\"] &gt; bolus_limit) &amp; (\n                    df[\"value\"] &lt;= max_limit\n                )\n                result_df[\"type\"] = \"\"  # Will be filled by metadata if available\n\n                # Track classification stats\n                processing_notes.extend(\n                    [\n                        \"Initial dose-based classification:\",\n                        f\"- {result_df['is_bolus'].sum()} doses classified as bolus (\u2264{bolus_limit}U)\",\n                        f\"- {result_df['is_basal'].sum()} doses classified as basal (&gt;{bolus_limit}U)\",\n                        f\"- Dropped {(df['value'] &gt; max_limit).sum()} doses exceeding {max_limit}U\",\n                    ]\n                )\n\n        # Update classification with metadata if available\n        if meta_cols and not result_df.empty:\n            meta_updates = 0\n            for col in meta_cols:\n                for idx, meta_value in col.dataframe[\"value\"].items():\n                    if idx in result_df.index:\n                        is_bolus, is_basal, insulin_type = self._extract_meta_info(\n                            meta_value\n                        )\n                        if insulin_type:\n                            result_df.loc[idx, \"is_bolus\"] = is_bolus\n                            result_df.loc[idx, \"is_basal\"] = is_basal\n                            result_df.loc[idx, \"type\"] = insulin_type\n                            meta_updates += 1\n\n            if meta_updates &gt; 0:\n                processing_notes.append(\n                    f\"Updated {meta_updates} classifications using metadata\"\n                )\n\n        # Final stats\n        processing_notes.append(\n            f\"Final dataset contains {len(result_df)} insulin records\"\n        )\n\n        return ProcessedTypeData(\n            dataframe=result_df,\n            source_units=result_units,\n            processing_notes=processing_notes,\n        )\n\n    except Exception as e:\n        raise ProcessingError(f\"Error processing insulin data: {str(e)}\") from e\n</code></pre>"},{"location":"api/processors/#src.processors.NotesProcessor","title":"<code>NotesProcessor</code>","text":"<p>               Bases: <code>BaseTypeProcessor</code></p> <p>Processes text notes/comments data.</p> Source code in <code>src/processors/notes.py</code> <pre><code>@DataProcessor.register_processor(DataType.NOTES)\nclass NotesProcessor(BaseTypeProcessor):\n    \"\"\"Processes text notes/comments data.\"\"\"\n\n    def process_type(\n        self,\n        columns: List[ColumnData],\n    ) -&gt; ProcessedTypeData:\n        \"\"\"Process notes data ensuring safe string storage.\n\n        Args:\n            columns: List of ColumnData containing notes columns\n\n        Returns:\n            ProcessedTypeData containing processed notes\n        \"\"\"\n        processing_notes = []\n\n        try:\n            # Validate we have at least one primary column\n            if not any(col.is_primary for col in columns):\n                raise ProcessingError(\"No primary notes column found\")\n\n            # Sort columns to ensure primary first\n            sorted_columns = sorted(columns, key=lambda x: (not x.is_primary))\n\n            # Initialize result dataframe\n            result_df = pd.DataFrame(index=pd.DatetimeIndex([]))\n\n            # Process each column\n            for idx, col_data in enumerate(sorted_columns):\n                # Generate column name\n                col_name = self._generate_column_name(\n                    DataType.NOTES, col_data.is_primary, idx\n                )\n\n                # Get the notes series\n                notes_series = col_data.dataframe[\"value\"]\n\n                # Replace None with pd.NA for better handling\n                notes_series = notes_series.replace([None], pd.NA)\n\n                # Convert non-NA values to string and strip whitespace\n                notes_series = notes_series.apply(\n                    lambda x: x.strip() if pd.notna(x) else pd.NA\n                )\n\n                # Remove empty strings\n                notes_series = notes_series.replace({\"\": pd.NA})\n\n                # Add to result DataFrame only if we have any valid notes\n                if not notes_series.isna().all():\n                    result_df[col_name] = notes_series\n\n                    # Track stats\n                    valid_notes = notes_series.notna()\n                    processing_notes.append(\n                        f\"Column {col_name}: found {valid_notes.sum()} valid notes\"\n                    )\n\n            # Drop rows where all values are NA\n            if not result_df.empty:\n                result_df = result_df.dropna(how=\"all\")\n\n            processing_notes.append(\n                f\"Final dataset contains {len(result_df)} notes entries\"\n            )\n\n            return ProcessedTypeData(\n                dataframe=result_df,\n                source_units={},  # No units for text data\n                processing_notes=processing_notes,\n            )\n\n        except Exception as e:\n            raise ProcessingError(f\"Error processing notes data: {str(e)}\") from e\n</code></pre>"},{"location":"api/processors/#src.processors.NotesProcessor.process_type","title":"<code>process_type(columns)</code>","text":"<p>Process notes data ensuring safe string storage.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>List[ColumnData]</code> <p>List of ColumnData containing notes columns</p> required <p>Returns:</p> Type Description <code>ProcessedTypeData</code> <p>ProcessedTypeData containing processed notes</p> Source code in <code>src/processors/notes.py</code> <pre><code>def process_type(\n    self,\n    columns: List[ColumnData],\n) -&gt; ProcessedTypeData:\n    \"\"\"Process notes data ensuring safe string storage.\n\n    Args:\n        columns: List of ColumnData containing notes columns\n\n    Returns:\n        ProcessedTypeData containing processed notes\n    \"\"\"\n    processing_notes = []\n\n    try:\n        # Validate we have at least one primary column\n        if not any(col.is_primary for col in columns):\n            raise ProcessingError(\"No primary notes column found\")\n\n        # Sort columns to ensure primary first\n        sorted_columns = sorted(columns, key=lambda x: (not x.is_primary))\n\n        # Initialize result dataframe\n        result_df = pd.DataFrame(index=pd.DatetimeIndex([]))\n\n        # Process each column\n        for idx, col_data in enumerate(sorted_columns):\n            # Generate column name\n            col_name = self._generate_column_name(\n                DataType.NOTES, col_data.is_primary, idx\n            )\n\n            # Get the notes series\n            notes_series = col_data.dataframe[\"value\"]\n\n            # Replace None with pd.NA for better handling\n            notes_series = notes_series.replace([None], pd.NA)\n\n            # Convert non-NA values to string and strip whitespace\n            notes_series = notes_series.apply(\n                lambda x: x.strip() if pd.notna(x) else pd.NA\n            )\n\n            # Remove empty strings\n            notes_series = notes_series.replace({\"\": pd.NA})\n\n            # Add to result DataFrame only if we have any valid notes\n            if not notes_series.isna().all():\n                result_df[col_name] = notes_series\n\n                # Track stats\n                valid_notes = notes_series.notna()\n                processing_notes.append(\n                    f\"Column {col_name}: found {valid_notes.sum()} valid notes\"\n                )\n\n        # Drop rows where all values are NA\n        if not result_df.empty:\n            result_df = result_df.dropna(how=\"all\")\n\n        processing_notes.append(\n            f\"Final dataset contains {len(result_df)} notes entries\"\n        )\n\n        return ProcessedTypeData(\n            dataframe=result_df,\n            source_units={},  # No units for text data\n            processing_notes=processing_notes,\n        )\n\n    except Exception as e:\n        raise ProcessingError(f\"Error processing notes data: {str(e)}\") from e\n</code></pre>"},{"location":"api/readers/","title":"Readers API","text":""},{"location":"api/readers/#complete-reference","title":"Complete Reference","text":""},{"location":"api/readers/#src.readers","title":"<code>src.readers</code>","text":"<p>Reader initialization and registration.</p>"},{"location":"api/readers/#src.readers.BaseReader","title":"<code>BaseReader</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for all file format readers.</p> <p>This class provides core functionality for reading diabetes device data files and automatic reader selection based on file types. It handles timestamp processing, data validation, and resource management.</p> Source code in <code>src/readers/base.py</code> <pre><code>class BaseReader(ABC):\n    \"\"\"Abstract base class for all file format readers.\n\n    This class provides core functionality for reading diabetes device data files\n    and automatic reader selection based on file types. It handles timestamp processing,\n    data validation, and resource management.\n    \"\"\"\n\n    _readers: Dict[FileType, Type[\"BaseReader\"]] = {}\n\n    @classmethod\n    def register(cls, file_type: FileType):\n        \"\"\"Register a reader class for a specific file type.\n\n        Args:\n            file_type: FileType enum value to associate with the reader\n\n        Returns:\n            Decorator function that registers the reader class\n        \"\"\"\n\n        def wrapper(reader_cls):\n            cls._readers[file_type] = reader_cls\n            return reader_cls\n\n        return wrapper\n\n    @classmethod\n    def get_reader_for_format(cls, fmt: DeviceFormat, file_path: Path) -&gt; \"BaseReader\":\n        \"\"\"Get appropriate reader instance for the detected format.\n\n        Args:\n            fmt: Detected device format specification\n            file_path: Path to the data file\n\n        Returns:\n            Instance of appropriate reader class\n\n        Raises:\n            ReaderError: If no reader is registered for the file type\n        \"\"\"\n        for file_config in fmt.files:\n            if Path(file_path).match(file_config.name_pattern):\n                reader_cls = cls._readers.get(file_config.file_type)\n                if reader_cls is None:\n                    raise ReaderError(\n                        f\"No reader registered for file type: {file_config.file_type.value}\"\n                    )\n                return reader_cls(file_path, file_config)\n\n        raise ReaderError(f\"No matching file configuration found for {file_path}\")\n\n    def __init__(self, path: Path, file_config: FileConfig):\n        \"\"\"Initialize reader with file path and configuration.\n\n        Args:\n            path: Path to the data file\n            file_config: Configuration for the file format\n\n        Raises:\n            ValueError: If file does not exist\n        \"\"\"\n        if not path.exists():\n            raise ValueError(f\"File not found: {path}\")\n\n        self.file_path = path\n        self.file_config = file_config\n\n    def __enter__(self):\n        \"\"\"Context manager entry.\"\"\"\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        \"\"\"Cleanup resources if needed.\"\"\"\n        self._cleanup()\n\n    def _cleanup(self):\n        \"\"\"Override this method in derived classes if cleanup is needed.\"\"\"\n\n    @abstractmethod\n    def read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n        \"\"\"Read and process a single table according to its structure.\n\n        This method must be implemented by each specific reader.\n        \"\"\"\n\n    def read_all_tables(self) -&gt; Dict[str, TableData]:\n        \"\"\"Read and process all tables defined in the file configuration.\"\"\"\n        results = {}\n        for table_config in self.file_config.tables:\n            table_data = self.read_table(table_config)\n            if table_data is not None:\n                if table_data.missing_required_columns:\n                    logger.warning(\n                        \"Table %s missing required data in columns: %s\",\n                        table_data.name,\n                        table_data.missing_required_columns,\n                    )\n                results[table_data.name] = table_data\n            else:\n                logger.error(\"Failed to process table: %s\", table_config.name)\n\n        return results\n\n    def detect_timestamp_format(self, series: pd.Series) -&gt; TimestampType:\n        \"\"\"Detect the format of timestamp data, assuming chronological order.\"\"\"\n        try:\n            # Sample timestamps without sorting\n            sample = series.dropna().head(10)\n            if sample.empty:\n                logger.warning(\"No non-null timestamps found in sample\")\n                return TimestampType.UNKNOWN\n\n            # Check if values are monotonically increasing\n            if not sample.is_monotonic_increasing:\n                logger.warning(\"Timestamps are not in chronological order\")\n                return TimestampType.UNKNOWN\n\n            # pylint: disable=R1705\n            # Supress pylint warning and use elif functionality for efficiency\n            # Check for UNIX epoch formats\n            if all(sample.astype(float) &lt; 1e10):  # Seconds\n                logger.debug(\"Detected timestamp type: UNIX_SECONDS\")\n                return TimestampType.UNIX_SECONDS\n            elif all(sample.astype(float) &lt; 1e13):  # Milliseconds\n                logger.debug(\"Detected timestamp type: UNIX_MILLISECONDS\")\n                return TimestampType.UNIX_MILLISECONDS\n            elif all(sample.astype(float) &lt; 1e16):  # Microseconds\n                logger.debug(\"Detected timestamp type: UNIX_MICROSECONDS\")\n                return TimestampType.UNIX_MICROSECONDS\n\n            # Try ISO 8601 for string timestamps\n            try:\n                pd.to_datetime(sample, utc=True)\n                logger.debug(\"Detected timestamp type: ISO_8601\")\n                return TimestampType.ISO_8601\n            except TimestampProcessingError:\n                pass\n\n            logger.warning(\"Could not determine timestamp format\")\n            return TimestampType.UNKNOWN\n\n        except TimestampProcessingError as e:\n            logger.error(\"Error during timestamp detection: %s\", e)\n            return TimestampType.UNKNOWN\n\n    def _convert_timestamp_to_utc(\n        self, df: pd.DataFrame, timestamp_column: str\n    ) -&gt; Tuple[pd.DataFrame, TimestampType]:\n        \"\"\"Convert timestamp column to UTC datetime and set as index.\"\"\"\n        fmt = self.detect_timestamp_format(df[timestamp_column])\n\n        if fmt == TimestampType.UNKNOWN:\n            raise TimestampProcessingError(\n                f\"Could not detect timestamp format for column {timestamp_column}\"\n            )\n\n        try:\n            if fmt == TimestampType.UNIX_SECONDS:\n                df[timestamp_column] = pd.to_datetime(\n                    df[timestamp_column], unit=\"s\", utc=True\n                )\n            elif fmt == TimestampType.UNIX_MILLISECONDS:\n                df[timestamp_column] = pd.to_datetime(\n                    df[timestamp_column], unit=\"ms\", utc=True\n                )\n            elif fmt == TimestampType.UNIX_MICROSECONDS:\n                df[timestamp_column] = pd.to_datetime(\n                    df[timestamp_column], unit=\"us\", utc=True\n                )\n            elif fmt == TimestampType.ISO_8601:\n                df[timestamp_column] = pd.to_datetime(df[timestamp_column], utc=True)\n\n            return df.set_index(timestamp_column).sort_index(), fmt\n\n        except DataProcessingError as e:\n            logger.error(\"Error converting timestamps: %s\", e)\n            raise DataProcessingError(f\"Invalid value: {timestamp_column}\") from e\n\n    def _validate_required_data(\n        self, df: pd.DataFrame, columns: List[ColumnMapping]\n    ) -&gt; List[str]:\n        \"\"\"Check for missing data in required columns.\"\"\"\n        missing_required = []\n        for col in columns:\n            if (\n                col.requirement == ColumnRequirement.REQUIRED_WITH_DATA\n                and col.source_name in df.columns\n                and df[col.source_name].isna().all()\n            ):\n                missing_required.append(col.source_name)\n        return missing_required\n\n    @staticmethod\n    def _validate_identifier(identifier: str) -&gt; bool:\n        \"\"\"Validate that an identifier only contains safe characters.\"\"\"\n        return all(c.isalnum() or c in [\"_\", \".\"] for c in identifier)\n</code></pre>"},{"location":"api/readers/#src.readers.BaseReader.__enter__","title":"<code>__enter__()</code>","text":"<p>Context manager entry.</p> Source code in <code>src/readers/base.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    return self\n</code></pre>"},{"location":"api/readers/#src.readers.BaseReader.__exit__","title":"<code>__exit__(exc_type, exc_val, exc_tb)</code>","text":"<p>Cleanup resources if needed.</p> Source code in <code>src/readers/base.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Cleanup resources if needed.\"\"\"\n    self._cleanup()\n</code></pre>"},{"location":"api/readers/#src.readers.BaseReader.__init__","title":"<code>__init__(path, file_config)</code>","text":"<p>Initialize reader with file path and configuration.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>Path to the data file</p> required <code>file_config</code> <code>FileConfig</code> <p>Configuration for the file format</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If file does not exist</p> Source code in <code>src/readers/base.py</code> <pre><code>def __init__(self, path: Path, file_config: FileConfig):\n    \"\"\"Initialize reader with file path and configuration.\n\n    Args:\n        path: Path to the data file\n        file_config: Configuration for the file format\n\n    Raises:\n        ValueError: If file does not exist\n    \"\"\"\n    if not path.exists():\n        raise ValueError(f\"File not found: {path}\")\n\n    self.file_path = path\n    self.file_config = file_config\n</code></pre>"},{"location":"api/readers/#src.readers.BaseReader.detect_timestamp_format","title":"<code>detect_timestamp_format(series)</code>","text":"<p>Detect the format of timestamp data, assuming chronological order.</p> Source code in <code>src/readers/base.py</code> <pre><code>def detect_timestamp_format(self, series: pd.Series) -&gt; TimestampType:\n    \"\"\"Detect the format of timestamp data, assuming chronological order.\"\"\"\n    try:\n        # Sample timestamps without sorting\n        sample = series.dropna().head(10)\n        if sample.empty:\n            logger.warning(\"No non-null timestamps found in sample\")\n            return TimestampType.UNKNOWN\n\n        # Check if values are monotonically increasing\n        if not sample.is_monotonic_increasing:\n            logger.warning(\"Timestamps are not in chronological order\")\n            return TimestampType.UNKNOWN\n\n        # pylint: disable=R1705\n        # Supress pylint warning and use elif functionality for efficiency\n        # Check for UNIX epoch formats\n        if all(sample.astype(float) &lt; 1e10):  # Seconds\n            logger.debug(\"Detected timestamp type: UNIX_SECONDS\")\n            return TimestampType.UNIX_SECONDS\n        elif all(sample.astype(float) &lt; 1e13):  # Milliseconds\n            logger.debug(\"Detected timestamp type: UNIX_MILLISECONDS\")\n            return TimestampType.UNIX_MILLISECONDS\n        elif all(sample.astype(float) &lt; 1e16):  # Microseconds\n            logger.debug(\"Detected timestamp type: UNIX_MICROSECONDS\")\n            return TimestampType.UNIX_MICROSECONDS\n\n        # Try ISO 8601 for string timestamps\n        try:\n            pd.to_datetime(sample, utc=True)\n            logger.debug(\"Detected timestamp type: ISO_8601\")\n            return TimestampType.ISO_8601\n        except TimestampProcessingError:\n            pass\n\n        logger.warning(\"Could not determine timestamp format\")\n        return TimestampType.UNKNOWN\n\n    except TimestampProcessingError as e:\n        logger.error(\"Error during timestamp detection: %s\", e)\n        return TimestampType.UNKNOWN\n</code></pre>"},{"location":"api/readers/#src.readers.BaseReader.get_reader_for_format","title":"<code>get_reader_for_format(fmt, file_path)</code>  <code>classmethod</code>","text":"<p>Get appropriate reader instance for the detected format.</p> <p>Parameters:</p> Name Type Description Default <code>fmt</code> <code>DeviceFormat</code> <p>Detected device format specification</p> required <code>file_path</code> <code>Path</code> <p>Path to the data file</p> required <p>Returns:</p> Type Description <code>BaseReader</code> <p>Instance of appropriate reader class</p> <p>Raises:</p> Type Description <code>ReaderError</code> <p>If no reader is registered for the file type</p> Source code in <code>src/readers/base.py</code> <pre><code>@classmethod\ndef get_reader_for_format(cls, fmt: DeviceFormat, file_path: Path) -&gt; \"BaseReader\":\n    \"\"\"Get appropriate reader instance for the detected format.\n\n    Args:\n        fmt: Detected device format specification\n        file_path: Path to the data file\n\n    Returns:\n        Instance of appropriate reader class\n\n    Raises:\n        ReaderError: If no reader is registered for the file type\n    \"\"\"\n    for file_config in fmt.files:\n        if Path(file_path).match(file_config.name_pattern):\n            reader_cls = cls._readers.get(file_config.file_type)\n            if reader_cls is None:\n                raise ReaderError(\n                    f\"No reader registered for file type: {file_config.file_type.value}\"\n                )\n            return reader_cls(file_path, file_config)\n\n    raise ReaderError(f\"No matching file configuration found for {file_path}\")\n</code></pre>"},{"location":"api/readers/#src.readers.BaseReader.read_all_tables","title":"<code>read_all_tables()</code>","text":"<p>Read and process all tables defined in the file configuration.</p> Source code in <code>src/readers/base.py</code> <pre><code>def read_all_tables(self) -&gt; Dict[str, TableData]:\n    \"\"\"Read and process all tables defined in the file configuration.\"\"\"\n    results = {}\n    for table_config in self.file_config.tables:\n        table_data = self.read_table(table_config)\n        if table_data is not None:\n            if table_data.missing_required_columns:\n                logger.warning(\n                    \"Table %s missing required data in columns: %s\",\n                    table_data.name,\n                    table_data.missing_required_columns,\n                )\n            results[table_data.name] = table_data\n        else:\n            logger.error(\"Failed to process table: %s\", table_config.name)\n\n    return results\n</code></pre>"},{"location":"api/readers/#src.readers.BaseReader.read_table","title":"<code>read_table(table_structure)</code>  <code>abstractmethod</code>","text":"<p>Read and process a single table according to its structure.</p> <p>This method must be implemented by each specific reader.</p> Source code in <code>src/readers/base.py</code> <pre><code>@abstractmethod\ndef read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n    \"\"\"Read and process a single table according to its structure.\n\n    This method must be implemented by each specific reader.\n    \"\"\"\n</code></pre>"},{"location":"api/readers/#src.readers.BaseReader.register","title":"<code>register(file_type)</code>  <code>classmethod</code>","text":"<p>Register a reader class for a specific file type.</p> <p>Parameters:</p> Name Type Description Default <code>file_type</code> <code>FileType</code> <p>FileType enum value to associate with the reader</p> required <p>Returns:</p> Type Description <p>Decorator function that registers the reader class</p> Source code in <code>src/readers/base.py</code> <pre><code>@classmethod\ndef register(cls, file_type: FileType):\n    \"\"\"Register a reader class for a specific file type.\n\n    Args:\n        file_type: FileType enum value to associate with the reader\n\n    Returns:\n        Decorator function that registers the reader class\n    \"\"\"\n\n    def wrapper(reader_cls):\n        cls._readers[file_type] = reader_cls\n        return reader_cls\n\n    return wrapper\n</code></pre>"},{"location":"api/readers/#src.readers.CSVReader","title":"<code>CSVReader</code>","text":"<p>               Bases: <code>BaseReader</code></p> <p>Reads and processes CSV files according to the provided format configuration.</p> Source code in <code>src/readers/csv.py</code> <pre><code>@BaseReader.register(FileType.CSV)\nclass CSVReader(BaseReader):\n    \"\"\"Reads and processes CSV files according to the provided format configuration.\"\"\"\n\n    def __init__(self, path: Path, file_config: FileConfig):\n        super().__init__(path, file_config)\n        self._data = None\n\n    def _cleanup(self):\n        \"\"\"Cleanup any held resources.\"\"\"\n        self._data = None\n\n    def read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n        \"\"\"Read and process a single table according to its structure.\n\n        For CSV files, we treat each file as a single table, reading all data at once\n        and caching it for subsequent operations if needed.\n        \"\"\"\n        try:\n            # Read data if not already cached\n            if self._data is None:\n                try:\n                    self._data = pd.read_csv(\n                        self.file_path,\n                        encoding=\"utf-8\",\n                        low_memory=False,  # Prevent mixed type inference warnings\n                    )\n                except Exception as e:\n                    raise FileAccessError(f\"Failed to read CSV file: {e}\") from e\n\n            if self._data.empty:\n                raise DataExistsError(f\"No data found in CSV file {self.file_path}\")\n\n            # Get required columns\n            columns_to_read = [\n                col.source_name\n                for col in table_structure.columns\n                if col.requirement != ColumnRequirement.CONFIRMATION_ONLY\n            ]\n            columns_to_read.append(table_structure.timestamp_column)\n\n            # Check for missing columns\n            missing_columns = [\n                col for col in columns_to_read if col not in self._data.columns\n            ]\n            if missing_columns:\n                logger.error(\n                    \"Required columns missing from CSV: %s\", \", \".join(missing_columns)\n                )\n                return None\n\n            # Select only needed columns and make a copy\n            df = self._data[columns_to_read].copy()\n\n            # Process timestamps\n            df, fmt = self._convert_timestamp_to_utc(\n                df, table_structure.timestamp_column\n            )\n\n            # Validate required data\n            missing_required = self._validate_required_data(df, table_structure.columns)\n\n            return TableData(\n                name=table_structure.name,\n                dataframe=df,\n                missing_required_columns=missing_required,\n                timestamp_type=fmt,\n            )\n\n        except DataValidationError as e:\n            logger.error(\"Validation error: %s\", e)\n            return None\n        except DataExistsError as e:\n            logger.error(\"No data error: %s\", e)\n            return None\n        except DataProcessingError as e:\n            logger.error(\"Processing error: %s\", e)\n            return None\n        except ProcessingError as e:\n            logger.error(\"Unexpected error processing CSV: %s\", e)\n            return None\n</code></pre>"},{"location":"api/readers/#src.readers.CSVReader.read_table","title":"<code>read_table(table_structure)</code>","text":"<p>Read and process a single table according to its structure.</p> <p>For CSV files, we treat each file as a single table, reading all data at once and caching it for subsequent operations if needed.</p> Source code in <code>src/readers/csv.py</code> <pre><code>def read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n    \"\"\"Read and process a single table according to its structure.\n\n    For CSV files, we treat each file as a single table, reading all data at once\n    and caching it for subsequent operations if needed.\n    \"\"\"\n    try:\n        # Read data if not already cached\n        if self._data is None:\n            try:\n                self._data = pd.read_csv(\n                    self.file_path,\n                    encoding=\"utf-8\",\n                    low_memory=False,  # Prevent mixed type inference warnings\n                )\n            except Exception as e:\n                raise FileAccessError(f\"Failed to read CSV file: {e}\") from e\n\n        if self._data.empty:\n            raise DataExistsError(f\"No data found in CSV file {self.file_path}\")\n\n        # Get required columns\n        columns_to_read = [\n            col.source_name\n            for col in table_structure.columns\n            if col.requirement != ColumnRequirement.CONFIRMATION_ONLY\n        ]\n        columns_to_read.append(table_structure.timestamp_column)\n\n        # Check for missing columns\n        missing_columns = [\n            col for col in columns_to_read if col not in self._data.columns\n        ]\n        if missing_columns:\n            logger.error(\n                \"Required columns missing from CSV: %s\", \", \".join(missing_columns)\n            )\n            return None\n\n        # Select only needed columns and make a copy\n        df = self._data[columns_to_read].copy()\n\n        # Process timestamps\n        df, fmt = self._convert_timestamp_to_utc(\n            df, table_structure.timestamp_column\n        )\n\n        # Validate required data\n        missing_required = self._validate_required_data(df, table_structure.columns)\n\n        return TableData(\n            name=table_structure.name,\n            dataframe=df,\n            missing_required_columns=missing_required,\n            timestamp_type=fmt,\n        )\n\n    except DataValidationError as e:\n        logger.error(\"Validation error: %s\", e)\n        return None\n    except DataExistsError as e:\n        logger.error(\"No data error: %s\", e)\n        return None\n    except DataProcessingError as e:\n        logger.error(\"Processing error: %s\", e)\n        return None\n    except ProcessingError as e:\n        logger.error(\"Unexpected error processing CSV: %s\", e)\n        return None\n</code></pre>"},{"location":"api/readers/#src.readers.SQLiteReader","title":"<code>SQLiteReader</code>","text":"<p>               Bases: <code>BaseReader</code></p> <p>Reads and processes SQLite files according to the provided format configuration.</p> Source code in <code>src/readers/sqlite.py</code> <pre><code>@BaseReader.register(FileType.SQLITE)\nclass SQLiteReader(BaseReader):\n    \"\"\"Reads and processes SQLite files according to the provided format configuration.\"\"\"\n\n    def __init__(self, path: Path, file_config: FileConfig):\n        super().__init__(path, file_config)\n        self._engine = None\n\n    @property\n    def engine(self):\n        \"\"\"Lazy initialization of database engine.\"\"\"\n        if self._engine is None:\n            self._engine = create_engine(f\"sqlite:///{self.file_path}\")\n        return self._engine\n\n    def _cleanup(self):\n        \"\"\"Cleanup database connections.\"\"\"\n        if self._engine is not None:\n            self._engine.dispose()\n            self._engine = None\n\n    def read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n        \"\"\"Read and process a single table according to its structure.\"\"\"\n        try:\n            # Validate identifiers\n            if not self._validate_identifier(table_structure.name):\n                raise DataValidationError(f\"Invalid table name: {table_structure.name}\")\n\n            # Read only needed columns\n            columns_to_read = [\n                col.source_name\n                for col in table_structure.columns\n                if col.requirement != ColumnRequirement.CONFIRMATION_ONLY\n            ]\n            columns_to_read.append(table_structure.timestamp_column)\n\n            # Validate column names\n            for col in columns_to_read:\n                if not self._validate_identifier(col):\n                    raise DataValidationError(f\"Invalid column name: {col}\")\n\n            # Create query with quoted identifiers for SQLite\n            quoted_columns = [f'\"{col}\"' for col in columns_to_read]\n            query = text(\n                f\"\"\"\n                SELECT {', '.join(quoted_columns)}\n                FROM \"{table_structure.name}\"\n                ORDER BY \"{table_structure.timestamp_column}\"\n            \"\"\"\n            )\n\n            # Execute query within connection context\n            with self.engine.connect() as conn:\n                df = pd.read_sql_query(query, conn)\n\n            # Process timestamps\n            df, fmt = self._convert_timestamp_to_utc(\n                df, table_structure.timestamp_column\n            )\n\n            # Validate required data\n            missing_required = self._validate_required_data(df, table_structure.columns)\n\n            return TableData(\n                name=table_structure.name,\n                dataframe=df,\n                missing_required_columns=missing_required,\n                timestamp_type=fmt,\n            )\n\n        except DataValidationError as e:\n            # Handle specific ValueErrors such as invalid table or column names\n            logger.error(\"ValueError: %s\", e)\n            return None\n        except SQLAlchemyError as e:\n            # Handle any database-related errors (e.g., connection, query execution)\n            logger.error(\n                \"SQLAlchemyError processing table %s: %s\", table_structure.name, e\n            )\n            return None\n        except DataExistsError as e:\n            # Handle case where there is no data in the result set\n            logger.error(\n                \"EmptyDataError processing table %s: %s\", table_structure.name, e\n            )\n            return None\n        except ReaderError as e:\n            # Catch any unexpected errors\n            logger.error(\n                \"Unexpected error processing table %s: %s\", table_structure.name, e\n            )\n            return None\n</code></pre>"},{"location":"api/readers/#src.readers.SQLiteReader.engine","title":"<code>engine</code>  <code>property</code>","text":"<p>Lazy initialization of database engine.</p>"},{"location":"api/readers/#src.readers.SQLiteReader.read_table","title":"<code>read_table(table_structure)</code>","text":"<p>Read and process a single table according to its structure.</p> Source code in <code>src/readers/sqlite.py</code> <pre><code>def read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n    \"\"\"Read and process a single table according to its structure.\"\"\"\n    try:\n        # Validate identifiers\n        if not self._validate_identifier(table_structure.name):\n            raise DataValidationError(f\"Invalid table name: {table_structure.name}\")\n\n        # Read only needed columns\n        columns_to_read = [\n            col.source_name\n            for col in table_structure.columns\n            if col.requirement != ColumnRequirement.CONFIRMATION_ONLY\n        ]\n        columns_to_read.append(table_structure.timestamp_column)\n\n        # Validate column names\n        for col in columns_to_read:\n            if not self._validate_identifier(col):\n                raise DataValidationError(f\"Invalid column name: {col}\")\n\n        # Create query with quoted identifiers for SQLite\n        quoted_columns = [f'\"{col}\"' for col in columns_to_read]\n        query = text(\n            f\"\"\"\n            SELECT {', '.join(quoted_columns)}\n            FROM \"{table_structure.name}\"\n            ORDER BY \"{table_structure.timestamp_column}\"\n        \"\"\"\n        )\n\n        # Execute query within connection context\n        with self.engine.connect() as conn:\n            df = pd.read_sql_query(query, conn)\n\n        # Process timestamps\n        df, fmt = self._convert_timestamp_to_utc(\n            df, table_structure.timestamp_column\n        )\n\n        # Validate required data\n        missing_required = self._validate_required_data(df, table_structure.columns)\n\n        return TableData(\n            name=table_structure.name,\n            dataframe=df,\n            missing_required_columns=missing_required,\n            timestamp_type=fmt,\n        )\n\n    except DataValidationError as e:\n        # Handle specific ValueErrors such as invalid table or column names\n        logger.error(\"ValueError: %s\", e)\n        return None\n    except SQLAlchemyError as e:\n        # Handle any database-related errors (e.g., connection, query execution)\n        logger.error(\n            \"SQLAlchemyError processing table %s: %s\", table_structure.name, e\n        )\n        return None\n    except DataExistsError as e:\n        # Handle case where there is no data in the result set\n        logger.error(\n            \"EmptyDataError processing table %s: %s\", table_structure.name, e\n        )\n        return None\n    except ReaderError as e:\n        # Catch any unexpected errors\n        logger.error(\n            \"Unexpected error processing table %s: %s\", table_structure.name, e\n        )\n        return None\n</code></pre>"},{"location":"api/readers/#src.readers.XMLReader","title":"<code>XMLReader</code>","text":"<p>               Bases: <code>BaseReader</code></p> <p>Reads and processes XML files according to the provided format configuration.</p> Source code in <code>src/readers/xml.py</code> <pre><code>@BaseReader.register(FileType.XML)\nclass XMLReader(BaseReader):\n    \"\"\"Reads and processes XML files according to the provided format configuration.\"\"\"\n\n    def __init__(self, path: Path, file_config: FileConfig):\n        super().__init__(path, file_config)\n        self._tree = None\n        self._root = None\n\n    def _cleanup(self):\n        \"\"\"Cleanup any held resources.\"\"\"\n        self._tree = None\n        self._root = None\n\n    def _init_xml(self):\n        \"\"\"Initialize XML parsing if not already done.\"\"\"\n        if self._root is None:\n            try:\n                self._tree = ET.parse(self.file_path)\n                self._root = self._tree.getroot()\n            except ET.ParseError as e:\n                raise DataExistsError(f\"Failed to parse XML file: {e}\") from e\n            except Exception as e:\n                raise DataExistsError(f\"Error reading XML file: {e}\") from e\n\n    @staticmethod\n    def _extract_value(element: ET.Element, column: str) -&gt; str:\n        \"\"\"Extract value from XML element, checking both attributes and text.\n\n        Args:\n            element: XML element to extract from\n            column: Column name to look for\n\n        Returns:\n            Value from attribute or element text\n        \"\"\"\n        # Check attributes first\n        if column in element.attrib:\n            return element.attrib[column]\n\n        # Then check child elements\n        child = element.find(column)\n        if child is not None:\n            return child.text if child.text else \"\"\n\n        return \"\"\n\n    def read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n        \"\"\"Read and process a single table according to its structure.\n\n        For XML files, each table is expected to be contained within elements\n        matching the table name or a configured xpath.\n        \"\"\"\n        try:\n            self._init_xml()\n\n            # Get required columns\n            columns_to_read = [\n                col.source_name\n                for col in table_structure.columns\n                if col.requirement != ColumnRequirement.CONFIRMATION_ONLY\n            ]\n            columns_to_read.append(table_structure.timestamp_column)\n\n            # Find all elements for this table\n            table_elements = self._root.findall(f\".//{table_structure.name}\")\n            if not table_elements:\n                logger.error(\"No elements found for table: %s\", table_structure.name)\n                return None\n\n            # Extract data for each column\n            data: Dict[str, List[str]] = {col: [] for col in columns_to_read}\n\n            for element in table_elements:\n                for column in columns_to_read:\n                    value = self._extract_value(element, column)\n                    data[column].append(value)\n\n            # Convert to DataFrame\n            df = pd.DataFrame(data)\n\n            if df.empty:\n                raise DataExistsError(f\"No data found in table {table_structure.name}\")\n\n            # Process timestamps\n            df, fmt = self._convert_timestamp_to_utc(\n                df, table_structure.timestamp_column\n            )\n\n            # Validate required data\n            missing_required = self._validate_required_data(df, table_structure.columns)\n\n            return TableData(\n                name=table_structure.name,\n                dataframe=df,\n                missing_required_columns=missing_required,\n                timestamp_type=fmt,\n            )\n\n        except DataValidationError as e:\n            logger.error(\"Validation error: %s\", e)\n            return None\n        except DataExistsError as e:\n            logger.error(\"No data error: %s\", e)\n            return None\n        except DataProcessingError as e:\n            logger.error(\"Processing error: %s\", e)\n            return None\n        except ReaderError as e:\n            logger.error(\"Unexpected error processing XML: %s\", e)\n            return None\n</code></pre>"},{"location":"api/readers/#src.readers.XMLReader.read_table","title":"<code>read_table(table_structure)</code>","text":"<p>Read and process a single table according to its structure.</p> <p>For XML files, each table is expected to be contained within elements matching the table name or a configured xpath.</p> Source code in <code>src/readers/xml.py</code> <pre><code>def read_table(self, table_structure: TableStructure) -&gt; Optional[TableData]:\n    \"\"\"Read and process a single table according to its structure.\n\n    For XML files, each table is expected to be contained within elements\n    matching the table name or a configured xpath.\n    \"\"\"\n    try:\n        self._init_xml()\n\n        # Get required columns\n        columns_to_read = [\n            col.source_name\n            for col in table_structure.columns\n            if col.requirement != ColumnRequirement.CONFIRMATION_ONLY\n        ]\n        columns_to_read.append(table_structure.timestamp_column)\n\n        # Find all elements for this table\n        table_elements = self._root.findall(f\".//{table_structure.name}\")\n        if not table_elements:\n            logger.error(\"No elements found for table: %s\", table_structure.name)\n            return None\n\n        # Extract data for each column\n        data: Dict[str, List[str]] = {col: [] for col in columns_to_read}\n\n        for element in table_elements:\n            for column in columns_to_read:\n                value = self._extract_value(element, column)\n                data[column].append(value)\n\n        # Convert to DataFrame\n        df = pd.DataFrame(data)\n\n        if df.empty:\n            raise DataExistsError(f\"No data found in table {table_structure.name}\")\n\n        # Process timestamps\n        df, fmt = self._convert_timestamp_to_utc(\n            df, table_structure.timestamp_column\n        )\n\n        # Validate required data\n        missing_required = self._validate_required_data(df, table_structure.columns)\n\n        return TableData(\n            name=table_structure.name,\n            dataframe=df,\n            missing_required_columns=missing_required,\n            timestamp_type=fmt,\n        )\n\n    except DataValidationError as e:\n        logger.error(\"Validation error: %s\", e)\n        return None\n    except DataExistsError as e:\n        logger.error(\"No data error: %s\", e)\n        return None\n    except DataProcessingError as e:\n        logger.error(\"Processing error: %s\", e)\n        return None\n    except ReaderError as e:\n        logger.error(\"Unexpected error processing XML: %s\", e)\n        return None\n</code></pre>"},{"location":"getting-started/","title":"Overview","text":"Getting Started with CGM Data Processor <p>Process your diabetes device data in minutes</p>"},{"location":"getting-started/#overview","title":"\ud83c\udfaf Overview","text":"<p>CGM Data Processor is a Python framework that helps you:</p> <ul> <li>Import data from multiple diabetes management systems</li> <li>Process and align CGM readings with treatment data</li> <li>Export cleaned, validated datasets for analysis</li> </ul>"},{"location":"getting-started/#requirements","title":"\ud83d\udccb Requirements","text":"<ul> <li>Python 3.10 or higher</li> <li>Supported data formats: SQLite, CSV, XML</li> <li>Basic Python knowledge for custom analysis</li> </ul>"},{"location":"getting-started/#next-steps","title":"\ud83d\ude80 Next Steps","text":"<ul> <li>Installation Guide - Set up the package</li> <li>Basic Usage - Process your first dataset</li> <li>Data Import - Learn about supported formats</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"Installation <p>Set up CGM Data Processor for development</p>"},{"location":"getting-started/installation/#simple-install","title":"Simple install","text":"<pre><code># Clone repository\ngit clone https://github.com/Warren8824/cgm-data-processor.git\ncd cgm-data-processor\n\n# Install dependencies using pip\npip install -r requirements.txt\n\n# Or using Poetry \npoetry install\n</code></pre> <p>And as simple as that the system is ready to use. - Check out our Basic Usage page.</p>"},{"location":"getting-started/installation/#development-setup","title":"\ud83d\udee0\ufe0f Development Setup","text":"<ul> <li>Python 3.10+ required</li> <li>Poetry for dependency management (Preferred)</li> <li>Git for version control</li> </ul> <pre><code># Clone repository\ngit clone https://github.com/Warren8824/cgm-data-processor.git\ncd cgm-data-processor\n\n# Install Poetry (if not installed)\ncurl -sSL https://install.python-poetry.org | python3 -\n\n# Create and activate new environment\n\n# macOS/Linux:\npoetry env activate\nsource $(poetry env info --path)/bin/activate\n\n# Windows(Powershell):\npoetry env activate\n(Invoke-Expression \"$(poetry env info --path)\\Scripts\\Activate\")\n\n# Install development dependencies\n\n# Using poetry:\npoetry install --with dev\n\n# or using venv:\npip install -r requirements-dev.txt\n\n\n# Setup pre-commit hooks\npoetry run pre-commit install\n\n# Run tests\npoetry run pytest\n</code></pre> <p>\u2705 Verify Installation</p> <pre><code>from src.core.format_registry import FormatRegistry\n\n# Should print available formats\nregistry = FormatRegistry()\nprint(registry.formats)\n</code></pre>"},{"location":"getting-started/quickstart/basic/","title":"Basic Usage","text":"Basic Usage <p>Process and analyze your diabetes data</p>"},{"location":"getting-started/quickstart/basic/#command-line-usage","title":"\ud83d\ude80 Command Line Usage","text":"<ul> <li>Basic: <code>python -m src.cli data.sqlite</code></li> <li>Custom output: <code>python -m src.cli data.sqlite --output my_folder</code></li> <li>Debug mode: <code>python -m src.cli data.sqlite --debug</code></li> </ul>"},{"location":"getting-started/quickstart/basic/#processing-options","title":"\u2699\ufe0f Processing Options","text":"<pre><code>python -m src.cli data.sqlite \\\n    --interpolation-limit 6   # Max CGM gaps to fill (6 = 30 mins)\n    --bolus-limit 10.0       # Max bolus insulin units\n    --max-dose 20.0          # Max valid insulin dose\n    --output ./my_analysis   # Output location\n</code></pre>"},{"location":"getting-started/quickstart/basic/#parameter-guide","title":"\ud83d\udcca Parameter Guide","text":"<ul> <li><code>interpolation-limit</code>: Gaps larger than this won't be filled (default: 4 = 20 mins)</li> <li><code>bolus-limit</code>: Doses above this classified as basal (default: 8.0 units)</li> <li><code>max-dose</code>: Doses above this flagged as invalid (default: 15.0 units)</li> </ul>"},{"location":"getting-started/quickstart/import/","title":"Data Import","text":"Data Import <p>Supported formats and data sources</p>"},{"location":"getting-started/quickstart/import/#supported-devices","title":"\ud83d\udcf1 Supported Devices","text":"<ul> <li>XDrip+ (SQLite format)</li> <li>Dexcom (CSV format - coming soon)</li> <li>Freestyle Libre (CSV format - coming soon)</li> </ul>"},{"location":"getting-started/quickstart/import/#xdrip-export-guide","title":"\ud83d\udcbe XDrip+ Export Guide","text":"<ul> <li>Open XDrip+</li> <li>Navigate to Settings \u2192 Data Export</li> <li>Select \"Export Database\"</li> <li>Save the .sqlite file</li> </ul>"},{"location":"getting-started/quickstart/import/#data-requirements","title":"\ud83d\udd0d Data Requirements","text":"<ul> <li>CGM readings with timestamps</li> <li>Treatment records (insulin, carbs)</li> <li>No missing required columns</li> <li>Valid data ranges for readings</li> </ul>"},{"location":"getting-started/quickstart/processing/","title":"Data Processing","text":"Data Processing <p>Understanding and configuring data processing options</p>"},{"location":"getting-started/quickstart/processing/#processing-parameters","title":"\ud83d\udd04 Processing Parameters","text":"<ul> <li>CGM Gap Handling: How many missing readings to interpolate</li> <li>Insulin Classification: Thresholds for bolus vs basal</li> <li>Data Validation: Maximum valid insulin doses</li> </ul>"},{"location":"getting-started/quickstart/processing/#configuration-examples","title":"\u2699\ufe0f Configuration Examples","text":"<pre><code># Conservative gap filling (15 mins max)\npython -m src.cli data.sqlite --interpolation-limit 3\n\n# Higher insulin thresholds\npython -m src.cli data.sqlite --bolus-limit 12.0 --max-dose 25.0\n\n# Strict validation\npython -m src.cli data.sqlite --bolus-limit 6.0 --max-dose 12.0\n</code></pre>"},{"location":"getting-started/quickstart/processing/#output-structure","title":"\ud83d\udcca Output Structure","text":"<ul> <li>complete_dataset/: Full processed data with applied parameters</li> <li>monthly/: Split data maintaining processing settings</li> <li>processing_notes.json: Configuration and quality metrics</li> </ul> Example Output Structure:  <pre><code>data/exports\n\u251c\u2500\u2500 2023-06-03_to_2024-09-28_complete\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 aligned_data.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 carbs.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 cgm.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 insulin.csv\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 notes.csv\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 processing_notes.json\n\u2514\u2500\u2500 monthly\n    \u251c\u2500\u2500 2023-06\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 aligned_data.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 carbs.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 cgm.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 insulin.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 notes.csv\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 processing_notes.json\n    \u251c\u2500\u2500 2023-07\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 aligned_data.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 carbs.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 cgm.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 insulin.csv\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 notes.csv\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 processing_notes.json\n</code></pre>"},{"location":"user-guide/data-types/","title":"Data Types & Processing","text":"Data Types <p>Core data types and their processing</p>"},{"location":"user-guide/data-types/#cgm-data","title":"\ud83d\udcc8 CGM Data","text":"<ul> <li>5-minute glucose readings</li> <li>Gap detection and interpolation</li> <li>Units: mg/dL and mmol/L</li> <li>Quality metrics: completeness, noise levels</li> </ul>"},{"location":"user-guide/data-types/#insulin-data","title":"\ud83d\udc89 Insulin Data","text":"<ul> <li>Bolus: Meal and correction doses</li> <li>Basal: Long-acting background insulin</li> <li>Automatic classification based on dose size</li> <li>Metadata support for insulin types</li> </ul>"},{"location":"user-guide/data-types/#carbohydrate-data","title":"\ud83c\udf4e Carbohydrate Data","text":"<ul> <li>Meal entries in grams</li> <li>Timestamp alignment with insulin</li> <li>Minimum 1g threshold</li> </ul>"},{"location":"user-guide/data-types/#notes-data","title":"\ud83d\udcdd Notes Data","text":"<ul> <li>Treatment notes and events</li> <li>Timestamped annotations</li> <li>Flexible text storage</li> </ul>"},{"location":"user-guide/errors/","title":"Error Handling","text":"Error Handling <p>Common errors and troubleshooting</p>"},{"location":"user-guide/errors/#common-errors","title":"\u274c Common Errors","text":"<ul> <li>FormatDetectionError: File format not recognized</li> <li>DataProcessingError: Invalid or corrupt data</li> <li>AlignmentError: Cannot align datasets</li> <li>FileAccessError: Cannot read input file</li> </ul>"},{"location":"user-guide/errors/#troubleshooting","title":"\ud83d\udd0d Troubleshooting","text":"<pre><code># Enable debug mode for detailed errors\npython -m src.cli data.sqlite --debug\n\n# Common debug output:\n\u2713 Format Detection Successful\n\u2717 Data Reading Failed: Missing required columns\n   Details: Table 'BgReadings' missing 'calculated_value'\n</code></pre>"},{"location":"user-guide/errors/#data-validation-errors","title":"\ud83d\udeab Data Validation Errors","text":"<ul> <li>Invalid glucose values (outside 40-400 mg/dL)</li> <li>Insulin doses exceeding max_dose</li> <li>Missing timestamps or required fields</li> <li>Duplicate timestamps in CGM data</li> </ul>"},{"location":"user-guide/formats/","title":"Supported Formats","text":"Supported File Formats <p>Supported diabetes device data formats</p>"},{"location":"user-guide/formats/#xdrip-sqlite","title":"\ud83d\udcf1 XDrip+ SQLite","text":"<ul> <li>Default SQLite database from XDrip+</li> <li>Contains BgReadings and Treatments tables</li> <li>Full CGM and treatment data support</li> <li>Export: Settings \u2192 Data Export \u2192 Export Database</li> </ul>"},{"location":"user-guide/formats/#schema-structure","title":"\ud83d\udcd1 Schema Structure","text":"<pre><code>-- BgReadings Table\ntimestamp          -- UTC timestamp\ncalculated_value   -- Glucose in mg/dL\nraw_data          -- Raw sensor data\n\n-- Treatments Table\ntimestamp    -- UTC timestamp\ninsulin      -- Insulin dose in units\ninsulinJSON  -- Insulin type metadata\ncarbs       -- Carbohydrates in grams\nnotes       -- Treatment notes\n</code></pre>"},{"location":"user-guide/formats/#other-formats-coming-soon","title":"\ud83d\udd04 Other Formats (Coming Soon)","text":"<ul> <li>Dexcom CSV Export</li> <li>Freestyle Libre CSV</li> <li>Nightscout Data</li> </ul>"},{"location":"user-guide/formats/#add-custom-file-formats","title":"Add Custom File Formats","text":"<p>Check out our API and developer guide if you would like to add your own formats to the program.</p>"}]}